<html lang="en-US" xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Séminaires du département d'informatique et de génie logiciel</title>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>
<div id="page">
<center>
</center>
<div id="content">
<div id="main">
<h1>Séminaires du département d'informatique et de génie logiciel</h1>

<p>À la session d'hiver 2024, les séminaires du département
d'informatique et de génie logiciel auront lieu en classe les vendredis à 13h30
au local PLT-2551 (sauf exception).
Ces séminaires sont publics et tous peuvent y assister. Si vous désirez donner un séminaire, veuillez
contacter <a href="mailto:claude-guy.quimper@ift.ulaval.ca?subject=Séminaire départemental">Claude-Guy Quimper</a>.
<p>Vous pouvez recevoir les annonces des séminaires par courriel en vous abonnant à la liste de diffusion SEMINAIRES-IFT sur le serveur de diffusion <a href="http://listes.ulaval.ca/cgi-bin/wa?SUBED1=SEMINAIRES-IFT&A=1%E2%80%8B%E2%80%8B">LISTSERV</a>.
  <!-- <p>Les séminaires de la session d'hiver 2019 sont <a href="https://www.ift.ulaval.ca/recherche-et-innovation/seminaires/seminaires-2019">archivés ici</a>. -->

<h2>Séminaires à venir</h2><div id="1422025"><h3>Vendredi 14 f&eacute;vrier 2025</h3><p style="text-align:left;"><b>Méta-modélisation, configuration et optimisation de bâtiments dans un contexte d'industrie 4.0</b><br>Manuel Chastenay<br><i>Étudiant à la maîtrise au Lab-Usine</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'industrie 4.0 révolutionne les usines manufacturières en substituant la production de masse par la personnalisation de produit de manière agile et dynamique.
Pilotée par des systèmes interconnectés et robotisés, cette transition offre des opportunités innovantes dans le milieu industriel.<p>
Cette présentation abordera ces nouvelles possibilités ainsi que certains défis rencontrés lors de leur implémentation chez une usine partenaire, notamment:
<ul style="margin-top:0px;">
<li>L'utilisation du métamodèle, pièce centrale de l'usine 4.0 répertoriant l'information ainsi que l'intelligence du produit. Ce dernier est utilisé et bonifié par les différents systèmes propres à l'usine afin d'offrir une meilleure cohésion inter-systèmes.</li>
<li>La configuration intelligente de produits par le client, régie par un système de contraintes préalablement défini par le manufacturier.</li>
<li>L'optimisation des processus et des matériaux de manière automatisée, pouvant être visualisée en temps réel à l'aide d'un jumeau numérique sur l'application Unity.</li>
</ul></resume></p></div><h2>Séminaires passés</h2><div id="2222013"><h3>Vendredi 22 f&eacute;vrier 2013</h3><p style="text-align:left;"><b>Modeling the data manifold with autoencoders and unnormalized probability density models</b><br>Pascal Vincent, Université de Montréal</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>The manifold hypothesis, according to which real-world high dimensional data likely concentrates near a lower dimensional non-linear manifold embedded in high dimensional input space, brings a fruitful geometric perspective to the problem of learning meaningful representations. It naturally arises from our intuitive notion of continuous underlying factors (e.g. pose and color parameters of an object in an image) that we would like to uncover. It also has the seductive potential to reduce the curse of dimensionality to a more manageable size. In this presentation I will explain my view on the best angle of attack for modeling data that is believed to have such a manifold support structure and how it has evolved, form simple local non-parametric  approaches, to implicit modeling of tangent spaces with regularized autoencoders and energy-based models. I will also showcase how  tangent space information extracted from such models can be leveraged e.g. to build better classifiers. From a geometric interpretation of familiar single layer representations as realizing manifold modeling through a patchwork of local linear tangent spaces, I will discuss the strengths, limits and potential of such representations in the quest towards building yet more meaning-carrying high-level ones. Finally I will propose modifications to established training criteria to explicitly incorporate a manifold inductive bias.</resume></p><p><b>Biographie: </b>Pascal Vincent est professeur agrégé au Département d'Informatique et de Recherche Opérationnelle de l'Université de Montréal. Son domaine de recherche est l'apprentissage machine. Depuis 2011, il est également membre associé du Canadian Institute for Advanced Research (CIfAR), dans le programme Neural Computation and Adaptive Perception. Pascal Vincent est diplômé d'une grande école d'ingénieurs Française (ESIEE Paris, 1996), et a complété un doctorat en informatique à l'Université de Montréal en 2004. Il a aussi eu l'occasion de travailler dans des laboratoires de recherche industriels renommés (Bell Labs, AT&T Labs Research, Microsoft Research), et a co-fondé et co-dirigé une entreprise dans le domaine du data-mining.
Son principal intérêt de recherche actuel est la découverte de principes et d'algorithmes qui, à partir de données représentant des perceptions sensorielles brutes, permettent de construire, couche par couche et de façon autonome, à la manière des réseaux de neurones du cerveau, des représentations de plus haut niveau porteuses de sens. Cela correspond à modéliser la structure de la réalité observée en y découvrant des régularités statistiques complexes.</p><p><b>Note: </b>Cette présentation sera donnée en français.</p><p><b>Lien</b>: <a href="files/2013_presentation_ULaval.pdf">présentation</a></p></div><div id="1522013"><h3>Vendredi 15 f&eacute;vrier 2013</h3><p style="text-align:left;"><b>Programmation par contraintes pour la planification de chemins sous incertitude: résoudre le problème du chemin de recherche optimal</b><br>Michael Morin</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le problème de chemin de recherche optimal (de l'anglais Optimal Search Path problem) est un problème de détection issu de la théorie de la recherche où la position et la détectabilité d'un objet mobile sont incertaines. Une solution à ce problème NP-difficile est un chemin sur les sommets d'un graphe visant à maximiser la probabilité de trouver l'objet. La connaissance sur la position de l'objet est représentée par une distribution de probabilité sur les sommets du graphe et varie dans le temps en fonction de son modèle de déplacement Markovien et des observations antérieures du ou des chercheurs. Nous présentons deux modèles de programmation par contraintes développés pour le cas à un seul chercheur ainsi qu'une nouvelle heuristique de branchement performante basée sur la probabilité totale de détection. Le problème de l'OSP est particulièrement intéressant en ce qu'il généralise plusieurs problèmes de recherche probabiliste tels que la détection d'intrusions, l'identification de code malveillant dans les réseaux distribués, la recherche sauvetage et la surveillance.</resume></p><p><b>Liens</b>: <a href="http://www.ift.ulaval.ca/~quimper/publications/osp.pdf">article</a>, <a href="files/mm2013IftLaval.pdf">présentation</a></p></div><div id="822013"><h3>Vendredi 8 f&eacute;vrier 2013</h3><p style="text-align:left;"><b>Learning a peptide-protein binding affinity predictor with kernel ridge regression</b><br>Sébastien Giguère</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>We propose a specialized string kernel for small bio-molecules, peptides and pseudo-sequences of binding interfaces. The kernel incorporates physico-chemical properties of amino acids and elegantly generalize eight kernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the Radial Basis Function. We provide a low complexity dynamic programming algorithm for the exact computation of the kernel and a linear time algorithm for it's approximation. Combined with kernel ridge regression and SupCK, a novel binding pocket kernel, the proposed kernel yields biologically relevant and good prediction accuracy on the PepX database. For the first time, a machine learning predictor is capable of accurately predicting the binding affinity of any peptide to any protein. The method was also applied to both single-target and pan-specific Major Histocompatibility Complex class II benchmark datasets and three Quantitative Structure Affinity Model benchmark datasets. 
On all benchmarks, our method significantly (p-value &lt; 0.057) outperforms the current state-of-the-art methods at predicting peptide-protein binding affinities. The proposed approach is flexible and can be applied to predict any quantitative biological activity. The method should be of value to a large segment of the research community with the potential to accelerate peptide-based drug and vaccine development.</resume></p><p><b>Note: </b>La présentation sera en français. Aucune connaissance en biologie, en biochimie ou en immunologie n'est requise pour comprendre la présentation, je présenterai tous les concepts nécessaire à la compréhension de nos travaux. Il s'agit de la méthode qui nous a mérité le premier prix lors du 2nd Machine Learning Competition in Immunology 2012.</p><p><b>Liens</b>: <a href="http://arxiv.org/abs/1207.7253">article</a>, <a href="files/BindingAffinityRegression.pdf">présentation</a></p></div><div id="122013"><h3>Vendredi 1<sup>er</sup> f&eacute;vrier 2013</h3><p style="text-align:left;"><b>I see you, you see me: Cooperative Localization through Bearing-Only Mutually Observing Robots</b><br>Philippe Giguère</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Plusieurs environnements ne permettent pas l'utilisation de signaux GPS pour la localisation : milieu sous-marin, à l'intérieur d'édifices ou tout simplement sur d'autres planètes. La localisation coopérative, qui se base sur l'estimation des positions relatives entre les membres d'un essaim de robots mobiles, est l'une des approches proposées pour faciliter la localisation globale de ces membres. Ce séminaire propose une nouvelle approche de localisation relative, basée sur l'utilisation de marqueurs lumineux et d'une caméra par robot. Cette technique peu coûteuse nécessite deux conditions essentielles : la prise synchronisée de photos, et l'observation mutuelle des robots et de leur caméra. Le séminaire présentera la solution géométrique analytique pour le problème minimal en 2D, comprenant une paire de robots. Au passage seront introduits certains principes de base de la localisation par caméra. Des comparaisons avec des méthodes classiques de localisation (caméras stéréo, utilisation d'une seule caméra et de nombreux marqueurs, capteurs lasers) seront présentées, ainsi que la validation expérimentale sur du matériel. Finalement, l'ébauche de la solution en 3D sera abordée.</resume></p><p><b>Liens</b>: <a href="http://www.ift.ulaval.ca/~pgiguere/papers/IROS2012MutualCamera.pdf">article</a>, <a href="files/philippegiguere.pdf">présentation</a></p></div><div id="2932013"><h3>Vendredi 29 mars 2013</h3><p style="text-align:left;"><b>Vendredi saint (congé)</b><br></p><p></p></div><div id="2232013"><h3>Vendredi 22 mars 2013</h3><p style="text-align:left;"><b>Comparaison d'algorithmes d'apprentissage et combinaison de modèles, une approche Bayesienne</b><br>Alexandre Lacoste</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Bien que le théorème du « no free lunch » nous indique que l'algorithme d'apprentissage optimal soit une utopie, nous proposons une approche pour déterminer si un algorithme est meilleur qu'on autre dans un contexte donné. Ceci est possible, car le théorème du « no free lunch » suppose une distribution uniforme sur toutes les tâches possibles. Dans notre approche, nous faisons autrement en supposant que les jeux de données observés proviennent d'une distribution quelconque sur l'ensemble des tâches. Avec uniquement cette supposition et quelques données de test, nous pouvons répondre de manière probabiliste à la question suivante : « Est-ce que l'algorithme A produira plus fréquemment un meilleur classificateur que l'algorithme B dans le contexte donné? » 

En appliquant cette même idée sur la validation croisée, nous obtenons une distribution a posteriori sur les différents hyperparamètres. À l'aide de ce postérieur, il nous est possible de faire de la marginalisation de modèles pour améliorer notre performance de généralisation. Plusieurs tests empiriques montrent que cette combinaison de modèles apporte un gain significatif et systématique.</resume></p><p><b>Lien</b>: <a href="files/presDepart_kile.pdf">présentation</a></p></div><div id="1532013"><h3>Vendredi 15 mars 2013</h3><p style="text-align:left;"><b>Semaine de lecture</b><br></p><p></p></div><div id="832013"><h3>Vendredi 8 mars 2013</h3><p style="text-align:left;"><b>La planification d'horaires de ligues sportives par programmation par contraintes</b><br>Gilles Pesant, École polytechnique de Montréal</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le sport professionnel soulève les passions populaires et anime de nombreux secteurs de l'économie. Un facteur méconnu du succès d'une ligue sportive est la planification du calendrier des matchs. Au-delà d'une structure combinatoire assez rigide, des considérations diverses font qu'il est difficile de produire un calendrier optimal (et souvent même d'exprimer ce qu'on doit optimiser) ou parfois simplement de trouver un calendrier respectant les différentes restrictions. On doit par exemple considérer l'équité entre les horaires individuels des équipes, la montée d'un suspense de fin de saison, les conflits potentiels avec l'horaire d'autres compétitions, l'intérêt suscité par certains matchs pour les revenus de diffusion et la sécurité des partisans lors de matchs à forte rivalité.<br><br>
La programmation par contraintes est une approche de représentation et de résolution de problèmes combinatoires qui est particulièrement bien adaptée au contexte des horaires de ligues sportives. Elle exploite les structures combinatoires d'un problème, exprimées sous la forme de contraintes, pour restreindre dynamiquement l'espace de recherche d'une solution et pour guider cette recherche. Or notre problème regorge de telles structures.<br><br>
Deux exemples, le premier étant une abstraction et le second très concret, serviront à illustrer la complexité du domaine et comment on peut produire de bons calendriers avec la programmation par contraintes.</resume></p><p><b>Lien</b>: <a href="files/sportSched.pdf">présentation</a></p></div><div id="132013"><h3>Vendredi 1<sup>er</sup> mars 2013</h3><p style="text-align:left;"><b>Assemblage de novo distribué de génomes avec Ray et RayPlatform: des graphes de Bruijn aux polytopes</b><br>Sébastien Boisvert</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Deux concepts intéressants - la granularité et le passage de messages - permettent d'architecturer des systèmes distribués pour l'analyse de données, les cadriciels, ou encore la visualisation dans les nuages. Un système distribué est strictement composé de composantes orthogonales et indépendantes. La composition peut se faire à la construction ou à l'utilisation. Les composantes de ces systèmes distribués peuvent être liées par différents graphes. Les modules d'extension par dessus un coeur sont souvent liés dans un arbre. Les données génétiques peuvent être liées dans un graphe de Bruijn. Et les 4096 processus d'un calcul distribué peut être liés par la surface d'un polytope régulier convexe. Dans cette présentation, les graphes présents dans les systèmes Ray, RayPlatform, et Ray Cloud Browser seront présentés. L'importance de la granularité des opérations ainsi que le passage de messages entre les composantes sera soulignée.</resume></p><p><b>Liens</b>: <a href="files/ift-seminaire-2013-03-01.pdf">présentation</a>, <a href="http://genomebiology.com/2012/13/12/R122/abstract">article</a>, <a href="http://online.liebertpub.com/doi/abs/10.1089/cmb.2009.0238">article</a>, <a href="http://browser.cloud.boisvert.info/client">démo</a>, <a href="http://github.com/sebhtml">code</a></p></div><div id="1942013"><h3>Vendredi 19 avril 2013</h3><p style="text-align:left;"><b>Quelles politiques de sécurités sont applicables par les Moniteurs?</b><br>Raphaël Khoury</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le monitorage (runtime monitoring) est une méthode bien établie pour assurer la sûreté du code et la littérature sur le sujet fait état de plusieurs implémentations de moniteurs.  Ceux-ci diffèrent cependant quant aux politiques de sécurité qu'ils sont capables d'appliquer.  Dans cette présentation, j'examine la question de déterminer exactement quelles politiques de sécurité peuvent être appliquées par les moniteurs.  Il s'agit des politiques pour lesquelles un moniteur qui opère sous certaines contraintes peut assurer qu'une violation ne se produit pas tout au long de l'exécution du programme. Cette question, en apparence simple, n'admet pas une réponse aisée et a été le sujet de plusieurs études académiques.  Pourtant, la question de déterminer dans quel contexte une politique de sécurité donnée est applicable est centrale à la sélection  et même la conception des mécanismes d'application.  J'examine les différentes réponses qui ont été suggérées à cet égard et j'identifie les problèmes qui demeurent sans réponse dans la littérature.</resume></p><p><b>Liens</b>: <a href="http://www.ift.ulaval.ca/~khoury/publications/EnforceablePropertiesSurvey.pdf">article</a>, <a href="files/raphaelkhoury.pdf">présentation</a></p></div><div id="1242013"><h3>Vendredi 12 avril 2013</h3><p style="text-align:left;"><b>Clustering and Non-negative Matrix Factorization</b><br>Mohammad Sajjad Ghaemi</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>In recent years, clustering has become a progressively important research topic in the machine learning field, pattern recognition area and several different disciplines such as data mining, bioinformatics, computer vision, page ranking, social network, etc. In this presentation I am going to talk about preliminary definition of cluster, conventional existing approaches that extract the clusters from the data with an emphasis on the Non-negative Matrix Factorization (NMF) as a new advancement in recent years to both clustering problem and dimension reduction task.

NMF has been investigated and applied for various purposes especially for high-dimensional data with elements containing non-negative values. The ultimate goal of the NMF is to provide a non-negative low-rank approximation of the original data. Directly imposing the non-negativity constraints on both factors and reconstruction co-efficients results in finding a lower rank approximation of the data that is often more meaningful and interpretable than other similar low-rank estimations such as SVD with even lesser reconstruction error. In other words, due to the NP-hardness of the clustering analysis as a discrete optimization problem, NMF based low-rank approximation gives a relaxation for clustering with equivalent variants to K-means family and spectral clustering.</resume></p><p><b>Note: </b>Cette présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="files/Mohammad_Seminar.pdf">Présentation</a></p></div><div id="542013"><h3>Vendredi 5 avril 2013</h3><p style="text-align:left;"><b>L'adaptation de domaine en apprentissage automatique: introduction et approche PAC-bayesienne</b><br>Pascal Germain</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>En apprentissage automatique, le problème d'adaptation de domaine consiste à entraîner un classificateur sur une distribution de données qui diffère (plus ou moins légèrement) de la distribution des données sur laquelle le classificateur devra effectuer une prédiction. Voici un exemple d'un tel problème: on possède deux ensembles de textes critiques concernant des livres et des films. Chaque critique de livre est accompagnée d'une appréciation (une côte numérique), mais on ne possède pas l'appréciation associée aux critiques de films. À partir de ces données, on désire entraîner un classificateur à prédire 
l'appréciation d'un film à partir de sa critique. Lors de l'apprentissage, il faudra non seulement minimiser l'erreur sur la classification de livres, mais aussi "s'adapter" à la tâche de classification de film. Pour ce faire, il faut analyser la divergence entre les distributions de critiques de livres et de critiques de films. Lors de la présentation, j'introduirai une analyse de l'adaptation de domaines inspirée de la théorie PAC-Bayésienne et je présenterai un algorithme d'apprentissage que nous avons développé à partir de cette analyse théorique.
</resume></p><p><b>Lien</b>: <a href="files/adaptation_de_domaine.pdf">Présentation</a></p></div><div id="2642013"><h3>Vendredi 26 avril 2013</h3><p style="text-align:left;"><b>Apprentissage de structure de modèle graphique probabiliste avec des graphes dirigés acycliques infinis</b><br>Patrick Dallaire</p><p><b>Heure</b>: 12h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'une des tâches fondamentales de l'apprentissage automatique est de découvrir de la structure dans les données, un processus pouvant s'avérer difficile. Lorsqu'il est question de modèles graphiques, ce processus pourrait être d'apprendre la structure de dépendance entre les variables aléatoires pour ensuite la décrire sous forme de graphe. Nous nous intéresserons à un processus stochastique définissant une distribution de probabilité dans l'espace des graphes dirigés acycliques, lequel peut être utilisé dans un cadre bayésien à des fins d'apprentissage de structure. L'un des avantages de ce processus est qu'il permet de considérer des structures ayant un nombre illimité de variables cachées, et ce, tout en pénalisant la complexité du modèle. Le processus stochastique en question est une extension du processus du buffet indien en cascade dont le support ne se limite plus uniquement à des structures stratifiées, mais permet aux connexions d'outrepasser plusieurs couches pour en rejoindre une autre. Les performances du processus étendu ont été évaluées sur des problèmes d'estimation de densité à plusieurs variables et montre que notre approche est capable d'apprendre des fonctions de densités reposant sur des structures plus simple. </resume></p><p><b>Lien</b>: <a href="files/sr.pdf">Présentation</a></p></div><div id="2792013"><h3>Vendredi 27 septembre 2013</h3><p style="text-align:left;"><b>On Geometric Spanners</b><br>Prosenjit Bose, School of Computer Science, Carleton University</p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume> A geometric graph G is a graph whose vertices are points in the plane and whose edges are line segments weighted by the Euclidean distance between their endpoints. Geometric graphs are often used to model different networks such as road networks, or communication networks. An important property to strive for in a network is connectivity. The most connected network is the compete network where every pair of points is connected. However, this is problematic since connecting every pair of points is expensive and results in too many connections. As such, to address this, one tries to build networks that approximate the complete network. What does it mean to approximate? This brings us to the notion of a t-spanner.<p>
In this setting, a t-spanner of G is a spanning subgraph G' with the property that for every pair of vertices x, y, the shortest path from x to y in G' has weight at most t times the shortest path from x to y in G. The parameter t is commonly referred to as the spanning ratio, or the stretch factor. In addition to having bounded spanning ratio, it is desireable to build t-spanners that possess other properties, such as bounded degree, low weight, small spanning ratio, planarity, high connectivity and/or fault-tolerance to name a few. Notice that many of these properties are competing in the sense that achieving them simultaneously is not possible. In this talk, we review various results on how to build spanners.</resume></p><p><b>Note: </b>Cette présentation sera donnée en français avec support visuel en anglais.</p></div><div id="2792013"><h3>Vendredi 27 septembre 2013</h3><p style="text-align:left;"><b>Chercher à l'aveuglette</b><br>Jean-Lou De Carufel, School of Computer Science, Carleton University</p><p><b>Heure</b>: 12h30<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume>Le titre est une traduction volontairement maladroite du problème intitulé "Searching on a Line". Dans ce problème, un robot se déplace sur une droite à la recherche d'un objet déposé à un endroit inconnu. Pour trouver l'objet, le robot doit passer par le point où se trouve cet objet. Quelle est la stratégie optimale pour le robot ?<p>
 Supposons qu'au départ, la distance entre le robot et l'objet est de D. Dans ce type de problèmes de recherche, on utilise habituellement D comme unité de mesure. Par exemple, si le robot connaît D, sa stratégie optimale coûte 3D en pire cas. En effet, dans le pire des cas, il parcourt D en s'éloignant de l'objet, revient à son point de départ puis se rend jusqu'à l'objet dans le sens opposé. (On suppose que le robot est capable d'évaluer les distances.)<p>
 Si le robot ne connaît pas D, on peut montrer que sa stratégie optimale coûte 9D en pire cas [Baeza-Yates et al., 1993]. Si on ne connaît pas D, mais qu'on connaît une borne supérieure sur D, quelle est la stratégie optimale ? Nous ferons un survol des résultats les plus récents à propos de ce dernier problème. Nous expliquerons aussi comment ces résultats permettent de concevoir des algorithmes de routage dans des graphes plus complexes qu'une droite.<p>
</resume></p></div><div id="22112013"><h3>Vendredi 22 novembre 2013</h3><p style="text-align:left;"><b>Formal Probabilistic Analysis using Theorem Proving</b><br>Dr. Sofiene Tahar, Professor & Tier 1 Research Chair, ECE Department, Concordia University</p><p><b>Heure</b>: 10h00<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume>Probabilistic analysis is a tool of fundamental importance to virtually all scientists and engineers as they often have to deal with systems that exhibit random or unpredictable elements. Traditionally, computer simulation techniques are used to perform probabilistic analysis. However, they provide less accurate results and cannot handle large-scale problems due to their enormous computer processing time requirements. To overcome these limitations, we propose to perform formal probabilistic and statistical analysis using higher-order logic theorem proving. We provide a framework for the formalization of both discrete and continuous random variables and the ability to formally verify system's probabilistic and statistical properties. The analysis carried out in this way is free from any approximation or precision issues due to the mathematical nature of the models and the inherent soundness of the theorem proving approach. In order to illustrate the practical effectiveness of the proposed framework, we present the probabilistic analysis of four examples across four application areas: the Coupon Collector's problem (software), the Stop-and-Wait protocol (telecommunications), the reliability of memory arrays (microelectronics), and floating-point error analysis (computer hardware).</resume></p><p><b>Biographie: </b>Sofiene Tahar is Professor in the Department of Electrical and Computer Engineering at Concordia University, Montreal, Quebec, Canada. He received his PhD in Computer Science in 1994 from the University of Karlsruhe and his Diploma in Computer Engineering in 1990 from the University of Darmstadt. Prof. Tahar has made contributions and published papers in the areas of formal hardware verification, microprocessor and system-on-chip verification, analog and mixed signal circuits verification, VLSI design automation, and formal probabilistic, statistical and reliability analysis of systems. Prof. Tahar is founder and director of the <a href="http://hvg.ece.concordia.ca">Hardware Verification Group</a>. Since 2007 he is holding a Senior Research Chair in Formal Verification of System-on-Chip.</p></div><div id="13122013"><h3>Vendredi 13 d&eacute;cembre 2013</h3><p style="text-align:left;"><b>Dynamique de la Topologie de l'Internet. Outil de Mesure et analyse préliminaire</b><br>Frédéric Tounwendyam Ouédraogo, Université de Koudougou, Burkina Faso</p><p><b>Heure</b>: 10h00<br><b>Local</b>: PLT-2548</p><p><b>Résumé: </b><resume>Le réseau Internet  est une structure de grande taille qui évolue très rapidement ce qui rend complexe l'étude de la dynamique de sa topologie. Nous proposons de se focaliser sur ce qu'une machine peut observer de cette topologie, appellé vue ego-centrée. En faisant des mesures périodiques de vues ego-centrées, que nous  apellons mesures radar, nous pouvons observer la dynamique de l'Internet. Nous avons conçu et implémenté un outil de mesure ego-centrée appelé tracetree qui fait des mesures de type radar. A l'aide de cet outil, nous avons conduit plusieurs mesures à partir d'une centaine de machines et obtenu des données. Les premières analyses des données ont permis de montrer des propriétés importantes de la dynamique de l'Internet.</resume></p><p><b>Biographie: </b>Frédéric Ouédraogo a obtenu un doctorat en informatique à l'Université Pierre et Marie Curie (Paris 6), en 2009. Il a publié plusieurs articles dans des conférences et des revues internationales. Depuis 2006, il enseigne à l'Université de Koudougou et y occupe maintenant un poste permanent. En plus de son enseignement et de sa recherche, il intervient dans la mise en place des technologies d'information et de communication sur le campus de l'université.</p></div><div id="3112014"><h3>Vendredi 31 janvier 2014</h3><p style="text-align:left;"><b>Un algorithme hybride pour la planification de chemins couvrants avec capteurs imparfaits</b><br>Michael Morin</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nous nous intéressons aux problèmes de couverture avec capteurs imparfaits dans le contexte des opérations de déminage robotisées. Dans le problème étudié, un véhicule autonome sous-marin (AUV) équipé d'un sonar balaye le fond marin à la recherche de mines. Dans notre formalisme, le fond marin est discrétisé par une grille uniforme de cellules carrées et un nombre constant de cellules est balayé par le capteur de chaque côté du robot au fil de ses mouvements. La probabilité de détecter une mine lors du balayage d'une cellule distante est fonction de deux paramètres : la distance entre la position du robot et la cellule balayée ainsi que le type de fond marin de la cellule balayée. L'objectif est de trouver le chemin de longueur minimale avec le moins de virages possible atteignant la couverture minimale requise dans chacune des cellules (définie en termes de probabilité de détection). Nous proposons un algorithme hybride basé sur la programmation dynamique qui utilise une réduction vers le problème du voyageur de commerce (TSP). Par nos résultats expérimentaux, nous démontrons l'efficacité de l'algorithme en termes de qualité de la solution et de temps de calcul par rapport aux résultats précédemment publiés rendant ainsi l'algorithme utilisable dans un contexte pratique.</resume></p><p><b>Liens</b>: <a href="http://www.ift.ulaval.ca/~quimper/publications/root.pdf">article</a>, <a href="http://www.michaelmorin.info/publications/mm2014IftLaval.pdf">présentation</a></p></div><div id="2412014"><h3>Vendredi 24 janvier 2014</h3><p style="text-align:left;"><b>Une approche hybride combinant analyse statique et dynamique pour l'application de la politique de flot d'information sécuritaire</b><br>Andrew Bedford, Josée Desharnais, Théophane Gloria Godonou et Nadia Tawbi</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Cette présentation porte sur une approche d'application de politiques de sécurité utilisant une analyse multi-valuée  basée sur  un système de types. Cette analyse est suivie d'une instrumentation lorsque nécessaire. Le langage cible est un langage impératif. Notre approche vise à réduire les faux positifs générés par une analyse statique, et à réduire la surcharge d'exécution en n'instrumentant que lorsque nécessaire. Les faux-positifs surviennent dans l'analyse statique de systèmes informatiques lorsqu'il est impossible ou difficile de décider, souvent  à cause d'une information manquante, par exemple le nom d'un fichier, et par conséquent, son niveau de sécurité. L'idée principale de notre approche est de distinguer les réponses négatives des réponses incertaines. Au lieu de rejeter les commandes potentiellement non sécuritaires, elles sont identifiées et étiquetées pour la seconde phase de l'analyse. Les réponses négatives et positives sont traitées comme cela se fait d'habitude. Pour augmenter la précision de l,analyse nous avons opté pour une  analyse est sensible au flot. Ainsi, une variable a le niveau de sécurité de l'information qu'elle contient. Seules les sources et les cibles d'information, les canaux, ont des niveaux de sécurité fixés a priori. Le résultat est une analyse hybride d'application de politique de sécurité : les points potentiellement sécuritaires du programme détectés par notre analyse par typage sont instrumentés par la suite avec des tests dynamiques.  Cela garantit moins de faux positifs et moins de surcharge à l'exécution.</resume></p><p><b>Liens</b>: <a href="files/PresentationULJanvier2014.pdf">présentation</a>, <a href="files/HybridFlowAnalysiFinal.pdf">article</a></p></div><div id="2822014"><h3>Vendredi 28 f&eacute;vrier 2014</h3><p style="text-align:left;"><b>Towards a Regression using Tensors</b><br>Hou Ming</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nowadays, more and more data of interest such as colored images and videos admits a natural tensorial representation. On one hand, the data represented by high order tensor promises to improve the generalization of machine learning problem by incorporating some structural prior information. On the other hand, such structured representation also poses great challenges for many classical machine learning problems including regression.<p>
 In this presentation, I will focus on the class of regression problems where the independent predictors take the form of tensor representation, namely, tensor regression. In contrast to classical regression approaches, the new problem bring the new challenges mainly in two aspects. First, the ultrahigh dimensionality of tensor input results in gigantic number of parameters; Furthermore, the complex multi-way structure embedding in the tensorial data is difficult to capture.<p>
 To deal with these challenges, I will briefly talk about the linear tensor regression by exploiting the well-studied tensor decomposition approaches. I will also talk about some typical regularization techniques tailored to tensorial format in order to improve the predictability of the regression model.</resume></p><p><b>Note: </b>Cette présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="files/seminar2_28.pdf">présentation</a></p></div><div id="2122014"><h3>Vendredi 21 f&eacute;vrier 2014</h3><p style="text-align:left;"><b>Parallel Discrepancy-Based Search</b><br>Thierry Moisan</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Les stratégies de recherche basées sur le calcul de divergences se sont démontrées très utiles pour résoudre des problèmes combinatoires de grande taille. Elles démontrent de très bonnes performances lorsqu'elles sont utilisées en combinaison avec des heuristiques de branchement spécifiques au domaine du problème à résoudre (heuristique de choix de variable et de valeur). De telles heuristiques existent pour de nombreux problèmes industriels. Nous proposons une nouvelle approche, Parallel Limited Discrepancy-depth based Search (PLDS), qui permet de paralléliser une stratégie de recherche basée sur les divergences. L'ensemble de processeurs visite les feuilles de l'arbre de recherche exactement dans le même ordre que le ferait l'algorithme centralisé. Cela est fait en assignant implicitement un seul processeur à chaque feuille de l'arbre de recherche. Cette implémentation permet un équilibre naturel de la charge de travail. En effet, il est possible de borner théoriquement les changements de la charge de travail entre les processeurs lorsqu'un filtrage est effectué sur le domaine des variables. De plus, aucune communication n'existe entre les processeurs. Les propriétés de PLDS en font un algorithme qui peut être déployé sur des superordinateurs massivement parallèles comprenant plusieurs milliers de processeurs, tels que Colosse de l'Université Laval.<p>
Nous avons appliqué cet algorithme à des problèmes industriels de rabotage du bois grâce à des jeux de données provenant de compagnies forestières québécoises. Cela nous a permis d'obtenir des solutions de meilleure qualité que les meilleures connues jusqu'à maintenant.</resume></p><p><b>Note: </b>Thierry a remporté le prix du meilleur article à CP 2013.</p><p><b>Liens</b>: <a href="files/ThierryMoisan.pdf">présentation</a>, <a href="http://www.ift.ulaval.ca/~quimper/publications/pldscp2013.pdf">article</a></p></div><div id="1422014"><h3>Vendredi 14 f&eacute;vrier 2014</h3><p style="text-align:left;"><b>Introduction aux algorithmes MapReduce</b><br>Mathieu Dumoulin</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le monde du big data prend de plus en plus de place dans l'actualité technologique. La plate-forme distribuée Hadoop est au coeur de ce mouvement. Hadoop a récemment fait son entrée à l'Université Laval, étant maintenant disponible sur Colosse (en test).<p>

Au-delà du «&nbsp;hype&nbsp;», il y a bien une classe de problèmes qui justifient d'utiliser Hadoop. Lorsque confronté à un tel problème, il devient nécessaire de formuler une solution au problème pour qu'elle puisse fonctionner sur Hadoop. Hadoop est basé sur le modèle de programmation fonctionnel MapReduce. Comment formuler une solution MapReduce? À quoi ressemble un algorithme MapReduce au-delà de l'omniprésent WordCount?<p>

Afin de répondre à ce genre de questions, je vais présenter MapReduce d'un point de vue algorithmique. J'entends aller plus en profondeur que les nombreuses "introductions à Hadoop" pour bien expliquer comment un algorithme peut être formulé "façon MapReduce". Divers algorithmes seront vus comme le tri, le calcul de moyenne et des algorithmes de graphes comme PageRank.<p>

Aucune connaissance préalable de Hadoop ou MapReduce ne sera nécessaire. Ce ne sera pas une présentation Hadoop mais bien sur l'algorithme MapReduce.</resume></p><p><b>Lien</b>: <a href="files/Introduction_aux_algorithmes_MapReduce.pdf">présentation</a></p></div><div id="722014"><h3>Vendredi 7 f&eacute;vrier 2014</h3><p style="text-align:left;"><b>Algorithmes d'apprentissage et bornes sur le risque pour l'approche de la régression à la prédiction de structures</b><br>Mario Marchand</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nous présentons des garanties rigoureuses sur l'approche de la régression à la prédiction de sorties structurées. Nous montrons que le risque quadratique (de régression) borne supérieurement le risque de prédiction lorsque le noyau de sortie vérifie certaines conditions relativement au risque de prédiction. Nous proposons deux bornes sur le risque de prédiction qui dépendent du risque empirique quadratique. Le minimiseur de la première borne se trouve à être le prédicteur proposé par Cortes et al (2007). Le minimiseur de la deuxième borne est un prédicteur proposé pour la première fois. Ces deux prédicteurs sont comparés sur la reconnaissance de mots manuscrits et la classification hiérarchique d'enzymes.</resume></p><p><b>Lien</b>: <a href="files/PresentationSteEtienne.pdf">présentation</a></p></div><div id="2132014"><h3>Vendredi 21 mars 2014</h3><p style="text-align:left;"><b>Prédiction du mouvement humain au moyen d'un modèle dynamique basé sur les processus gaussiens</b><br>Yali Wang</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La prédiction du mouvement humain est un défi pour la tâche de vision par ordinateur. Tout d'abord, les données relatives à ce mouvement  sont généralement de hautes dimensions. Ensuite, les mouvements plus complexes sont souvent multi-modaux. Dans ce contexte, un modèle dynamique basé sur les processus gaussiens (GPDM) a été proposé pour le suivi du mouvement humain [J. M. Wang, D. J. Fleet and A. Hertzmann, NIPS 2005]. Dans GPDM, les processus gaussien (GPS) sont utilisés pour construire le modèle à la fois au niveau des transitions et au niveau de la vraisemblance. Dès lors, les états latents et les hyper-paramètres peuvent être efficacement tirés de la forme fermée au niveau du postérieur en utilisant une forme de gradient pour l'optimisation. Dans cette présentation, je me concentrerai principalement sur ce modèle GPDM, et je proposerai quelques extensions possibles.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>anglais</b>.</p><p><b>Lien</b>: <a href="files/YaliWang.pdf">présentation</a></p></div><div id="1432014"><h3>Vendredi 14 mars 2014</h3><p style="text-align:left;"><b>Algorithmes de filtrage pour des problèmes d'ordonnancement disjonctif</b><br>Hamed Fahimi</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La programmation par contraintes permet de résoudre de nombreux problèmes d'optimisation combinatoire, dont les problèmes d'ordonnancement de tâches. Ces problèmes consistent à déterminer dans quel ordre et à quel moment les tâches doivent être exécutées afin que les temps de sortie, les échéances et les temps de traitements de chaque tâche soient respectés. De plus, nous cherchons souvent à minimiser le « makespan », c'est-à-dire le moment de complétion de la dernière tâche. Les solveurs de contraintes procèdent à une exploration systématique de toutes les solutions candidates au problème. Afin d'accélérer le processus de recherche, des algorithmes de filtrage permettent d'éliminer des solutions partielles qui ne peuvent être complétées en solutions complètes. Nous présentons de nouveaux algorithmes de filtrage pour des problèmes d'ordonnancement.
    </resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>anglais</b></p><p><b>Lien</b>: <a href="files/Hamed.pdf">présentation</a></p></div><div id="2832014"><h3>Vendredi 28 mars 2014</h3><p style="text-align:left;"><b>Le Cloud Computing&nbsp;: modèle et évaluation de performances</b><br>Valérie-Danielle Justafort</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Depuis plusieurs années, nous assistons à l'essor d'un nouveau paradigme, le Cloud Computing. Ce dernier fait référence à l'utilisation de mémoires et de capacités d'énormes centres de traitement appelés Datacenters, dans le but de faciliter le stockage et le traitement d'un grand volume de données. Ces datacenters, essentiellement constitués de serveurs interconnectés et distants, peuvent être regroupés pour former un InterCloud, où les clients peuvent rapidement dimensionner des machines virtuelles (VMs) et y installer les applications voulues. Toutefois, malgré ces nombreux avantages, la gestion et le fonctionnement d'une telle infrastructure nécessitent de grandes quantités d'énergies électriques, ce qui, non seulement, entraîne des coûts d'opérations largement supérieurs aux coûts d'investissements, mais est également à l'origine d'émission de grandes quantités de gaz à effet de serre (GES) et particulièrement du CO<sub>2</sub>.<p>
Dans cette présentation, nous adressons le problème de placement des VMs dans un InterCloud de manière à réduire l'empreinte carbone du système. Plus précisément, nous proposons un nouveau modèle de formulation mathématique qui se base sur la programmation en nombres entiers mixtes (MIP) pour minimiser l'impact environnemental d'un tel système. Plusieurs simulations ont montré que le modèle proposé permet d'obtenir des configurations optimales avec une empreinte carbone minimale à l'échelle de l'InterCloud.</resume></p><p><b>Lien</b>: <a href="files/ValerieDJustafort.pdf">présentation</a></p></div><div id="2542014"><h3>Vendredi 25 avril 2014</h3><p style="text-align:left;"><b>Big Data, a statistician's perspective</b><br>Arthur Charpentier, professeur au département de mathématiques à l'UQAM</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>In this talk, I will get back on the "Big Data revolution". Big data is "high volume", "high velocity", and/or "high variety information assets" that require new forms of processing to enable enhanced decision making. Thus, this is a challenge for statisticians. As claimed by Alan Mitchell, "<i>Big data is all about statistics: divining patterns and trends from large data sets. Statistics are incredibly powerful and useful for the way they challenge the assumptions and inferences naturally made by human minds - many of them faulty. As I said, that's great</i>". I will discuss the epistemological challenge of big data, and the impact on statistical modeling. Several examples will also be discussed, based on some experiences in finance, insurance, and sports analytics.</resume></p><p><b>Note: </b>Cette présentation de sera donnée en <b>français</b>.</p><p><b>Liens</b>: <a href="http://freakonometrics.hypotheses.org">Freakonometrics</a>, <a href="http://freakonometrics.hypotheses.org/13121">présentation</a></p></div><div id="1142014"><h3>Vendredi 11 avril 2014</h3><p style="text-align:left;"><b>Balance : Une famille de contraintes</b><br>Émilie Picard-Cantin</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le balancement des charges de travail entre employés dans un horaire de travail et la répartition équilibrée des heures de cours dans l'horaire d'un étudiant sont de bons exemples de la nécessité d'une contrainte de balancement efficace en programmation par contraintes. Il existe déjà dans la littérature la contrainte <i>Balance</i> ayant pour objectif d'équilibrer des affectations. Nous avons montré que d'obtenir la cohérence de domaine sur cette contrainte est NP-difficle. C'est pourquoi nous proposons une nouvelle famille de contraintes. Pour chaque nouvelle contrainte, nous proposons une décomposition simple à encoder à l'aide d'un solveur standard. Nous proposons aussi un algorithme spécialisé de filtrage pour l'une de ces contraintes qui établit la cohérence de domaine en temps polynomial.
</resume></p><p><b>Lien</b>: <a href="files/presentation_balance.pdf">présentation</a></p></div><div id="1842014"><h3>Vendredi 18 avril 2014</h3><p style="text-align:left;"><b>Vendredi Saint (pas de séminaire)</b><br></p><p></p></div><div id="442014"><h3>Vendredi 4 avril 2014</h3><p style="text-align:left;"><b>Point and shoot sky probes</b><br>Jean-François Lalonde</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Image-based lighting has allowed the creation of photo-realistic computer-generated content. However, it requires specially designed calibration targets such as mirrored sphere or custom hardware, which may not be available to an average camera user. In this paper, we present an approach to directly estimate an HDR sky probe from a single LDR photograph of an outdoor scene shot with a consumer camera, without specialized calibration target or equipment. Our insight is that outdoor scenes contain several objects which can act as natural calibration targets. In this paper, we use LDR photographs of three such objects: landmarks, skies, and faces. To estimate the HDR sky probes from such objects, we propose a novel dataset of HDR sky probes, and employ it to learn the structure of outdoor illumination and its relationship to intensity variations across photographs of these objects. We use the learnt mapping to estimate sky probes directly from the LDR photographs, and show results of relighting objects inserted into them.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>français</b>.</p></div><div id="3052014"><h3>Vendredi 30 mai 2014</h3><p style="text-align:left;"><b>Applying Data Mining to Real-life Crime Investigation</b><br>Benjamin Fung</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Data mining has demonstrated a lot of success in many domains, from direct marking to bioinformatics. Yet, limited research has been conducted to leverage the power of data mining in real-life crime investigation. In this presentation, I will discuss two data mining methods for crime investigation. The first method aims at identifying the true author of a given anonymous e-mail. The second method is a subject-based search engine that can help investigators to retrieve criminal information from a large collection of textual documents.</resume></p><p><b>Biographie: </b>Dr. Benjamin Fung is an Associate Professor of Information Studies (SIS) at McGill University and a Research Scientist in the National Cyber-Forensics and Training Alliance Canada (NCFTA Canada). He received a Ph.D. degree in computing science from Simon Fraser University in 2007. Dr. Fung collaborates closely with the national defense, law enforcement, and healthcare sectors. He has over 70 refereed publications that span across the prestigious research forums of data mining, privacy protection, cyber forensics, services computing, and building engineering. His data mining works in crime investigation and authorship analysis have been reported by media worldwide. Before pursuing his academic career, he worked at SAP Business Objects for four years. He is a licensed professional engineer in software engineering.</p><p><b>Note: </b>Cette présentation sera donnée en <b>anglais</b>.</p><p><b>Lien</b>: <a href="http://dmas.lab.mcgill.ca/fung">Page web</a></p></div><div id="1652014"><h3>Vendredi 16 mai 2014</h3><p style="text-align:left;"><b>Robotique mobile et cartographie 3D dans un monde en mouvement</b><br>François Pomerleau</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La robotique mobile a pour mission de créer des robots capables d'opérer de façon autonome dans des environnements complexes et divers. La première partie de la présentation a pour but de donner une vue d'ensemble sur la diversité des types de déplacements avec des exemples de robots, développés à Zurich, fonctionnant sous l'eau, sur l'eau, sur le sol et dans l'air. La deuxième partie focalisera sur deux exemples de plateformes utilisées sur le terrain et présentera quelques résultats reliés aux tâches 1) de surveillance d'algues toxiques en milieux pre-alpin et alpin et 2) de navigation en environnements urbains hautement dynamiques.</resume></p><p><b>Biographie: </b>François Pomerleau a commencé ses premières implications en recherche supervisée par l'agence spatiale canadienne et européenne lors de ses études en génie informatique à l'Université de Sherbrooke. Il a obtenu sa maîtrise de la même université en 2009 après un séjour d'un an à l'EPFL (Lausanne - Suisse). Il a complété son doctorat à l'ETH Zurich (Suisse) en 2013 durant lequel il a participé à plusieurs déploiements robotiques en environnements non contrôlés. Il est présentement chercheur postdoctoral à l'Université Laval dans le groupe de recherche de Philippe Giguère. Ses intérêts de recherches incluent la reconstruction 3D d'environnements basée sur les données laser, les activités de recherches et sauvetage, la planification de trajectoires et la méthodologie scientifique appliquée à la robotique.</p></div><div id="2292014"><h3>Lundi 22 septembre 2014</h3><p style="text-align:left;"><b>The Neural Autoregressive Distribution Estimator</b><br>Hugo Larochelle</p><p><b>Heure</b>: 14h30<br><b>Local</b>: PLT-2505</p><p><b>Résumé: </b><resume>The problem of estimating the distribution of multivariate data is one of the most general problems addressed by machine learning research. Good models of multivariate distributions can be used to extract meaningful features from unlabeled data, to provide a good prior over some output space in a structured prediction task (e.g. image denoising) or to provide regularization cues to a supervised model (e.g. a neural network classifier).<p>

The most successful distribution/density models are usually based on graphical models, that represent the joint distribution p(x) of observations x using latent variables that explain the statistical structure within x. Mixture models are a well known example and rely on a latent representation with mutually exclusive components. For high dimensional data however, models that rely on a distributed representation, i.e. a representation where components can vary separately, are usually preferred. These include the very successful restricted Boltzmann machine (RBM) model. However, unlike mixture models, the RBM is not a tractable distribution estimator, in the sense that computing the probability of observations p(x) can only be computed up to a multiplicative constant and must be approximated.<p>

In this talk, I'll describe the Neural Autoregressive Distribution Estimator (NADE), which combines the tractability of mixture models with the modeling power of the RBM, by relying on distributed representations. The main idea in NADE is to use the probability product rule to decompose the joint distribution p(x) into the sequential product of its conditionals, and model all these conditionals using a single neural network.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>français</b>.</p></div><div id="14112014"><h3>Vendredi 14 novembre 2014</h3><p style="text-align:left;"><b>Flips</b><br>Prosenjit Bose</p><p><b>Heure</b>: 14h30<br><b>Local</b>: PLT-2700</p><p><b>Résumé: </b><resume>We review results concerning edge flips in triangulations concentrating mainly on various aspects of the following question: Given two different triangulations of the same size, how many edge flips are necessary and sufficient to transform one triangulation into the other? We focus both on the combinatorial perspective (where only a combinatorial embedding of the graph is specified) and the geometric perspective (where the graph is embedded in the plane, vertices are points and edges are straight-line segments). We highlight some of the techniques used to prove the main results and mention a few of the challenges remaining in this area.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>français</b>.</p></div><div id="14112014"><h3>Vendredi 14 novembre 2014</h3><p style="text-align:left;"><b>Routage géométrique dans les triangulations de Delaunay</b><br>Jean-Lou De Carufel</p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2700</p><p><b>Résumé: </b><resume>Un robot se déplace dans la ville de Québec à la recherche du campus de l'Université Laval. Il connaît les coordonnées GPS du campus, mais il n'a pas accès à la carte de la ville. Lorsqu'il se trouve à une intersection, il ne voit que les intersections adjacentes. De plus, le robot est sans mémoire: lorsqu'il arrive à une intersection, il oublie tout ce qui s'est passé au préalable. (Il ne retient que les coordonnées de l'université.) Notre objectif est de construire un algorithme de routage qui minimise la distance que le robot doit marcher pour atteindre sa destination.

Lorsqu'un réseau est plongé dans le plan ou dans l'espace, il satisfait certaines propriétés géométriques qui peuvent être mises à profit pour concevoir des algorithmes de routage efficaces. Nous étudierons le cas des triangulations de Delaunay. Supposons qu'au départ la distance euclidienne entre le robot et sa destination est de D. Le meilleur algorithme connu jusqu'à tout récemment pour faire du routage sur les triangulations de Delaunay construisait un chemin de longueur 45 D. Nous présenterons un algorithme de routage qui mène à un chemin de longueur 15 D. De plus, nous expliquerons comment nous pensons être capable d'abaisser la longueur du chemin à moins de 6 D.</resume></p></div><div id="3012015"><h3>Vendredi 30 janvier 2015</h3><p style="text-align:left;"><b>Analyse des études génétiques familiales : établir le lien variants rares - maladie et intégrer l'expression des gènes</b><br>Alexandre Bureau</p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Les échantillons familiaux présentent d'importants avantages pour étudier les variants génétiques impliqués dans les maladies complexes. Dans un premier volet, j'expliquerai comment l'observation du partage de variants génétiques rares par des malades apparentés peut servir à lier ces variants à la maladie étudiée. Je présenterai des expressions pour calculer des probabilités exactes de partage de variants génétiques rare parmi des sujets apparentés, et leur implantation dans un module pour l'environnement statistique R. Cette approche sera comparée à des alternatives impliquant un groupe témoin de la population. Dans un deuxième volet, je présenterai la problématique d'intégrer des mesures d'expression des gènes dans une analyse d'association entre une maladie et des variants génétiques fréquents, en soulignant les difficultés posées par la structure familiale. Une approche prometteuse qui pourrait être étendue au contexte des études familiales sera présentée.</resume></p><p><b>Biographie: </b>Professeur au département de médecine sociale et préventive. Voir sa <a href="http://www.crulrg.ulaval.ca/pages_perso_chercheurs/bureau_a/index.html">page personnelle</a> et son <a href="http://www.crulrg.ulaval.ca/fr/recherche/axes/chercheurs/chercheur.php?id=47&axe=1">profil de recherche</a>.</p><p><b>Lien</b>: <a href="files/AlexandreBureau.pdf">diapositives</a></p></div><div id="2312015"><h3>Vendredi 23 janvier 2015</h3><p style="text-align:left;"><b>Un réseau de neurones pour l'adaptation de domaine</b><br>Pascal Germain</p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>L'apprentissage automatique s'intéresse à la création de programmes qui apprennent à effectuer une tâche par l'observation d'exemples. En pratique, un algorithme d'apprentissage entraîne un classificateur par l'étude d'une série d'éléments déjà classés (par exemple, des textes de critiques de films associés à une cote de 1 à 5 étoiles). Ensuite, on utilise le classificateur obtenu afin de connaître la classe d'un nouvel élément (on demande au classificateur de prédire la cote que donnera un utilisateur à un film en se basant sur le texte de sa critique).  
Plus spécifiquement, le problème d'adaptation de domaine consiste à entraîner un classificateur sur une distribution de données source qui diffère (plus ou moins légèrement) de la distribution des données cible sur laquelle le classificateur devra effectuer une prédiction.  Voici un exemple d'un tel problème: on possède deux ensembles de textes critiques concernant des films (le domaine source) et des livres (le domaine cible). Chaque critique de film est accompagnée d'une cote, mais on ne possède pas la cote associée aux critiques de livres. À partir de ces données, on désire entraîner un classificateur à prédire la cote d'un livre à partir de sa critique. Lors de l'apprentissage, il faudra non seulement minimiser l'erreur sur la classification de livres, mais aussi "s'adapter" à la tâche de classification de films. 
Les travaux présentés se basent  sur une théorie de l'adaptation de domaine qui affirme qu'une famille de classificateurs permet de s'adapter au domaine cible si elle:
(1) permet de bien classifier le domaine source; et
(2) n'est pas capable de distinguer les exemples sources des exemples cibles.
Nous intégrons cette idée à un algorithme d'apprentissage du type réseau de neurones.  Notre algorithme trouve une nouvelle représentation des exemples d'apprentissage pour laquelle le réseau de neurones possède les caractériques (1) et (2).</resume></p><p><b>Lien</b>: <a href="files/seminaire_23janvier2015_handout.pdf">diapositives</a></p></div><div id="1612015"><h3>Vendredi 16 janvier 2015</h3><p style="text-align:left;"><b>Compréhension phonétique du texte écrit</b><br>Richard Khoury</p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Cette présentation détaille le développement de deux algorithmes de compréhension phonétique du texte écrit. Étant donné un nouveau mot, l'objectif de ces algorithmes est de déterminer la prononciation approximative. Le premier algorithme utilise une approche probabiliste, et le second utilise un ensemble de règles de décision. Deux domaines d'applications sont également présentés, soit la normalisation de mots mal épelés dans les microtexts envoyés par cellulaire, Facebook, Twitter et autre, et l'analyse de la littérature médiévale.</resume></p><p><b>Lien</b>: <a href="files/Phonetique.pdf">diapositives</a></p></div><div id="2022015"><h3>Vendredi 20 f&eacute;vrier 2015</h3><p style="text-align:left;"><b>A progress-sensitive flow-sensitive inlined information-flow control monitor</b><br>Andrew Bedford<br><i>Étudiant au doctorat au LSFM</i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>We increasingly rely on computer systems to safeguard confidential information, and maintain the integrity of critical data and operations. But in our highly interconnected world, these trusted systems often need to communicate  with untrusted parties. Trusted systems risk leaking confidential information to untrusted parties, or allowing input from untrusted parties to corrupt data or the operation of the trusted system.<p>

Information-flow control is a promising approach that enable trusted systems to interact with untrusted parties, providing fine-grained application-specific control of confidential and untrusted information. Static mechanisms for information-flow control analyse a program before execution to determine whether the program's execution will satisfy the appropriate information flow requirements. This has low runtime overhead, but accepts or rejects a program in its entirety. Dynamic mechanisms accept or reject individual executions at runtime, without performing any static program analysis. Dynamic mechanisms can incur significant runtime overheads. Hybrid information-flow control techniques combine static and dynamic program analysis and strive to achieve the benefits of both static and dynamic approaches: precise (i.e., per-execution) enforcement of security, with low runtime overhead.<p>

    After introducing the concept of information-flow control (what? why? how?), I will present a progress-sensitive, flow-sensitive hybrid information-flow control monitor for an imperative interactive language.<p></resume></p><p><b>Note: </b>La présentation sera donnée en français.</p><p><b>Lien</b>: <a href="files/AndrewBedford2015.pdf">diapositives</a></p></div><div id="1322015"><h3>Vendredi 13 f&eacute;vrier 2015</h3><p style="text-align:left;"><b>Un algorithme d'apprentissage parcimonieux maximisant le désaccord</b><br>Jean-Francis Roy</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>En apprentissage automatique, ou plus précisément lors de la résolution d'un problème de classification, un algorithme d'apprentissage a pour tâche de prédire la catégorie d'un nouvel élément observé après avoir été entraîné sur un ensemble d'éléments déjà classifiés. Les applications sont multiples : on pourrait par exemple s'intéresser à prédire si oui ou non un client s'intéressera à un produit en fonction de ses achats antérieurs, ou bien déterminer si un courriel est un «spam» ou non à partir de son contenu.

Plusieurs algorithmes d'apprentissage retournent la fonction de décision apprise sous la forme d'un vote de majorité pondéré de «votants» de base. Il est commun en pratique d'observer des votes de majorité dont la performance est très bonne, alors que les votants sont individuellement très peu performants. Nous présenterons une borne sur le risque du classificateur par vote de majorité expliquant ce phénomène en introduisant non seulement la performance individuelle des votants, mais aussi la corrélation de leurs erreurs (leur désaccord). Nous présenterons un programme quadratique nommé MinCq qui minimise directement cette borne, dont la performance est égale à l'état de l'art. Un désavantage de cette approche est que le vote de majorité obtenu est «dense», et donc difficile à interpréter par un être humain.

Nous présenterons comment utiliser la dualité de Lagrange et la technique d'optimisation nommée «génération de colonnes» pour obtenir une nouvelle version de MinCq. L'algorithme résultant, nommé CqBoost, est plus performant que les autres algorithmes obtenus par génération de colonnes, et retourne un vote de majorité parcimonieux plus facile à interpréter par un humain.</resume></p><p><b>Note: </b>Heure inhabituelle!</p><p><b>Lien</b>: <a href="files/presentation_cqboost.pdf">diapositives</a></p></div><div id="2722015"><h3>Vendredi 27 f&eacute;vrier 2015</h3><p style="text-align:left;"><b>Meilleures régressions quantiles grâce aux réseaux de neurones</b><br>Adam Salvail<br><i>Étudiant au doctorat à <a href="http://adam.salvail.net">l'Université de Sherbrooke</a></i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Supposons que nous ayons un phénomène mesurable (ex.: la température qu'il fera demain), et que nous souhaitons prédire cette valeur en n'utilisant que l'information présentement disponible (ex.: température actuelle, vents, etc.). Pour y arriver, nous utilisons un outil, appeler régression, qui tente de modéliser le phénomène à l'étude et de faire une prédiction la plus précise possible. En général, nous voulons que le modèle puisse nous donner l'espérance de la valeur à prédire, c'est-à-dire la valeur qui minimise la différence au carré entre notre prédiction et la réalité. Or, cette prédiction ne donne aucune information sur la certitude du modèle concernant la valeur prédite!<p>

Une autre possibilité intéressante est la régression quantile. Plutôt que de tenter de prédire l'espérance de la valeur recherchée, nous nous intéressons à prédire, par exemple, les quartiles de la distribution dont provient la valeur. De cette façon, il est plus facile de se faire une idée de la confiance que le modèle accorde à sa prédiction.<p>

Cette présentation montre comment effectuer cette régression quantile à l'aide de réseaux de neurones, une méthode plus puissante que la régression quantile linéaire généralement utilisée.<p>
</resume></p><p><b>Lien</b>: <a href="files/QRegNNet.pdf">diapositives</a></p></div><div id="622015"><h3>Vendredi 6 f&eacute;vrier 2015</h3><p style="text-align:left;"><b>Replanifier les opérations de production en minimisant les perturbations</b><br>Thierry Moisan</p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Dans un contexte de production manufacturière, il est courant d'avoir en main un plan de production prêt à être appliqué prochainement. Si une nouvelle commande arrive à la dernière minute, il peut être difficile de l'incorporer à ce plan sans le changer. Dans ce type de situation, l'approche typique est de replanifier l'ensemble de la production pour y inclure cette nouvelle demande. De plus, les commandes déjà garanties aux clients doivent demeurer dans le plan de production. Pour ce faire, les commandes garanties sont forcées dans cette replanification. Cette approche a l'inconvénient de créer un plan de production qui peut être radicalement différent du plan original. Cela peut créer des problèmes si son exécution a déjà débuté ou si des tâches préalables doivent être effectuées. Dans le contexte de l'industrie du bois, à l'étape de rabotage, le choix de la famille de bois traitée à chaque période a une grande influence sur la planification des opérations. Si des familles de bois différentes sont traitées dans deux quarts de travail subséquents, un changement de configuration de la machinerie doit être effectué entre ces quarts de travail. Ces changements de configurations sont coûteux en temps et financièrement, car des employés supplémentaires doivent être assignés à cette tâche. Si le plan de production change de façon drastique, la planification de ces changements de configuration doit aussi être modifiée ce qui perturbe l'ensemble des opérations. Afin de résoudre cette problématique, nous travaillons au développement d'outils de planification qui permettent de satisfaire les commandes tout en minimisant les différences entre l'ancien plan de production et le nouveau (c.-à-d. en minimisant les perturbations). Pour ce faire, nous modifions les modèles d'optimisation mathématique pour les inciter à produire des plans de production semblables aux précédents.
</resume></p><p><b>Lien</b>: <a href="files/Moisan.pdf">diapositives</a></p></div><div id="2732015"><h3>Vendredi 27 mars 2015</h3><p style="text-align:left;"><b>Bio-informatique des systèmes appliquée à la neuroscience. Du neurone au réseau</b><br>Simon Hardy<br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>La bio-informatique des systèmes est un domaine multidisciplinaire qui exploite la puissance computationnelle et les analyses systémiques pour formuler et solutionner des problématiques biologiques complexes. Cette complexité peut être de deux natures : une complexité d'échelle ou une complexité dynamique. Dans mon groupe, nous utilisons la modélisation et la simulation pour étudier la complexité dynamique en collaboration avec les biologistes. Lors de cette présentation, je présenterai deux projets en cours en neurosciences et nos résultats préliminaires.</resume></p><p><b>Note: </b>Heure inhabituelle!</p><p><b>Lien</b>: <a href="files/SeminaireInfo_2015-03.pdf">diapositives</a></p></div><div id="2032015"><h3>Vendredi 20 mars 2015</h3><p style="text-align:left;"><b>Sélection de modèle sous la contrainte de la confidentialité différentielle</b><br>Anne-Sophie Charest<br><i>Professeure au <a href="http://www.mat.ulaval.ca/departement-et-professeurs/direction-personnel-et-etudiants/professeurs/fiche-de-professeur/show/charest-anne-sophie/">département de mathématiques et de statistique</a></i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Toute organisation qui souhaite partager des données ou résultats d'analyses statistiques doit s'assurer de respecter la promesse de confidentialité offerte aux individus ou entreprises sur lesquels les données ont été collectées. Ce n'est pas une tâche facile, surtout avec l'augmentation rapide de la quantité d'informations personnelles disponibles facilement. La confidentialité différentielle est un critère rigoureux pour mesurer la protection de la confidentialité offerte par un mécanisme de divulgation de données ou de résultats statistiques. Après avoir établi les bases du problème de protection de la confidentialité, je décrirai en détails le critère de confidentialité différentielle. Je présenterai ensuite mes travaux récents sur la sélection de modèle sous la contrainte de la confidentialité différentielle.
</resume></p><p><b>Note: </b>Heure inhabituelle!</p></div><div id="1332015"><h3>Vendredi 13 mars 2015</h3><p style="text-align:left;"><b>Robot Learning: Algorithms and Applications</b><br>Abdeslam Boularias<br><i>Chercheur postdoctoral (<a href="http://www.abdeslam.net">Robotics Institute of Carnegie Mellon University</a>)</i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Robot learning is a research area at the intersection of machine learning and robotics. The question driving research in this area is: How can a robot learn from data to perform complex tasks? Machine learning provides powerful tools for solving problems in robotics that, due to their complexity, cannot be addressed solely by engineering. Object recognition is a prime example of a robot vision problem that can hardly be solved without any learning. Grasping and manipulating unknown objects is another type of a robotic task that is difficult to solve by hand-coding. Robotics also provides researchers in machine learning with interesting new challenges. For instance, the field of reinforcement learning has been deeply influenced by problems that typically arise in robotics, such as partial observability. Another example is learning by demonstration, which is a particular type of supervised learning that is specific to robotics.<p>

    My talk will focus on problems in machine learning that originate from robotics. In particular, I will present challenging problems related to reinforcement learning and to learning by demonstration. I will show how these problems can be solved by formulating them as optimization problems, derived from a higher principle such as entropy minimization. I will also show how the proposed algorithms were used to solve problems in robotics.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>français</b>.</p><p><b>Lien</b>: <a href="files/AbdeslamTalkatLaval.pdf">diapositives</a></p></div><div id="632015"><h3>Vendredi 6 mars 2015</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></p><p></p></div><div id="2442015"><h3>Vendredi 24 avril 2015</h3><p style="text-align:left;"><b>Apprentissage de modèles parcimonieux à partir de génomes complets avec application à la résistance aux antibiotiques</b><br>Alexandre Drouin<br><i>Étudiant au doctorat au GRAAL</i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>La diminution des coûts associés au séquençage d'ADN permet de faire des études à grandes échelles avec un flot constant de données génomiques. Ces données peuvent être analysées, dans le contexte d'études cas-témoin, en vue de déterminer les variations génétiques qui sont associées à un état biologique. Nous nous intéressons au problème d'apprentissage de modèles permettant de prédire l'occurence d'un état biologique chez un individu à partir de son génome complet, de la partie codante (exome) ou exprimée (transcriptome). Ce problème engendre son lot de défis, tant au niveau de la représentation des données, qu'au niveau computationnel. De plus, afin de contribuer à la compréhension des mécanismes biologiques sous-jacents, ces modèles doivent être interprétables par les experts du domaine. Nous cherchons donc des modèles qui sont parcimonieux et qui ont une structure propice à l'interprétation. Notre approche s'appuie sur une représentation de type "bag-of-words" des génomes et sur l'algorithme du Set Covering Machine. Nous présentons un exemple d'application à la prédiction de la résistance aux antibiotiques de 4 pathogènes humains importants.</resume></p><p><b>Lien</b>: <a href="files/drouin_2015_04_24.pdf">diapositives</a></p></div><div id="1742015"><h3>Vendredi 17 avril 2015</h3><p style="text-align:left;"><b>Distributions de probabilités sur l'espace des graphes orientés acycliques : une approche Bayésienne nonparamétrique</b><br>Patrick Dallaire<br><i>Étudiant au doctorat au DAMAS</i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>L'apprentissage Bayésien requiert la spécification d'une distribution de
probabilités a priori sur l'espace qui doit faire l'objet d'apprentissage,
lequel pourrait être les paramètres d'un modèle de régression par exemple.
Il existe des distributions de probabilités pour plusieurs types d'objets,
tel que les réels (Gaussienne), les entiers positifs (Poisson), les
distributions discrètes finies (Dirichlet), les matrices de covariances
(Inverse Wishart), etc. Pour notre part, nous nous intéressons à
l'apprentissage de graphes orientés acycliques. L'approche Bayésienne
classique (paramétrique) à ce problème d'apprentissage consiste à définir le
prior sur un espace fini de graphes, ce qui peut s'avérer contraignant pour
certains problèmes. L'approche Bayésienne nonparamétrique, qui est plus
flexible, repose quant à elle sur l'utilisation de distributions a priori
sur des espaces dont le nombre de dimensions est infini, ce qui correspond
dans notre cas à des graphes infinis. Il n'existe actuellement aucune
distribution de probabilité ayant pour support l'entièreté de l'espace des
graphes orientés acycliques infinis. Nous présenterons notre récente
construction d'une telle distribution et discuterons de son application à
l'apprentissage de structures.</resume></p></div><div id="1042015"><h3>Vendredi 10 avril 2015</h3><p style="text-align:left;"><b>Learning to represent relations</b><br><a href="http://www.iro.umontreal.ca/~memisevr/">Roland Memisevic</a><br><i>Professeur à l'Université de Montréal</i></p><p><b>Heure</b>: 11h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Many tasks in computer vision become easy, once images are encoded using the right kind of representation.  Deep learning is an approach to image recognition and related tasks that attempts to learn the right representation along with any other system parameters from data. The approach has been highly successful recently, yielding break-through performance in problems like object, face, and activity recognition.  After providing a brief overview of feature learning and deep learning, I will describe what is missing in my opinion in terms of extending deep learning beyond recognition, towards more general computer vision, and potentially towards more general AI. I will argue that the capability to represent relations, rather than "things", is what is needed to solve many tasks in vision and AI with a single kind of model. I will also describe how to encode relations in deep learning models, and I will demonstrate some of our recent work on training systems to perform depth inference, motion estimation and analogy-making.</resume></p><p><b>Note: </b>La présentation sera donnée en français</p></div><div id="2742015"><h3>Lundi 27 avril 2015</h3><p style="text-align:left;"><b>Optimal and Information Theoretic Syntactic Pattern Recognition</b><br><a href="http:/www.scs.carleton.ca/~oommen">John Oommen</a><br><i>Chancellor's Professor; Fellow: IEEE; Fellow: IAPR; Carleton University<br>Adjunct Professor, University of Agder, Grimstad, Norway</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>In this talk we show how we can achieve Information Theoretic Optimal Syntactic Pattern Recognition for arbitrary systems. We do this by presenting a new model for noisy channels which permits arbitrarily distributed substitution, deletion and insertion errors. Apart from its straightforward applications in string generation and recognition, the model also has potential applications in speech and uni-dimensional signal processing. The model, which is specified in terms of a noisy string generation technique, is functionally complete and stochastically consistent.<p>
    Apart from presenting the channel we also specify a technique by which Pr[Y|U], the probability of receiving Y given that U was transmitted, can be computed in cubic time.  This procedure involves dynamic programming, and is to our knowledge, among the few non-trivial applications of dynamic programming which evaluate quantities involving relatively complex combinatorial expressions and which simultaneously maintain rigid probability consistency constraints.</resume></p><p><b>Biographie: </b>Dr. John Oommen was born in Coonoor, India on September 9, 1953. He obtained his B.Tech. degree from the Indian Institute of Technology, Madras, India in 1975. He obtained his M.E. from the Indian Institute of Science in Bangalore, India in 1977. He then went on for his M.S. and Ph. D. which he obtained from Purdue University, in West Lafayettte, Indiana in 1979 and 1982 respectively. He joined the School of Computer Science at Carleton University in Ottawa, Canada, in the 1981-82 academic year. He is still at Carleton and holds the rank of a Full Professor. Since July 2006, he has been awarded the honorary rank of Chancellor's Professor, which is a lifetime award from Carleton University. His research interests include Automata Learning, Adaptive Data Structures, Statistical and Syntactic Pattern Recognition, Stochastic Algorithms and Partitioning Algorithms. He is the author of more than 415 refereed journal and conference publications, and is a Fellow of the IEEE and a Fellow of the IAPR. Dr. Oommen has also served on the Editorial Board of the IEEE Transactions on Systems, Man and Cybernetics, and Pattern Recognition.</p><p><b>Note: </b>Cette présentation sera donnée en anglais.</p></div><div id="342015"><h3>Vendredi 3 avril 2015</h3><p style="text-align:left;"><b>Vendredi Saint (pas de séminaire)</b><br></p><p></p></div><div id="9102015"><h3>Vendredi 9 octobre 2015</h3><p style="text-align:left;"><b>Apprentissage profond pour lire et "comprendre" le langage</b><br>Alexandre Lacoste<br><i>Ingénieur en logiciels, Google</i></p><p><b>Heure</b>: 14h00<br><b>Local</b>: PLT-2700</p><p><b>Résumé: </b><resume>L'apprentissage profond (deep learning) gagne en popularité et permet de résoudre des tâches de plus en plus complexes par le billet de l'apprentissage automatique. Dans cette présentation, je ferai une revu de l'apprentissage automatique et présenterai pourquoi l'apprentissage profond est une approche efficace. Finalement, je présenterai différentes architectures pour apprendre à lire des textes et parlerai des mécanismes d'attention.</resume></p></div><div id="13112015"><h3>Vendredi 13 novembre 2015</h3><p style="text-align:left;"><b>Jeux de policiers et de voleurs stochastiques</b><br>Frédéric Simard<br><i>Étudiant à la maîtrise en informatique</i></p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Les jeux de policiers-voleurs sont étudiés depuis longtemps en théorie des graphes et décrivent de manière générale un jeu dans lequel un ou plusieurs policiers tentent de capturer un certain nombre de voleurs. Dans cette présentation, on montre que l'étude de ces jeux peut avoir des applications concrètes. En effet, dans un article présenté récemment, il a été démontré qu'il est possible d'accélérer la résolution du problème de retrouver un objet caché sur un graphe à l'aide d'un jeu de policier-voleur. D'un autre côté, il est coutume que chaque jeu de policiers-voleurs soit formulé puis résolu de manière unique. Ainsi, on présentera aussi un modèle permettant d'encoder l'ensemble des jeux de policiers-voleurs et de tous les résoudre avec une même équation.</resume></p></div><div id="20112015"><h3>Vendredi 20 novembre 2015</h3><p style="text-align:left;"><b>Routage géométrique dans les triangulations de Delaunay (2ième partie)
</b><br><a href="http://cglab.ca/~jdecaruf/Francais.html">Jean-Lou De Carufel</a><br><i>Professeur à l'Université d'Ottawa</i></p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Savoir se déplacer efficacement dans un réseau est un problème fondamental en informatique et dans la vie de tous les jours. Par réseau (ou graphe), on entend un ensemble de points (noeuds ou sommets) reliés par des arêtes. Par exemple, la carte routière du Québec est un réseau où chaque ville est un noeud et chaque route est une arête. On retrouve plusieurs exemples de réseaux au quotidien: cartes routières, internet, le réseau des antennes cellulaires, etc. Dans cet exposé, nous nous concentrerons sur les réseaux géométriques, c'est-à-dire les réseaux qui sont plongés dans le plan ou dans l'espace.

Étant donné deux noeuds s et t dans un réseau, le routage consiste à trouver un chemin de s vers t. Il existe plusieurs mesures de coût des algorithmes de routage. Nous nous intéresserons particulièrement au ratio de routage ("routing ratio"). On dit qu'un algorithme de routage admet un ratio de routage de c si l'algorithme nous fait parcourir un chemin de longueur au plus c|st| pour se déplacer de s vers t (où |st| représente la distance euclidienne entre s et t). Le routage est local si l'information disponible à chaque noeud ne concerne que le noeud lui-même et ses voisins directs.

Nous discuterons des algorithmes de routage dans les triangulations en général. Nous étudierons le cas particulier des triangulations de Delaunay pour lesquelles il existe un algorithme de routage local avec un ratio de routage de 5,9. Nous discuterons aussi des liens entre ratio de routage et mémoire disponible.
</resume></p></div><div id="6112015"><h3>Vendredi 6 novembre 2015</h3><p style="text-align:left;"><b>Contrôle, localisation et interaction humaine avec un performeur plus léger que l'air</b><br>David St-Onge, ing. M.Sc.<br><i>Candidat au doctorat en génie mécanique à l'Université Laval, <a href="http://robot.gmc.ulaval.ca">Laboratoire de robotique</a></i></p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Depuis plus de 3 ans, le professeur Philippe Giguère dirige un projet de recherche interuniversitaire portant sur le contrôle d'un dirigeable cubique autonome et son interaction dans des contextes artistiques avec des performeurs et le public. Un des prototypes de ces aérobots sera en fonction dans le hall du Pavillon Vachon la semaine du 9 au 15 novembre. Profitant de cette occasion, le coordonnateur technique et co-créateur du projet, David St-Onge, présentera le contexte particulier de cette plateforme de recherche ainsi que les derniers développements faits par les divers groupes de recherche.</resume></p></div><div id="14122015"><h3>Lundi 14 d&eacute;cembre 2015</h3><p style="text-align:left;"><b>STEP-archival: Storage Integrity and Tamper Resistance using Data Entanglement</b><br>Hugues Mercier (Joint work with Maxime Augier and Arjen Lenstra)</p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume>  We present STEP-archives, a practical model for censorship-resistant storage systems where an attacker who wants to censor or tamper with data must cause a large amount of obvious collateral damage. MDS erasure codes are used to entangle unrelated data blocks, in addition to providing redundancy against storage failures. We show a trade-off for the attacker between attack complexity, irrecoverability, and collateral damage. We also show that the problem is asymmetric between attackers and defenders: while a defender can efficiently recover from imperfect attacks, an attacker must solve a NP- hard problem to find a perfect (irrecoverable) attack that minimizes collateral damage, or even just approximate this minimum within a reasonable ratio. We then study efficient sample heuristic attack algorithms that lead to irrecoverable but large damage, and demonstrate how some strategies and parameter choices allow to resist these sample attacks. Finally, we provide evidence that it is possible, using archives with constant time read-write operations, to force an attacker who wants to irrecoverably tamper with a document archived long enough to destroy a constant fraction of the archive.
Index Terms-Distributed storage, anti-tampering, data integrity, data entanglement, MDS codes.
  </resume></p><p><b>Biographie: </b>I am maître-assistant for the Institutes of Computer Science and Mathematics at the Université de Neuchâtel in Switzerland.

Prior to my appointment in Neuchâtel, I was a postdoctoral research fellow in Electrical and Computer Engineering at McGill University with Professor Fabrice Labeau and a postdoctoral research fellow at the Harvard School of Engineering and Applied Sciences with Professor Vahid Tarokh. I completed my doctoral studies in Electrical Engineering at the University of British Columbia under the supervision of Professor Vijay K. Bhargava. I received the B.Sc. degree in mathematics from Université Laval, and the M.Sc. degree in computer science from the Université de Montréal.

I was born and raised in Québec, Canada. Outside of work, I enjoy running, anything played with a racket, any activity that can be done on snow, playing bridge, and spending time with my daughters.</p></div><div id="2212016"><h3>Vendredi 22 janvier 2016</h3><p style="text-align:left;"><b>Distribuer des événements dans un horaire cyclique</b><br><a href="http://www.emiliepicardcantin.com">Émilie Picard-Cantin</a><br><i>Étudiante au doctorat (Laboratoire de programmation par contraintes)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2744</p><p><b>Résumé: </b><resume>La distribution équilibrée d'événements dans le temps (ou encore d'objets dans l'espace) est un problème qui touche beaucoup de domaines. On peut penser entre autres à la distribution des examens, au positionnement d'équipement dans une usine, ou plus particulièrement dans notre cas, l'espacement de tâches dans un horaire de travail. La présentation portera sur le concept de qualité d'un horaire par rapport à la distribution des tâches et son incorporation dans un modèle mathématique d'optimisation à plusieurs objectifs. Deux modèles mathématiques linéaires seront présentés pour résoudre le problème. Des connaissances de base en optimisation et en mathématiques favoriseront la compréhension de l'exposé mais ne sont pas requises.
</resume></p></div><div id="2912016"><h3>Vendredi 29 janvier 2016</h3><p style="text-align:left;"><b>Doit-on avoir peur de l'IA ? La réponse est oui à moins que...</b><br><a href="http://www2.ift.ulaval.ca/~chaib/">Brahim Chaib-draa</a><br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'intelligence artificielle (IA) fait couler beaucoup d'encre ces derniers temps. Peut-elle nous surpasser dans les prochaines vingt à cent ans comme le prédisent certains ?  Si oui, quelles sont les conséquences pour l'homme ? Peut-elle mettre en péril sa propre existence ? Si elle présente des dangers, que convient-il de faire ? Doit-on commencer à s'alarmer dès maintenant ou attendre et voir ?
    Dans ma présentation, je tenterai de répondre à ces questions en faisant état de l'intelligence faible et de l'intelligence forte et comment celle-ci risque de poser bien des problèmes si on n'y prend pas garde. Je mettrai en particulier l'accent sur ce qu'on peut faire pour éviter que l'IA nous mène trop loin en particulier : (i) ce qu'on peut faire comme société; (ii) ce qu'on peut imposer comme contraintes externes sur l'éventuel comportement de l'IA; et finalement  (iii) comment on pourrait concevoir (via des contraintes internes) une IA sans danger (safe).</resume></p><p><b>Lien</b>: <a href="files/Chaib-AGI-H16-V2.pdf">Diapositives</a></p></div><div id="1922016"><h3>Vendredi 19 f&eacute;vrier 2016</h3><p style="text-align:left;"><b>Algorithmes de recherche pour la prédiction de séquences en apprentissage automatique</b><br>Amélie Roland<br><i>Étudiante à la maîtrise (Graal)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La prédiction de séquences est un problème d'apprentissage automatique où l'on cherche à apprendre un prédicteur qui prédit correctement la séquence associée à une entrée. Par exemple, on pourrait vouloir prédire la séquence de caractères contenue dans une image, la séquence de phonèmes représentant la prononciation d'un mot ou la protéine ayant la plus grande affinité de liaison avec une molécule cible. Pour ce faire, l'algorithme d'apprentissage apprend d'abord une fonction qui permet de prédire le score d'une séquence pour une entrée donnée. Une fois cette fonction apprise, l'algorithme doit trouver, pour une entrée donnée, la séquence ayant le score maximal dans cette fonction. Puisque le nombre de séquences possibles est exponentiellement grand, il est souvent trop coûteux de calculer le score pour chacune de ces séquences. Dans ce travail, nous présentons un algorithme permettant d'effectuer cette recherche en temps polynomial pour certains cas. Pour les cas plus complexes, nous présentons des bornes spécialisées pouvant être utilisées avec un algorithme de «Branch and Bound» afin d'effectuer cette recherche efficacement. Enfin, nous présentons des résultats qui se comparent favorablement à l'état de l'art sur des tâches de reconnaissance de mots et de conception de médicaments assistée par ordinateur.</resume></p><p><b>Lien</b>: <a href="http://jmlr.org/proceedings/papers/v37/giguere15.pdf">article</a></p></div><div id="1222016"><h3>Vendredi 12 f&eacute;vrier 2016</h3><p style="text-align:left;"><b>Suivi du carbone en forêt, et liens structure-fonctions des forêt: une approche utilisant le LiDAR</b><br>Martin Béland</p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La technologie LiDAR représente un potentiel considérable pour mieux comprendre et suivre l'état de nos forêts. Dans le contexte actuel de forte compétition dans le marché forestier, et d'un effet potentiel des changements climatiques sur le fonctionnement des forêts, il est primordial de miser sur ce potentiel. Les systèmes LiDAR considérés ici sont le terrestre et aéroporté; ce dernier permet une couverture cartographique à grande échelle, alors que le premier permet une description détaillée de la structure des canopées forestières. Les deux systèmes sont complémentaires par leur capacité de couverture et niveau de détail. Deux thèmes de recherche utilisant le LiDAR sont abordés dans cette présentation : (1) le suivi du carbone en forêt, et (2) l'étude des liens entre la structure 3D des forêts et leurs fonctions (photosynthèse et évapotranspiration).
    </resume></p></div><div id="522016"><h3>Vendredi 5 f&eacute;vrier 2016</h3><p style="text-align:left;"><b>Sythese de mécanismes à quatre barres par optimisation non-convexe</b><br>Vincent Goulet<br><i>Étudiant à la maîtrise (Laboratoire de programmation par contraintes)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nous présenterons en quelques points l'optimisation non-convexe pour ensuite aborder une approche développée pour la conception de mécanismes à quatre barres. Un logiciel simple prend une courbe en entrée et retourne un mécanisme capable d'approximer cette courbe. Parmi les éléments clés de cette approche, on compte un modèle utilisant des contraintes quadratiques, ainsi qu'une technique d'échantillonnage basée sur les caractéristiques de la courbe.</resume></p></div><div id="732016"><h3>Lundi 7 mars 2016</h3><p style="text-align:left;"><b>Ordonnancement optimal des liens pertinents pour une requête dans un moteur de recherche</b><br><a href="http://www.iro.umontreal.ca/~lecuyer/">Pierre Lecuyer</a><br><i>Chaire de recherche du Canada en simulation et optimisation stochastique</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2341</p><p><b>Résumé: </b><resume>When a keyword-based search query is received by a search engine (SE), a classified ads website, or an online retailer site, the platform has exponentially many choices in how to sort the output to the query. Two extreme rules are (a) to return a ranking based on relevance only, which attracts more requests (customers) in the long run because of perceived quality, and (b) to return a ranking based only on the expected revenue to be generated by immediate conversions, which maximizes short-term revenue. Typically, these two objectives (and the corresponding rankings) differ. A key question then is what middle ground between them should be chosen. We introduce stochastic models that yield elegant solutions for this situation, and we propose effective solution methods to compute a ranking strategy that optimizes long-term revenues. This strategy has a very simple form.  The optimal ranking for each query is by decreasing order of a score attributed to each matching link.  This score is a linear function of the estimated relevance and the expected
revenue of the link, normalized by a function of the estimated link relevance. It contains a single real-valued parameter, which can be optimized via simulation combined with the use of a contraction mapping (or another optimization method).</resume></p><p><b>Note: </b>La présentation sera donnée en français.</p></div><div id="432016"><h3>Vendredi 4 mars 2016</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></p><p></p></div><div id="2532016"><h3>Vendredi 25 mars 2016</h3><p style="text-align:left;"><b>Vendredi Saint (pas de séminaire)</b><br></p><p></p></div><div id="1132016"><h3>Vendredi 11 mars 2016</h3><p style="text-align:left;"><b>Apprentissage machine et neuroimagerie</b><br><a href="http://www.crulrg.ulaval.ca/fr/recherche/axes/chercheurs/chercheur.php?id=52">Simon Duchesne</a><br><i>Professeur sous octroi agrégé, Faculté de médecine</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Our main contribution to research lie in the extraction of biomarkers of interest useful in the diagnostic of neurodegenerative diseases and prediction of future clinical status in subjects with prodromal disease. Our goal is to contribute to the reduction of diagnostic uncertainty in clinical decision-making, therefore improving patient management. To do so we develop novel and robust methods for the analysis of structural magnetic resonance images (MRI) acquired in multi-centric settings. The central hypothesis underlying our techniques is that neurodegenerative processes exhibit macroscopic morphological changes whose features are detectable and quantifiable on MRI with advanced image processing. Using inductive machine learning techniques, clinically useful diagnostic information is extracted from the data. Now looking for a few good women/men, our current projects relate to the development of ontologies, novel image processing techniques, and classifiers using imaging and non-imaging data.</resume></p></div><div id="1832016"><h3>Vendredi 18 mars 2016</h3><p style="text-align:left;"><b>Un cadre conceptuel pour la description et la classification des formes de terrain</b><br><a href="https://www.scg.ulaval.ca/eric-guilbert">Éric Guilbert</a><br><i>Professeur adjoint, Département des sciences géomatiques</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Une forme de terrain est une partie de terrain reconnaissable à sa forme caractéristique. Bien que les formes de terrain, comme les montagnes ou les vallées, soient facilement reconnaissables par tout un chacun, en donner une définition formelle qui soit implantable est difficile. En effet, les formes de terrain sont souvent définies de manière qualitative  par des termes vagues dont l'interprétation dépend du contexte culturel ou de l'expertise de la personne. Nous proposons dans cette présentation un cadre de travail où ces définitions qualitatives sont transformées successivement sur quatre niveaux pour aboutir à une définition formelle implantable.  Le premier niveau correspond aux connaissances de l'utilisateur. Le second niveau définit les concepts généraux caractérisant les formes : les formes sont définies par un squelette formé d'éléments saillants du terrain et non pas des indicateurs quantitatifs. Le troisième niveau transforme ces concepts en s'appuyant sur le réseau de surface calculé sur le modèle numérique de terrain. Le dernier niveau est l'implantation dans un système d'information.
    </resume></p></div><div id="2942016"><h3>Vendredi 29 avril 2016</h3><p style="text-align:left;"><b>Wireless Brain-machine Interfaces for Combined Optogenetics and Multichannel Electrophysiology</b><br>Benoît Gosselin<br><i>Professeur agrégé, Département génie électrique et génie informatique</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Optogenetics is a neural stimulation technique that allows using light to control and monitor individual neurons in living tissue. Such innovative approach provides researchers with a strong tool of crucial importance for brain research and for the development of brain-computer interfaces. Optogenetics has recently been widely used to carry out various neuroscience experiments in freely-behaving rodents, especially mice, which serve as common disease models. As a result, the development of advanced optogenetic hardware combining multiple technologies, such as bioMEMS, optics, microelectronics, and wireless technology has become a source of significant interest. This talk presents advanced systems for enabling combined optogenetics and multichannel electrophysiology in freely moving transgenic rodents. First, we present a wireless headstage with real-time spike detection and data compression that includes 32 neural recording channels and 32 photo-stimulation channels at a wavelength of 465 nm. This system is based on a real time digital signal processor and a Microblaze microprocessor softcore that detects and compresses full action potential waveforms collected from 32 channels in parallel. We present experimental results obtained from simultaneous photo-stimulation and recording performed in-vivo in the somatosensory cortex and the Hippocampus of a ChannelRhodospin (Thy1::ChR2-YFP line4) transgenic mouse. Secondly, we present a fully-integrated, full-duplex transceiver that can interface with very high-density neural optrodes (>500 channels in parallel). It combines an ultra-wideband impulse radio transmitter with a 2.4-GHz narrowband receiver for providing asymmetric data rates above 100 Mbit/s in downlink, and above 500 Mbits/s in uplink over a single implantable antenna, and within a power consumption of 10.4 mW. Finally, we present a smart home-cage system based on arrays of parallel resonators and multicoil inductive links that provides uniform power distribution in 3D to power up small bioinstruments attached or implanted in the body of freely moving animal subjects. This system features a natural power localization mechanism that enables superior efficiency and simplified operation.</resume></p><p><b>Biographie: </b>Benoit Gosselin a obtenu son doctorat en Génie électrique de l'École Polytechnique de Montréal en 2009 et a effectué un stage postdoctoral à Georgia Tech en 2010. Il est présentement professeur agrégé au Départ. de génie électrique et de génie informatique de l'Université Laval où il dirige le Laboratoire de recherche sur les microsystèmes biomédicaux. Ses intérêts de recherche couvrent les microsystèmes sans fil dédiés aux interfaces cerveau-machine, les circuits intégrés analogiques/mixtes et RF pour la neuro-ingénierie, les circuits d'interface pour les capteurs et les actuateurs implantables et les microsystèmes de diagnostique et de soins mobiles. Le Dr Gosselin est Éditeur associé de la revue IEEE Transactions on Biomedical Circuits and Systems et Président-fondateur du Chapitre IEEE CAS/EMB de Québec (Prix du meilleur nouveau Chapitre IEEE, 2015). Il a siégé sur les comités de plusieurs conférences internationales dont IEEE BIOCAS, IEEE NEWCAS, IEEE EMBC/NER, et IEEE ISCAS, et participe présentement à l'organisation de IEEE ISCAS'16 et IEEE NEWCAS'16. Ses recherches consistent, entre autres, à développer des plateformes microélectroniques innovantes pour observer et étudier le cerveau de petits mammifères en temps réel. En plus de récolter plusieurs prix, dont le Prix d'Innovation Mitacs 2015 - Maîtrise et le Prix du meilleur article IEEE BioCAS'15, ses recherches ont mené à la commercialisation de la première plateforme sans fil incorporant l'optogénétique et le monitoring cérébral à grande échèle.</p><p><b>Note: </b>Cette présentation sera donnée en français</p></div><div id="2242016"><h3>Vendredi 22 avril 2016</h3><p style="text-align:left;"><b>Adaptation de domaine en apprentissage automatique</b><br>Hana Ajakan<br><i>Étudiante au doctorat (Graal)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'adaptation de domaine est un problème d'apprentissage automatique rencontré lorsque la distribution d'apprentissage (appelée domaine source) diffère de la distribution de test (appelée domaine cible). Dans cette présentation, nous allons nous situer dans le cas de l'adaptation de domaine non-supervisé où les données du domaine source sont étiquetées et celles du domaine cible sont non-étiquetées. L'objectif est alors de créer des algorithmes capables d'appliquer les connaissances acquises sur le domaine source à un ou plusieurs nouveaux domaines cibles. Voilà quelques exemples d'applications de l'adaptation de domaine:<p>

- Filtrage de Spams: On voudrait qu'un système de filtrage de Spams performant pour un utilisateur donné (domaine source) soit également bon pour un autre utilisateur recevant des e-mails de natures différentes (domaine cible).<p>

- Analyse de sentiments: On possède deux ensembles de textes critiques concernant des films et des livres où chaque critique de film est accompagnée d'une côte, mais qu'aucune critique de livre ne possède sa côte. À partir de ces données, on désire avoir un modèle capable de prédire la côte d'un livre à partir de sa critique.<p>

Afin de résoudre cette problématique, Nous proposons un algorithme d'apprentissage du type réseau de neurones (qu'on appelle  DANN). Notre algorithme trouve une nouvelle représentation des données pour laquelle le réseau de neurones permet de bien classifier les données du domaine source et n'est pas capable de distinguer les données sources des données cibles.<p>

    Lors de cette présentation, nous allons commencer par voir comment et quand il est possible d'apprendre un modèle d'apprentissage performant sur le domaine cible, en utilisant seulement les données étiquetées de la distribution source et les données non-étiquetées de la distribution cible. Nous allons aussi voir les grands types d'algorithmes d'adaptation de domaine, les travaux théoriques et enfin nous présenterons notre algorithme DANN.</resume></p><p><b>Lien</b>: <a href="files/DA_seminaire.pdf">diapositives</a></p></div><div id="1542016"><h3>Vendredi 15 avril 2016</h3><p style="text-align:left;"><b>Comprendre la stéréoscopie photométrique extérieure</b><br>Yannick Hold-Geoffroy<br><i>Étudiant au doctorat (Laboratoire de vision et systèmes numériques)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La stétéoscopie photométrique est une technique de reconstruction 3D analysée en profondeur depuis sa création il y a une trentaine d'année. Elle offre une grande qualité de reconstruction à faible coût, mais elle a été contrainte jusqu'à récemment aux environnements contrôlés en laboratoire. Depuis peu, elle est employée à la reconstruction de scènes extérieures, mais requiert plusieurs centaines de captures étalées sur plusieurs mois pour bien contraindre le problème. Nous présentons une nouvelle analyse permettant de réduire la quantité de captures requises à quelques unes, disséminées pendant une seule journée. Le concept clef pour rendre cette technique utilisable en plein air dans un laps de temps réduit est la compréhension de l'illumination extérieure. En particulier, qu'est-ce qui est important d'observer dans le ciel pour affecter la qualité de la reconstruction? Est-ce que ces événements peuvent survenir en quelques captures au sein d'une même journée? Le développement d'algorithmes de reconstruction à partir des analyses effectuées sera ensuite discuté. L'aboutissement de cette approche permettrait l'emploi du soleil comme scanner 3D.</resume></p></div><div id="842016"><h3>Vendredi 8 avril 2016</h3><p style="text-align:left;"><b>Apprentissage profond: Raisons qui nous ont fait reconsidérer les réseaux de neurones.</b><br>Ludovic Trottier<br><i>Étudiant au doctorat (Damas)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Au cours des dernières années, les réseaux de neurones profonds ont remporté de nombreuses compétitions en apprentissage automatique, et connaissent actuellement beaucoup de succès en vision numérique et reconnaissance vocale. Leur habileté à construire une représentation hiérarchique à plusieurs couches leur permettent de mieux modéliser les abstractions de haut-niveau contenues dans les observations. Bien que plusieurs entreprises et laboratoires de recherche prennent actuellement le virage "apprentissage profond", les raisons qui expliquent la popularité grandissante de cette nouvelle vague de réseaux de neurones restent obscures. Le terme "deep learning" est souvent caractérisé comme un buzzword, et certains parlent même de rebranding. Au cours de cette présentation, nous examinerons les trois principales avancées théoriques et technologiques entre les années 2006 et 2012 qui ont lancé les réseaux de neurones profonds: (i) l'entraînement non-supervisé vorace, (ii) les fonctions d'activation non-saturées, et (iii) la redécouverte des réseaux à convolution. L'étude de ces développements récents nous permettra de mieux cerner la différence entre les réseaux pré-2006 et ceux d'aujourd'hui, et nous amènera à mieux apprécier le paradigme de l'apprentissage profond.</resume></p><p><b>Lien</b>: <a href="files/deep.pdf">diapositives</a></p></div><div id="142016"><h3>Vendredi 1<sup>er</sup> avril 2016</h3><p style="text-align:left;"><b>Fast Recursive Tensor Sequential Learning</b><br>Ming Hou<br><i>Étudiant au doctorat (Damas)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Tensor-variate regression (learning) approaches arise frequently in machine learning community because of the increasing demands to deal with the problems that usually involve large-scale higher-order data with complex structure. Among them, higher-order partial least squares (HOPLS) is the state-of-the-art method for predicting tensor output from tensor input. However, HOPLS is a batch method which is computionally expensive and not suitable for dynamic environments. To this end, we develop a highly efficient sequential learning framework, namely recursive higher-order partial least squares (RHOPLS), that addresses the great challenge posed by the limited storage space and fast processing time allowed by huge high-speed dynamic tensor sequence environments. In conjunction with a fast modification strategy of Tucker model, the extracted factors from the proposed PLS-based framework successfully maximize the pairwise covariance between the whole input and output tensor sequences by recursively incorporating the maximum pairwise covariance between the incremental mini-batch pair into the previous low-rank approximation of model. Unlike batch or multi-pass iterative approaches, which require to access the entire data, RHOPLS conducts a singe-pass blockwise consecutive calculation scheme and thus only a small set of factors are needed to be stored. Our approach is orders of magnitude faster than the batch method while maintaining a comparable predictability. Finally, the effectiveness and efficiency of our method is verified on several real-life applications.</resume></p><p><b>Note: </b>The presentation will be given in English</p></div><div id="2652016"><h3>Jeudi 26 mai 2016</h3><p style="text-align:left;"><b>A progress-sensitive flow-sensitive inlined information-flow control monitor</b><br>Andrew Bedford<br><i>Étudiant au doctorat</i></p><p><b>Heure</b>: 11h00<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume>We present a novel progress-sensitive, flow-sensitive hybrid information-flow control monitor for an imperative interactive language. Progress-sensitive information-flow control is a strong information security guarantee which ensures that a program's progress (or lack of) does not leak information. Flow-sensitivity means that this strong security guarantee is enforced fairly precisely: we track information flow according to the source of information and not to an a priori given variable security level. We illustrate our approach on an imperative interactive language. Our hybrid monitor is inlined: source programs are translated, by a type-based analysis, into a target language that supports dynamic security levels. A key benefit of this is that the resulting monitored program is amenable to standard optimization techniques such as partial evaluation.</resume></p><p><b>Note: </b>Cette présentation de 30 minutes sera donnée en anglais.</p></div><div id="652016"><h3>Vendredi 6 mai 2016</h3><p style="text-align:left;"><b>Optimisation distribuée via des systèmes multi-agents</b><br>Marc-André Carle<br><i>Professeur sous octroi adjoint, Département d'opérations et systèmes de décision (FSA)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Au cours de ce séminaire, je présenterai en quoi l'optimisation distribuée constitue une stratégie des plus intéressantes pour résoudre des problèmes d'optimisation de très grande taille. Nous verrons comment CAT (collaborating agent teams), une approche inspirée des systèmes multi-agents, permet de diviser un problème complexe en différentes composantes et les résoudre séparément. Le développement de ce type d'approches devient sans cesse plus facile grâce à l'essor de nouvelles technologies (infonuagique, solveurs génériques). Des exemples d'application seront proposés pour l'optimisation stochastique ainsi que pour le design de chaînes logistiques.
    </resume></p></div><div id="1172016"><h3>Lundi 11 juillet 2016</h3><p style="text-align:left;"><b>Variations on the PAC-Bayesian Bound</b><br><a href="http://www.di.ens.fr/~germain/">Pascal Germain</a><br><i>Chercheur postdoctoral, Centre de recherche Inria de Paris.</i></p><p><b>Heure</b>: 14h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>In the first part of this talk, I will present the main ideas underlying the PAC-Bayesian learning theory - which provides statistical guarantees on the generalization of a weighted majority vote of many classifiers - using a simplified approach. This approach leads to a general theorem that embraces several existing PAC-Bayesian results. Moreover, our proof process eases the "customization" of PAC-Bayesian theorems. In particular, I will show how this result can be extended to the transductive framework, and to theorems where the common Kullback-Leibler divergence (which, in usual PAC-Bayesian theorems, quantifies the "distance" between a prior and a posterior distribution over the voting classifiers) is replaced by the Rényi divergence.

    In the second part, I will exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria.</resume></p><p><b>Note: </b>La présentation sera donnée en français.</p><p><b>Lien</b>: <a href="files/ulaval2016_handout.pdf">diapositives</a></p></div><div id="1992016"><h3>Lundi 19 septembre 2016</h3><p style="text-align:left;"><b>Tirer parti de la science des données pour apprendre directement des modèles traitables pour l'inférence probabiliste et la prise de décision</b><br><a href="https://cs.uwaterloo.ca/~ppoupart/">Pascal Poupart</a><br><i>Professeur agrégé à la David R. Cheriton School of Computer Science, University of Waterloo, Waterloo (Canada)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2700</p><p><b>Résumé: </b><resume>La plupart des problèmes en intelligence artificielle sont NP-difficiles et donc de nombreux travaux ont été consacrés à l'exploitation de structures spécifiques à certains problèmes et à la conception d'approximations pour la mise à l'échelle des algorithmes. Avec la montée de la science des données, un changement de paradigme est en cours. Au lieu de demander à un expert du domaine de spécifier un modèle pour lequel le raisonnement peut être intraitable, l'apprentissage machine nous permet d'apprendre des modèles traitables directement à partir des données. La réalité est que la plupart des modèles spécifiés par les experts du domaine sont approximatifs tandis que les modèles obtenus à partir de données peuvent être plus précis. En outre, en utilisant une hiérarchie de modèles pour lesquels la complexité de représentation est la même que la complexité du raisonnement, il est possible d'apprendre des modèles dont la complexité augmente avec la quantité de données et qui demeurent traitables. Par exemple, l'inférence dans les modèles graphiques probabilistes tels que les réseaux bayésiens et les réseaux markoviens sont #P-complet. Cependant, si nous estimons des réseaux somme-produit (type spécial de réseaux neuronaux profonds qui sont équivalents aux réseaux bayésiens et markoviens) directement à partir des données, l'inférence probabiliste exacte peut être faite en temps linéaire par rapport à la taille du réseau. Dans cet exposé, je vais expliquer comment apprendre les réseaux somme-produit à partir d'ensembles de données en transmission continue dans un mode en ligne et distribué pour une large gamme d'applications, y compris la modélisation du langage, la reconnaissance de l'écriture manuscrite, les systèmes de recommandation, les réseaux de communication, la classification et la prédiction de séries temporelles. Je vais aussi expliquer comment modéliser et estimer des extensions aux réseaux somme-produit pour la prise de décision dans diverses tâches de diagnostic.
Cette présentation sera basée sur les articles suivants:<p>

Han Zhao, Mazen Meliberi, Pascal Poupart, On the Relationship Between Sum-Product Networks and Bayesian Networks, ICML, 2015.<p>

Abdullah Rashwan, Han Zhao and Pascal Poupart, Online and Distributed Bayesian Moment Matching for Parameter Learning in Sum-Product Networks, AISTATS, 2016.<p>
Mazen Melibari, Pascal Poupart and Prashant Doshi, Sum-Product-Max Networks for Tractable Decision Making, IJCAI, 2016 <p>

Mazen Melibari, Pascal Poupart, Prashant Doshi, George Trimponias, Dynamic Sum-Product Networks for Tractable Inference on Sequence Data, PGM, 2016.<p>

Priyank Jaini, Abdullah Rashwan, Han Zhao, Yue Liu, Ershad Banijamali, Zhitang Chen and Pascal Poupart, Online Algorithms for Sum-Product Networks with Continuous Variables, PGM, 2016.<p>

Han Zhao and Pascal Poupart, A Unified Approach for Learning the Parameters of Sum-Product Networks, NIPS, 2016.
    </resume></p><p><b>Biographie: </b>Pascal Poupart is an Associate Professor in the David R. Cheriton School of Computer Science at the University of Waterloo, Waterloo (Canada).  He received the B.Sc. in Mathematics and Computer Science at McGill University, Montreal (Canada) in 1998, the M.Sc. in Computer Science at the University of British Columbia, Vancouver (Canada) in 2000 and the Ph.D. in Computer Science at the University of Toronto, Toronto (Canada) in 2005.  His research focuses on the development of algorithms for reasoning under uncertainty and machine learning with application to health informatics and natural language processing.  He is most well known for his contributions to the development of approximate scalable algorithms for partially observable Markov decision processes (POMDPs) and their applications in assistive technologies, including automated prompting for people with dementia for the task of handwashing. Other notable projects that his research team are currently working on include chatbots for automated personalized conversations and wearable analytics to assess modifiable health risk factors.  He co-founded Veedata, a startup that provides analytics services to the insurance industry and the research market. 

    Pascal Poupart received a David R. Cheriton Faculty Award in 2015 and an Early Researcher Award (competitive honor for top Ontario researchers) by the Ontario Ministry of Research and Innovation in 2008.  He was also a co-recipient of the Best Paper Award Runner Up at the 2008 Conference on Uncertainty in Artificial Intelligence (UAI) and the IAPR Best Paper Award at the 2007 International Conference on Computer Vision Systems (ICVS).  He also serves on the editorial board of the Journal of Machine Learning Research (JMLR) (2009 - present) and the Journal of Artificial Intelligence Research (JAIR) (2008 - 2011).  His research collaborators include Huawei, Google, Intel, Kik Interactive, In the Chat, Slyce, HockeyTech, the Alzheimer Association, the UW-Schlegel Research Institute for Aging, Sunnybrook Health Science Centre and the Toronto Rehabilitation Institute.</p><p><b>Note: </b>La présentation sera donnée en français.</p></div><div id="11112016"><h3>Vendredi 11 novembre 2016</h3><p style="text-align:left;"><b>A linear time algorithm for constrained optimal segmentation</b><br><a href="http://tdhock.github.io">Toby Hocking</a><br><i>Post-doctoral researcher, McGill University</i></p><p><b>Heure</b>: 11h00<br><b>Local</b>: PLT-2783</p><p><b>Résumé: </b><resume>Accurately detecting "peaks" or up changes is important in the analysis of time series and genomic data. Our <a href="http://jmlr.org/proceedings/papers/v37/hocking15.html">ICML2015 paper</a> describes PeakSeg, a constrained optimization model with state-of-the-art accuracy. The algorithm proposed in that paper has two drawbacks (1) it does not find the optimal solution to the PeakSeg optimization problem, and (2) its time complexity is quadratic in the number of data points. In this presentation I will show how a new functional pruning algorithm can be used to overcome both of those drawbacks. I will discuss the computational complexity (time, memory, disk space) as well as the accuracy of this new algorithm, which we have <a href="https://github.com/tdhock/coseg">implemented in an R package</a>.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>anglais</b>. Joint work with Guillem Rigaill, Paul Fearnhead, and Guillaume Bourque</p><p><b>Liens</b>: <a href="files/HOCKING-PeakSeg-functional-pruning-slides">diapositives</a>, <a href="http://jmlr.org/proceedings/papers/v37/hocking15.html">article</a>, <a href="https://github.com/tdhock/coseg">librairie</a></p></div><div id="2712017"><h3>Vendredi 27 janvier 2017</h3><p style="text-align:left;"><b>Optimisation bayésienne pour espaces d'hyperparamètres avec conditions</b><br>Julien-Charles Lévesque<br><i>Étudiant au doctorat, Laboratoire de vision et systèmes numériques, Département de génie électrique et de génie informatique</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume> L'optimisation bayésienne est couramment appliquée pour ajuster les hyperparamètres d'algorithmes d'apprentissage automatique. Ces hyperparamètres peuvent être structurés et il peut y avoir une relation de dépendance ou de condionalité entre certains hyper-paramètres. Dans cette présentation, nous allons cibler le problème combiné de la sélection d'un modèle d'apprentissage et de l'optimisation de ses hyperparamètres (en anglais, the combined algorithm selection and hyperparameter optimization problem). Ce problème comprend au moins un hyperparamètre conditionnel, le choix de l'algorithme d'apprentissage, et les hyperparamètres des modèles sous-jacents dépendent de ce choix. Nous verrons que l'optimisation bayésienne avec des processus gaussiens peut être utilisée pour l'optimisation d'espaces conditionnels avec l'injection de connaissances concernant les conditions dans les noyaux. Nous proposons et examinons le comportement de deux noyaux, un noyau conditionnel qui force une similarité nulle entre les échantillons provenant de différentes branches de condition, et le noyau de Laplace, basé sur des similitudes avec les processus de Mondrian et les forêts aléatoires. Nous démontrons l'intérêt d'utiliser ces noyaux ainsi que l'impact positif d'une imputation correcte des hyperparamètres inactifs sur un espace de classifieurs tirés de la librairie scikit-learn.</resume></p></div><div id="2012017"><h3>Vendredi 20 janvier 2017</h3><p style="text-align:left;"><b>Recherche d'information dynamique</b><br>Robin Joganah<br><i>Étudiant à la maîtrise, Département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La recherche d'information dynamique a pour objectif de satisfaire le besoin d'information d'un utilisateur sur plusieurs concepts autour d'une requête initiale. L'utilisateur (simulé) participe à cette recherche en fournissant son avis sur la pertinence des documents tout au long du processus itératif. Dans ce contexte, le défi consiste à être capable de proposer un équilibre entre la diversité de concepts et la pertinence vis à vis de la requête initiale de l'utilisateur, afin de couvrir ses besoins d'information dans la période de temps la plus courte possible. Les enjeux de ce type de recherches sont notamment les domaines complexes tels que des événements récents (Épidémie du virus Ebola), le "Deep Web" et les différents trafics (humains, produits illégaux) qu'il peut abriter.</resume></p><p><b>Lien</b>: <a href="files/Joganah.pdf">diapositives</a></p></div><div id="2422017"><h3>Vendredi 24 f&eacute;vrier 2017</h3><p style="text-align:left;"><b>GPU et projets académiques</b><br>David Landry<br><i>Étudiant à la maîtrise, Laboratoire de robotique mobile</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'arrivée des GPGPU (General Purpose Graphics Processing
    Unit) a ouvert de nouvelles possibilités à la communauté
    scientifique. Des programmes autrefois trop coûteux à calculer,
    mais massivement parallélisables, sont désormais à notre
    portée. L'adoption des GPGPU a été rapide dans les domaines où
    leur utilisation est critique (réseaux neuronaux, vision numérique
    par exemple).  Cependant, on peut argumenter que les GPGPU sont
    encore sous-utilisés dans d'autres domaines, malgré qu'il y ait
    des gains importants à faire sur le plan des performances. Notre
    objectif lors de ce séminaire sera de favoriser l'utilisation du
    GPGPU dans les projets programmés au département. Après un bref
    aperçu des particularités architecturales des GPU, nous décrirons
    quels programmes peuvent en bénéficier facilement. Par la suite
    nous étudierons différents outils qui permettent d'utiliser les
    GPU aisément. À la fin de ce tutoriel, les auditeurs sauront
    identifier les projets qui peuvent bénéficier des GPGPU. Ils
    sauront aussi à quel endroit débuter le travail pour améliorer les
    performances de leurs programmes.</resume></p><p><b>Liens</b>: <a href="files/Landry.pdf">Diapositives</a>, <a href="https://github.com/davidlandry93/opencv-gpu-example/">code OpenCV</a>, <a href="https://github.com/davidlandry93/arrayfire-example">code ArrayFire</a>, <a href="https://github.com/davidlandry93/openacc-example">code OpenAcc</a></p></div><div id="1722017"><h3>Vendredi 17 f&eacute;vrier 2017</h3><p style="text-align:left;"><b>Techniques de propagation de la contrainte de ressource en ordonnancement disjonctif et cumulatif</b><br>Roger Kameugne<br><i>Chercheur postdoctoral, Laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'approche par contraintes est de plus en plus appliquée pour la modélisation et la résolution des problèmes d'ordonnancement de la vie réels. Cet enthousiasme est dû essentiellement à l'efficacité et l'efficience des algorithmes utilisés pour propager les contraintes de ressources. Alors que les algorithmes de filtrage utilisés dans le cas disjonctif sont de faible complexité (entre O(n) et O(nlog n)), la complexité des algorithmes analogues dans le cas cumulatif sont encore de l'ordre O(n<sup>3</sup>) en moyenne. Cette différence est due au fait que dans le cas cumulatif, plusieurs tâches ou activités  peuvent s'exécuter en parallèle tant que la capacité de la ressource n'est pas violée. Dans ce contexte, l'un des défis consiste alors à réduire la complexité de ces différents algorithmes tout en augmentant leur pouvoir de filtrage. Après une brève introduction à la programmation par contraintes et à l'ordonnancement, nous présenterons et illustrerons les règles utilisées pour propager les contraintes de ressource en ordonnancement disjonctif et cumulatif. On s'intéressera particulièrement à l'edge-finder, au not-first/not-last et au raisonnement énergétique.</resume></p></div><div id="1022017"><h3>Vendredi 10 f&eacute;vrier 2017</h3><p style="text-align:left;"><b>Contextual Suggestion - word vector model and global interest</b><br>Jian Mo<br><i>Étudiant à la maîtrise</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>The contextual suggestion task is how to generate better recommendations to users with prior knowledge including user background, preference history, city location, time of the year and many other contextual information. In this talk, we will introduce our approach to the task at TREC 2016. We have two major models. The first model is a global interest regressor trained to model popular interests loved by all users (E.g. Museums and National Parks). The second model introduces word embeddings to captures individual user preference. Both user profiles and candidate places are represented as word vectors in a same Euclidean space. So that a similarity score between user and attraction can be measured by their vector distance. The combined model of the two gained the highest Precision at 5 among all other track results.</resume></p><p><b>Note: </b>Cette présentation sera donnée en <b>anglais</b>.</p></div><div id="322017"><h3>Vendredi 3 f&eacute;vrier 2017</h3><p style="text-align:left;"><b>Optimisation de fonctions à haut coût</b><br>Ulysse Côté-Allard<br><i>Étudiant au doctorat, Département de génie informatique et de génie électrique</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Une fonction à haut coût est une fonction pour laquelle chaque évaluation est coûteuse (nécessite beaucoup de ressources à calculer). Ce type de fonction ne peut être optimisée grâce aux techniques traditionnelles, car celles-ci assument des évaluations rapides et peu coûteuses. L'une des techniques les plus utilisées pour l'optimisation de fonctions à haut coût est l'optimisation bayésienne. Celle-ci consiste à utiliser un processus gaussien afin de simuler la fonction réelle.  La fonction de régression ainsi obtenue est rapide à évaluer. Le but devient alors de trouver comment explorer et exploiter la fonction à haut coût ainsi que la meilleure façon de balancer ces deux comportements. Pour ce faire, nous proposons un nouvel algorithme (SquidSwarm) basé sur l'optimisation bayésienne. SquidSwarm est repose sur un nouvel algorithme d'optimisation de groupe de particules (Particle Swarm Optimization) nommé Time Adaptation Decay Particle Swarm Optimization (TAD-PSO). La première partie de l'exposé consistera à présenter ce nouvel algorithme ainsi qu'à le comparer à certains des autres algorithmes de PSO les plus utilisés (PSO-G, PSO-L, CLPSO, HPSO-TVAC, OLPSO-L, OLPSO-G). En second lieu, nous verrons comment utiliser TAD-PSO dans le contexte de l'optimisation bayésienne, afin de donner naissance à l'algorithme de SquidSwarm. Des résultats préliminaires pour SquidSwarm seront présentés.</resume></p><p><b>Lien</b>: <a href="files/Presentation_seminaire_3_fevrier.pdf">diapositives</a></p></div><div id="2432017"><h3>Vendredi 24 mars 2017</h3><p style="text-align:left;"><b>Développement d'une nouvelle technique de compression pour les codes variables à fixes quasi-instantanés</b><br>Fatma Haddad (remplacée par Danny Dubé)</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>En compression de données, une des techniques d'encodage les plus
connues est celle de Huffman.  Typiquement, la fonction d'encodage
construite par la technique de Huffman associe chaque symbole de
l'alphabet source à un mot de code fait de bits.  La compression vient
du fait qu'un symbole plus fréquent se voit associé à un mot de code
plus court et vice-versa.  Afin que le décodage soit possible,
l'ensemble des mots de code doivent former un code préfixe.
L'encodage de Huffman est un cas particulier de code fixe à variable
(FV).<p>

À l'inverse, d'autres techniques d'encodage visent à construire des
fonctions d'encodage variables à fixes (VF).  La fonction d'encodage
associe les mots d'un dictionnaire source à des mots de code de taille
fixe.  Les mots du dictionnaire source sont de longueurs variables.
Typiquement, on spécifie la taille désirée du dictionnaire; la
longueur des mots de code découle de ce choix.  La compression vient
du fait que, dans le dictionnaire construit, les mots qui sont faits
de symboles plus fréquents sont plus longs et vice-versa.  La
technique de Tunstall est la plus connue parmi celles qui construisent
des fonctions d'encodage VF.  Elle est considérée comme étant
optimale; i.e. elle construit le meilleur dictionnaire de la taille
spécifiée.<p>

En fait, elle est optimale si on se restreint à des dictionnaires
formant un code préfixe.  Or, cette contrainte n'est pas obligatoire.
Si on relaxe cette contrainte, il est parfois possible de construire
de meilleurs fonctions d'encodage que celles de Tunstall.  En
particulier, Yamamoto et Yokoo (YY) ont proposé de construire des
fonctions d'encodage qui utilisent des dictionnaires formant des codes
quasi-instantanés (almost-instantaneous VF, AIVF).  Ils ont proposé
des algorithmes pour construire des arbres, lesquels représentent les
dictionnaires, dans deux modes: le mode mono-arbre et le mode
multi-arbre.<p>

Nous avons identifié deux défauts dans les techniques de YY: celui de
la racine complète dans le mode multi-arbre et celui de l'exécution
complète des propositions d'ajout de noeuds.  Nous avons proposé des
correctifs à ces deux défauts.  Enfin, nous avons proposé un nouvel
algorithme de construction de fonctions d'encodage AIVF basé sur le
principe de la programmation dynamique.
    </resume></p><p><b>Lien</b>: <a href="files/DannyDube.pdf">diapositives</a></p></div><div id="1732017"><h3>Vendredi 17 mars 2017</h3><p style="text-align:left;"><b>Apprentissage de modèles à base de règles pour la prédiction de la résistance aux antibiotiques</b><br>Alexandre Drouin<br><i>Étudiant au doctorat, GRAAL</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La résistance aux antibiotiques est un problème de santé publique majeur qui a des implications dans la pratique de la médecine à l'échelle mondiale. La prédiction du profil de résistance d'une bactérie à partir de son génome permettrait une meilleure utilisation des agents antimicrobiens. Par exemple, il serait possible de concevoir des plans de traitements personnalisés pour chaque individu, ce qui promouvrait une meilleure utilisation des antibiotiques et ralentirait le développement de résistances dans la population.<p>

Dans ce contexte, les algorithmes d'apprentissage parcimonieux sont des outils intéressants, parce qu'ils apprennent des modèles qui dépendent d'un petit nombre de règles. Celles-ci peuvent ensuite être raffinées et interprétées par les experts du domaine. Toutefois, comme pour la majorité des problèmes d'apprentissage en génomique, les données sont peu nombreuses et de très grande dimensionnalité. Il est donc nécessaire de concevoir des algorithmes qui ont une forte résistance au surapprentissage.<p>

Une étude récente a utilisé l'algorithme Set Covering Machine (SCM) pour obtenir des modèles concis et interprétables de la résistance aux antibiotiques de 6 pathogènes humains. En quelques minutes de calcul, des mécanismes de résistance connus et validés expérimentalement ont été retrouvés, confirmant ainsi la pertinence des modèles. De plus, une comparaison à d'autres algorithmes d'apprentissage a démontré qu'en plus d'être les plus parcimonieux, les modèles appris par SCM sont très compétitifs en matière d'erreur de prédiction. Finalement, une analyse théorique de la généralisation de cet algorithme lors de l'apprentissage à partir de données génomiques a démontré une forte résistance au surapprentissage.<p>

    Dans cette présentation, j'introduirai l'algorithme SCM, ainsi qu'une implémentation efficace pour les données génomiques (<a href="https://github.com/aldro61/kover">https://github.com/aldro61/kover</a>). Je m'appuierai sur des résultats théoriques et empiriques pour démontrer que cet algorithme est bien adapté à l'apprentissage à partir de données génomiques. J'introduirai aussi une extension de ces travaux à l'apprentissage d'arbre de décision et je montrerai l'amélioration qui en découle.</resume></p><p><b>Lien</b>: <a href="files/AlexandreDrouin.pdf">diapositives</a></p></div><div id="1032017"><h3>Vendredi 10 mars 2017</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></p><p></p></div><div id="3132017"><h3>Vendredi 31 mars 2017</h3><p style="text-align:left;"><b>Andrana: Quick and Accurate Malware Detection for Android</b><br>Hana Ajakan et Andrew Bedford<br><i>Étudiants au doctorat en informatique</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Android's domination of the mobile operating system market has attracted the attention of malware authors. In addition to its large user base, what makes Android attractive to malware authors is that, contrarily to iOS users, Android users can install applications from a wide variety of sources such as first and third-party application markets (e.g., Google Play Store, Samsung Apps), torrents and direct downloads.<p>
    In order to protect Android users and their information, we have developed a lightweight malware detection tool for Android called Andrana. It leverages machine learning techniques and static analysis to determine, with an accuracy of 94.90%, if an application is malicious. Its analysis can be performed directly on a mobile device in less than a second and using only 12 MB of memory.</resume></p><p><b>Note: </b>La présentation sera donnée en français.</p></div><div id="332017"><h3>Vendredi 3 mars 2017</h3><p style="text-align:left;"><b>Généralisation de règles de filtrage pour la contrainte Cumulative</b><br>Vincent Gingras<br><i>Étudiant au doctorat, Laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Les
problèmes d'ordonnancement d'atelier occupent une place omniprésente dans la production industrielle. On définit un problème d'ordonnancement comme étant un ensemble de tâches devant être exécutées sur une ou plusieurs ressources partagées. Le lien avec la
production industrielle est par conséquent très évident, alors qu'on peut imaginer la production des différents produits comme étant les tâches et la machinerie dont dispose l'entreprise comme étant la ressource partagée. Dans les récentes années, la programmation
par contraintes s'est avérée très efficace pour la résolution de ces problèmes d'ordonnancement. Ce paradigme de programmation emploie des algorithmes de filtrage pour réduire l'espace de solution et par le fait même accélérer l'acquisition d'une solution dans
celui-ci. Plusieurs règles et algorithmes de filtrage ont été proposés dans la littérature au cours des récentes années pour les contraintes d'ordonnancement. Le filtrage complet des valeurs incohérentes des domaines des variables ne peut être réalisé en temps
raisonnable vu la NP-Complétude du problème général d'ordonnancement. Ces algorithmes reposent donc sur une relaxation du problème faisant référence à l'élasticité des tâches. Dans le cadre de ce séminaire, je présenterai une généralisation de deux de ces
règles de filtrage basée sur une fonction estimant de façon optimiste le temps de fin le plus tôt d'un ensemble de tâches. Je présenterai également deux algorithmes permettant d'appliquer ces règles généralisées en utilisant une relaxation énergétique plus
réaliste que celle utilisée dans la littérature, et permettant donc un meilleur filtrage. Les résultats expérimentaux démontrent les bienfaits de cette généralisation alors que le temps d'exécution du solveur, ainsi que le nombre de retours-arrières effectués
par celui-ci, sont réduits lorsque la méthode proposée est utilisée.
    </resume></p></div><div id="1442017"><h3>Vendredi 14 avril 2017</h3><p style="text-align:left;"><b>Vendredi Saint (pas de séminaire)</b><br></p><p></p></div><div id="2142017"><h3>Vendredi 21 avril 2017</h3><p style="text-align:left;"><b>Reconnaissance visuelle des lieux à l'aide des réseaux de neurones profonds</b><br>Amar Ali-Bey<br><i>Étudiant au doctorat, Labaratoire Damas</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>En robotique mobile, reconnaitre un lieu déjà visité par le robot, à l'aide d'une caméra, représente une composante essentielle à la cartographie de l'environnement du robot. Cette reconnaissance visuelle des lieux est un problème qui n'a pas encore été résolu en raison des changements d'apparence des lieux du monde réel. Récemment les réseaux de neurones  profonds ont montré de bonnes performances sur des tâches de vision très complexes. Les principaux facteurs derrière ces bons résultats résident dans la capacité des réseaux profonds à apprendre des millions de paramètres en utilisant de très grandes bases de données étiquetées. Encouragés par ces performances, beaucoup de  chercheurs se sont orientés vers l'utilisation de ces réseaux pour faire la reconnaissance visuelle des lieux pour la robotique mobile. C'est dans ce cadre que se situe mon projet doctoral.<p>
Dans ce séminaire, je ferai état des recherches actuelles en lien avec la reconnaissance visuelle des lieux pour la robotique mobile, je dégagerai alors des avenues de recherches pour mon doctorat et je donnerai enfin mes premiers résultats expérimentaux.</resume></p></div><div id="742017"><h3>Vendredi 7 avril 2017</h3><p style="text-align:left;"><b>L'algorithme de Pacman pour la construction efficace des codes équilibrés</b><br>Mounir Mechqrane</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Une chaîne de bits est équilibrée si elle contient un nombre de bits 0 égal au nombre de bits 1. Les codes équilibrés sont construits de chaînes de bits équilibrées. Ces codes sont appliqués dans plusieurs domaines. Par exemple, ils sont utilisés dans les systèmes VLSI, en télécommunication par fibre optique et dans les systèmes RFID. Mathématiquement, la construction de ces codes peut se faire en utilisant le codage énumératif. En informatique, cette méthode nécessite des tables de consultation qui peuvent devenir d'une grandeur inappropriée. Une grandeur qui peut être de l'ordre du nombre des atomes dans l'univers. Plusieurs travaux de recherche ont été menés pour optimiser la construction de ces codes. Toutefois, les codes créés par les techniques proposées contiennent plus de redondance que nécessaire, ce qui diminue leur efficacité. Nous présentons dans ce séminaire une nouvelle technique basée sur les permutations, le personnage de jeu vidéo PAC-MAN et les entiers à précision limitée. Nous allons montrer que la redondance introduite par notre technique est particulièrement faible, que les résultats sont nettement meilleurs que ceux des travaux antérieurs et que les complexités temporelle et spatiale utilisées sont linéaires.</resume></p><p><b>Lien</b>: <a href="files/MounirMechqrane.pdf">diapositives</a></p></div><div id="2452017"><h3>Mercredi 24 mai 2017</h3><p style="text-align:left;"><b>Maximum Margin Interval Trees</b><br>Alexandre Drouin et Toby Dylan Hocking</p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.</resume></p><p><b>Note: </b>Une partie de la présentation sera en français et une autre partie en anglais.</p><p><b>Lien</b>: <a href="files/DrouinHockingLaviolette.pdf">diapositives</a></p></div><div id="1662017"><h3>Vendredi 16 juin 2017</h3><p style="text-align:left;"><b>Traitement des données automatisé et en temps réel pour balayage mobile et modélisation en 3D</b><br>Dr. Ville Lehtola (candidat à un poste au département)</p><p><b>Heure</b>: 9h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>Je présente mon travail utilisant des sensors (cameras, scanneurs de laser), et j'essaie de répondre à des questions comme: - Quelles sont les hypothèses minimales pour localiser un scanner laser deux-dimensionnelle dans un environnement trois dimensionnelle? - Comment étalonner une caméra monoculaire mobile sans plate-forme de calibrage? - Quel sera l'avenir de la perception?</resume></p><p><b>Biographie: </b>Bac. 1999 Lycée franco-finlandais d'Helsinki. Service militaire obligatoire finlandais, 2000. Échange d'étudiant aux États-Unis, 2005. MSc (physique), 2006, Helsinki University of Technology. Doctorat de physique statistique (computationnelle) "Dynamics of Single Biopolymer Translocation and Sedimentation", 2010, Aalto University. Grant from Academy of Finland (286k eur, PI): "Automated Indoor Visualization and 3D Modeling" 2012-2015. Senior research scientist, photogrammetry and remote sensing, Finnish Geospatial research Institute FGI, 2016.</p></div><div id="1962017"><h3>Lundi 19 juin 2017</h3><p style="text-align:left;"><b>La robotique mobile en milieux nordiques - Comment y arriver?</b><br>Dr. François Pomerleau (candidat à un poste au département)</p><p><b>Heure</b>: 9h30<br><b>Local</b>: PLT-2704</p><p><b>Résumé: </b><resume>La robotique mobile et les voitures autonomes ne sont plus limitées aux ouvrages de science-fiction. De nouveaux moyens de transport sont développés dans les régions tempérées et seront testés sous peu au Canada. Quelle sera la robustesse de ces systèmes lors de tempêtes? Peut-on naviguer hors route ou en environnements n'ayant pas été cartographiés? Qu'arrivera-t-il lorsque la topographie de l'environnement changera rapidement, par exemple lors d'une chute de neige importante?
    Ce séminaire élaborera autour de ces questions et se déroulera en trois parties. La première proposera les fondements théoriques de la localisation à l'aide de données laser. L'algorithme de recalage présenté correspond à ce qui est utilisé lors des démonstrations d'autonomie avancées, comme utilisé pour la voiture de Google. La deuxième partie présentera quelques travaux de recherche reliés à la localisation de systèmes autonomes et à leur utilisation en surveillance de l'environnement. La dernière partie proposera les grandes lignes d'un programme de recherche visant l'utilisation et la valorisation de la robotique mobile en milieux nordiques.</resume></p><p><b>Biographie: </b>François Pomerleau a fait ses premières armes en recherche en interagissant avec l'Agence spatiale canadienne et l'Agence spatiale européenne lors de ses études en génie informatique à l'Université de Sherbrooke. Il a obtenu sa maîtrise de la même université en 2009 après un séjour d'un an à l'EPFL (Lausanne - Suisse) où il a travaillé sur un prototype de voiture autonome. Il a complété son doctorat à l'ETH Zurich (Suisse) en 2013 durant lequel il a participé à plusieurs déploiements robotiques en environnements non contrôlés, entre autre avec des brigades de pompiers européennes et dans les lacs alpins. Après des activités de transfert technologique chez Alstom Inspection Robotics et un séjour à l'Université Laval, il fut boursier postdoctoral du Conseil de recherches en sciences naturelles et en génie du Canada afin de continuer ses recherches à l'Université de Toronto. Il est présentement chercheur postdoctoral à l'Université Laval dans le Laboratoire de robotique et travaille, en collaboration avec l'industrie, à l'élaboration de la manufacture 4.0.</p></div><div id="26102017"><h3>Jeudi 26 octobre 2017</h3><p style="text-align:left;"><b>Quelques exemples de Perception et Vision Artificielles pour la Commande des Systèmes Robotiques au sein de l'Institut Pascal, Université Clermont Auvergne, CNRS, SIGMA Clermont, France</b><br><a href="http://www.institutpascal.uca.fr/index.php/fr/">Paul Checchin</a><br><i>Institut Pascal</i></p><p><b>Heure</b>: 15h30<br><b>Local</b>: PLT-2744</p><p><b>Résumé: </b><resume>Sa présentation concernera les activités menées au sein du laboratoire Institut Pascal dans le domaine de la Perception et de la Vision Artificielles pour la Commande des Systèmes Robotiques. Ainsi, au cours de son exposé, les réalisations de l'Institut Pascal autour de ce domaine seront évoquées, notamment :
 les architectures matérielles et logicielles pour la perception artificielle;
 la reconnaissance automatique et suivi en temps réel de motifs visuels par des techniques d'apprentissage et plus particulièrement de réseaux de neurones profonds;
 la localisation absolue de véhicules mobiles (tout type d'environnement) et cartographie automatique;
 la modélisation et commande de systèmes robotiques;
 la navigation autonome de systèmes robotiques.
</resume></p><p><b>Biographie: </b>Paul Checchin est Docteur en Vision pour Robotique de l'Université de Blaise Pascal de Clermont-Ferrand depuis 1996. Il a rejoint l'Axe ISPR de l'Institut Pascal en 2004. En 2012, il a présenté son Habilitation à Diriger des Recherches intitulée « Perception multi-sensorielle : contributions à la détection et au suivi d'objets mobiles, à la localisation et la cartographie ». Actuellement, il dirige l'équipe PerSyst (Perception Systems: systèmes de
perception) de l'Axe ISPR.</p></div><div id="6102017"><h3>Vendredi 6 octobre 2017</h3><p style="text-align:left;"><b>Une nouvelle approche de modélisation des systèmes hypermédias adaptatifs utilisés en éducation basée sur les réseaux sociaux</b><br>Kenza Sakout Andaloussi<br><i>Étudiante au doctorat au laboratoire ERICAE</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2744</p><p><b>Résumé: </b><resume>Les systèmes d'apprentissage en ligne représentent une alternative intéressante permettant de fournir aux apprenants du contenu approprié à leurs besoins de formation et à leur façon d'apprendre. Parmi ces systèmes, les systèmes hypermédias adaptatifs en éducation (SHAE) s'inscrivent comme les plus connus  et les plus utilisés en matière de personnalisation ou d'adaptation de l'apprentissage. En effet, la personnalisation de l'apprentissage, comme le montrent les dernières avancées pédagogiques, est considérée comme la clé d'activation de la motivation des apprenants, ce qui implique un apprentissage plus efficace. Ayant comme finalité d'améliorer les modèles de SHAE existants, nous présentons les résultats d'une analyse comparative de 50 SHAE existants. Grâce à une telle analyse, nous avons pu déceler les différentes approches autour des trois principaux modèles constituant les SHAE à savoir: le modèle apprenant, le modèle du domaine et le modèle d'adaptation. Nous avons pu également mettre en évidence les avantages et les limites de chaque approche. Ainsi, nous proposons une nouvelle façon d'améliorer les modèles de SHAE tout en se basant sur les réseaux sociaux, ce qui nous permettra de maximiser la performance de tels systèmes et ainsi mieux satisfaire les besoins des apprenants.</resume></p><p><b>Lien</b>: <a href="files/KenzaSakoutAndaloussi.pdf">diapositives</a></p></div><div id="10112017"><h3>Vendredi 10 novembre 2017</h3><p style="text-align:left;"><b>Simple, Robust Interaction between Humans and Robot Teams</b><br><a href="https://rtv.github.io">Richard Vaughan</a><br><i>Associate Professor of Computing Science at Simon Fraser University</i></p><p><b>Heure</b>: 14h30<br><b>Local</b>: PLT-3904</p><p><b>Résumé: </b><resume>Sensing technology for robots has improved dramatically in the last few years, but we do not see robots around us yet. How should robots behave around people, animals and each other to get things done? My group works on behavioural strategies for mobile robots that exploit the new sensing capabilities, and allows them to perform sophisticated, robust interactions with the world and other agents.</resume></p><p><b>Biographie: </b>Richard Vaughan is Associate Professor of Computing Science at Simon Fraser University, where he directs the Autonomy Lab. Previously he was at HRL Laboratories, the University of Southern California, and Oxford University.  His research interests include long-term autonomous robots, multi-robot systems, behavioural ecology, human-robot interaction, and devising tools and techniques for developing and experimenting with robots. He demonstrated the first robot to control animal behaviour in 1998, co-created the Player/Stage Project in 2000, and recently showed the first uninstrumented HRI with teams of UAVs.  He currently serves on the administrative committee of the IEEE Robotics and Automation Society, and the editorial board of the Autonomous Robots journal, and was Program Chair for IROS 2017.</p></div><div id="15122017"><h3>Vendredi 15 d&eacute;cembre 2017</h3><p style="text-align:left;"><b>Building and Evaluating Data-Driven Neural Dialogue Systems</b><br>Jo&euml;lle Pineau<br><i>Professor, Reasoning and Learning Lab, School of Computer Science, McGill University</i></p><p><b>Heure</b>: 11h00<br><b>Local</b>: VCH-3068</p><p><b>Biographie: </b>Joëlle Pineau est la co-directrice du Reasoning and Learning Lab, School of Computer Science à l'Université McGill. La recherche de la Professeure Pineau est consacrée au développement de nouveaux modèles et algorithmes qui permettent aux ordinateurs d'apprendre à prendre de bonnes décisions dans des domaines complexes du monde réel, même dans des situations où l'information est incomplète ou incorrecte. Elle travaille également sur l'application de ces algorithmes à des problèmes complexes en robotique et en soins de santé. La Professeure Pineau est membre principal (Senior Fellow) de l'Institut canadien de recherches avancées et membre du Centre des Machines Intelligentes de McGill.<p>
 
Voici quelques liens pertinents qui donnent plus d'information sur ces travaux de recherche:<p>
 
<a href=https://arxiv.org/abs/1711.09050>https://arxiv.org/abs/1711.09050</a><p>
<a href=https://arxiv.org/abs/1709.02349>https://arxiv.org/abs/1709.02349</a><p>
<a href=https://arxiv.org/abs/1708.07149>https://arxiv.org/abs/1708.07149</a><p>
<a href=https://arxiv.org/pdf/1611.06216.pdf>https://arxiv.org/pdf/1611.06216.pdf</a>
    </p><p><b>Note: </b>La présentation sera donnée en français (les diapositives seront en anglais) Il s'agit d'un séminaire REPARTI organisé en collaboration avec le département d'informatique et de génie logiciel.</p></div><div id="2612018"><h3>Vendredi 26 janvier 2018</h3><p style="text-align:left;"><b>Robots in the Real World: Accounting for Resource Constraints To Achieve Tasks</b><br><a href="http://people.csail.mit.edu/lpaull/">Prof. Liam Paull</a><br><i>Professeur adjoint, Université de Montréal</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Robots that operate in the physical world have finite resources and hard realtime constraints. For autonomous systems to be able to perform useful tasks, their algorithms must scale well with time, space, and complexity. In this talk I will present a framework for resource-constrained graphical inference. The approach optimally utilizes available computation, memory, and bandwidth to provide the best possible estimates of the state of the system given the limited resources. The flexibility of the framework is demonstrated in two diverse application domains: underwater cooperative localization and mapping with communication constraints, and collision-free navigation with computation and memory constraints. In both cases, we exploit task-specific structure to optimize task performance without exceeding the finite available resources. I will also then take some time to describe some new projects and directions that I'm interested to explore in the newly formed robotics group at Université de Montréal.
    </resume></p><p><b>Biographie: </b>Liam Paull is an assistant professor at Université de Montréal in the Université de Montréal Département d'informatique et de recherche opérationnelle (DIRO), and an associate member of the Montreal Institute for Learning Algorithms (MILA).  He received the B.Sc. degree in computer engineering from McGill University, Montreal, QC, Canada, in 2004 and the Ph.D. degree in electrical and computer engineering from University of New Brunswick, Fredericton, NB, Canada, in 2013, where he studied marine robotics. He was previously a Research Scientist and postdoctoral associate in the Computer Science and Artificial Intelligence Laboratory (CSAIL) at Massachusetts Institute of Technology where he led the Toyota Research Institute Funded "Parallel Autonomy" self-driving car project. His research interests include inference on resource-constrained and safety-critical robotic systems as well as bridging deep learning methods to develop novel representations for increased mobile robot autonomy.</p></div><div id="912018"><h3>Mardi 9 janvier 2018</h3><p style="text-align:left;"><b>Méta-apprentissage pour la classification semi-supervisée à partir de peu de données</b><br><a href="http://info.usherbrooke.ca/hlarochelle/index_fr.html">Hugo Larochelle</a><br><i>Google Brain, Montréal, Professeur Associé, Dép. d'informatique, Université de Sherbrooke</i></p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-2700</p><p><b>Résumé: </b><resume>Dans la classification à partir de peu de données ("few-shot classification"), nous sommes intéressés aux algorithmes d'apprentissage qui peuvent produire un classificateur à partir d'une poignée d'exemples étiquetés. Les progrès récents dans ce domaine sont basés sur le méta-apprentissage, où un modèle paramétré d'un algorithme d'apprentissage est défini et entraîné sur des épisodes représentant différents problèmes de classification, chacun ayant un petit ensemble d'entraînement étiqueté et son ensemble de test correspondant. Dans les travaux derrière cette présentation, nous avançons ce paradigme de classification vers un scénario où des exemples non-étiquetés sont également disponibles dans chaque épisode. Pour s'attaquer à ce paradigme, nous proposons de nouvelles extensions des "Prototypical Networks" (Snell et al., 2017) qui sont augmentés par la possibilité d'utiliser des exemples non-étiquetés lors de la production de prototypes. Nos expériences confirment que l'approche proposée peut apprendre à améliorer ses prédictions grâce aux exemples non-étiquetés, comme le ferait un algorithme d'apprentissage semi-supervisé.</resume></p><p><b>Biographie: </b>Hugo Larochelle est chercheur et responsable de l'équipe Google Brain à Montréal. Il est également professeur adjoint à l'Université de Montréal et à l'Université de Sherbrooke, ainsi que directeur associé du programme "Learning in Machines and Brains" de l'Institut Canadien de Recherches Avancées (ICRA).</p><p><b>Note: </b>La présentation sera donnée en français (les diapositives seront en anglais).</p></div><div id="2322018"><h3>Vendredi 23 f&eacute;vrier 2018</h3><p style="text-align:left;"><b>Algorithme de vérification du raisonnement énergétique en O(n log<sup>2</sup> n) et algorithme de filtrage en O(n<sup>2</sup> log n).</b><br>Yanick Ouellet<br><i>Étudiant à la maîtrise, Laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La contrainte Cumulative est couramment utilisée pour résoudre des problèmes
d'ordonnancement à l'aide de la programmation par contraintes. Le raisonnement énergétique est une technique de filtrage très puissante
pour cette contrainte. Toutefois, ses algorithmes de vérification
et de filtrage traitent O(n<sup>2</sup>) intervalles de temps, ce qui les rend trop lents
pour être utilisés en pratique. Nous présentons une nouvelle technique permettant de
traiter uniquement O(n log n) intervalles. Nous montrons également comment calculer
l'énergie dans un intervalle en O(log n). Cela nous permet de proposer un algorithme
de vérification en O(n log<sup>2</sup> n) et un algorithme de filtrage en O(n<sup>2</sup> log n).
En pratique, ces deux algorithmes appliquent le raisonnement énergique plus rapidement
que l'état de l'art.</resume></p><p><b>Lien</b>: <a href="files/YanickOuellet2018.pdf">diapositives</a></p></div><div id="1622018"><h3>Vendredi 16 f&eacute;vrier 2018</h3><p style="text-align:left;"><b>Extraction d'informations à l'aide de réseaux de neurones appliqué au domaine juridique</b><br>Nicolas Garneau<br><i>Étudiant au doctorat, GRAAL</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Dans le domaine juridique, de nombreux jugements sont rendus publics et les informations qu'ils contiennent pourraient être réutilisées pour des fins de jurisprudence, d'analyse de risque (financiers ou autre) ou de compilation de statistiques. Cependant, le format non structuré de ces jugements (des textes) représente un frein à l'exploitation des informations pertinentes.<p>

Au cours des derniers mois, nous avons mené une étude sur des documents de la cour des petites créances comportant des jugements favorables ou non par rapport à une personne ou compagnie. Plusieurs méthodes existent pour arriver à extraire l'information de tels documents et je vous en présenterai une qui utilise un corpus annoté, soit une méthode supervisée. Nous avons mis en place une manière rapide et efficace pour construire un modèle d'extraction d'information en partant de zéro, c.-à-d. sans corpus annoté. Je présenterai d'abord 2 modèles d'extraction d'information, soient un modèle d'extraction d'entités nommées (Named Entity Recognition) et un modèle d'extraction de relations (Relation Extraction). Ces deux modèles sont mis en oeuvre par des réseaux de neurones et constitueront l'objet principal de cette présentation. Je présenterai ensuite un outil d'annotation fréquemment utilisé dans la communauté du traitement automatique de la langue naturelle (TALN ou NLP en anglais) pour ajouter efficacement les annotations à notre corpus. J'ai modifié cette librairie pour utiliser mes modèles d'extraction et ainsi itérer rapidement sur le processus d'annotation/entraînement de modèle ce qui accélère de beaucoup la construction d'un corpus de texte annoté. Je conclurai par la présentation des résultats expérimentaux que nous avons obtenus dans le cadre de ces travaux.
</resume></p></div><div id="922018"><h3>Vendredi 9 f&eacute;vrier 2018</h3><p style="text-align:left;"><b>Le contrôle du flot d'information au niveau des applications pour garantir la confidentialité et l'intégrité des données</b><br>Josée Desharnais et Nadia Tawbi<br><i>Professeures titulaires au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nous dépendons de plus en plus des systèmes informatiques pour le stockage et la manipulation des données confidentielles ainsi que pour le maintien de l'intégrité de ces données. Dans un monde hautement interconnecté et numérisé, nous aurons inévitablement besoin d'utiliser des applications provenant de sources non fiables. Ces applications risquent de divulguer de l'information confidentielle ou de corrompre les données lors de leur utilisation. Les systèmes largement utilisés comme le contrôle d'accès ou la détection de virus ne sont pas suffisants pour offrir les garanties demandées. Le contrôle d'accès n'agit pas à un niveau fin, et les antivirus détectent des codes malicieux déjà répertoriés en se basant sur des analyses plutôt syntaxiques.<p>
    Les techniques de contrôle du flot d'information sont prometteuses et permettent l'utilisation d'applications provenant de sources non fiables, et ce en analysant, sémantiquement, le code de ces applications avant l'exécution et en rejetant celles qui comportent des risques. Les approches hybrides permettent d'accepter plus d'applications même dans le doute, et ce en insérant dans le code des tests applicables durant l'exécution. Le travail présenté se situe dans les approches hybrides de contrôle de flot d'information combinant un contrôle statique et un contrôle dynamique. </resume></p><p><b>Lien</b>: <a href="files/LocalVersionComputerAndSecurity.pdf">article</a></p></div><div id="222018"><h3>Vendredi 2 f&eacute;vrier 2018</h3><p style="text-align:left;"><b>Apprentissage bayésien de graphe dirigé acyclique appliqué à l'identification du modèle en apprentissage machine</b><br><a href="http://crdm.ulaval.ca">Patrick Dallaire</a><br><i>Professionnel de recherche au CRDM</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Le graphe dirigé acyclique est une structure fréquemment utilisée en apprentissage automatique, plus particulièrement dans la modélisation de réseaux bayésiens et de réseaux de neurones. Comment apprendre une telle structure? Le point de vue bayésien suggère de pondérer tous les modèles en fonction de leur performance et de combiner l'ensemble de ces modèles en suivant cette pondération (Bayesian model Averaging). Une telle approche nécessite de formuler sous forme de distribution de probabilité nos a priori sur l'espace des modèles, ici les graphes dirigés acycliques. Lors de ce séminaire, il sera question du processus des chefs Indiens (PCI), un processus stochastique correspondant à une distribution de probabilité sur l'espace des graphes dirigés acycliques infinis. Nous verrons dans un premier temps une application du PCI à l'inférence bayésienne non paramétrique de réseaux bayésiens infinis avec variables cachées. Dans un second temps, nous couvrirons une application possible du PCI à l'apprentissage de réseaux de neurones infinis, un projet de recherche en cours visant à combiner l'apprentissage bayésien non paramétrique et les réseaux de neurones profonds.</resume></p><p><b>Lien</b>: <a href="files/Dallaire2018.pdf">diapositives</a></p></div><div id="2332018"><h3>Vendredi 23 mars 2018</h3><p style="text-align:left;"><b>Apprendre les paramètres de contraintes à partir de leurs solutions : une histoire de statistiques</b><br>Émilie Picard-Cantin<br><i>Étudiante au doctorat, Laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Construire un modèle d'optimisation pour un problème donné est une tâche complexe qui est continuellement à recommencer. En milieu médical par exemple, les horaires de travail de différents groupes de médecins ont souvent les mêmes types de contraintes avec des paramètres différents. Par conséquent, la génération d'un horaire de travail pour chaque groupe représente un nouveau problème à résoudre et un nouveau modèle d'optimisation à déterminer. Comment fait-on pour accélérer la création de ces modèles d'optimisation qui doivent être adaptés à chaque nouveau groupe de médecins?<p>

Et si on vous donnait des exemples de bons horaires ainsi qu'une liste de types de contraintes potentielles, seriez-vous capable de produire manuellement un horaire semblable? Évidemment, vous auriez besoin de beaucoup de temps et de plusieurs exemples afin de produire un nouvel horaire satisfaisant.<p>

Et si on vous fournissait un outil qui prend en entrée des exemples et un type de contraintes précis et qui vous retourne les meilleurs paramètres possibles, votre travail serait nettement plus facile, non? En fait, vous verrez que nous pouvons réduire ce problème à une histoire de statistiques.
</resume></p></div><div id="932018"><h3>Vendredi 9 mars 2018</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></p><p></p></div><div id="232018"><h3>Vendredi 2 mars 2018</h3><p style="text-align:left;"><b>Support vector comparison machines</b><br><a href="http://tdhock.github.io">Toby Hocking</a><br><i>Postdoctoral fellow, McGill University</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>In ranking problems, the goal is to learn a ranking function
  from labeled pairs of
  input points. In this paper, we consider the related comparison
  problem, where the label {-1,0,1} indicates which element of
  the pair is better {-1, 1}, or if there is no significant difference {0}. We
  cast the learning problem as a margin maximization, and show that it
  can be solved by converting it to a standard SVM. We use simulated
  nonlinear patterns and a real learning to rank sushi data set to
  show that our proposed SVMcompare algorithm outperforms SVMrank when
  there are equality y=0 labels. In addition, we show that SVMcompare outperforms
    the ELO rating system when predicting the outcome of chess matches.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Liens</b>: <a href="https://arxiv.org/abs/1401.8008">article</a>, <a href="https://github.com/tdhock/compare-slides/blob/master/HOCKING-compare-slides.pdf">diapositives</a></p></div><div id="3032018"><h3>Vendredi 30 mars 2018</h3><p style="text-align:left;"><b>Vendredi Saint (pas de séminaire)</b><br></p><p></p></div><div id="1632018"><h3>Vendredi 16 mars 2018</h3><p style="text-align:left;"><b>Se préparer à un monde où l'intelligence artificielle sera omniprésente</b><br>Brahim Chaib-draa<br><i>Professeur titulaire, département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'intelligence artificielle (IA) et l'apprentissage machine ont submergé notre vie ces dernières années, d'autant diront comme un Tsunami. Cette tendance va continuer durant bien des années et on pourrait se demander si (dans bien des années) l'IA dépassera l'humain et deviendra une "superintelligence" comme le prétendent certains.<p>

Dès lors bien des questions se posent : faut-il craindre l'IA? Courons-nous des risques avec l'IA? Faut-il concevoir une IA éthique? Ou faut-il faire appel à l'éthique et à la responsabilité des chercheurs pour concevoir une certaine IA qui s'alignerait avec les valeurs humaines?<p>

Dans cette présentation, j'essaierai d'apporter des éléments de réponse à ces questions tout en indiquant où en est actuellement la recherche et comment elle s'oriente concernant les années à venir.<p>

Je finirai en mettant l'accent sur l'espoir d'une vie "paradisiaque" que suscite l'IA en reprenant Khalil Gibran "l'espoir d'un paradis c'est déjà le paradis".</resume></p></div><div id="2742018"><h3>Vendredi 27 avril 2018</h3><p style="text-align:left;"><b>La procrastination structurée</b><br>Andrew Bedford<br><i>Étudiant au doctorat</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Avez-vous quelque chose d'important à faire? Cherchez-vous une excuse pour reporter ce travail à plus tard? Eh bien ne cherchez plus! Venez assister au séminaire départemental sur la "procrastination structurée" pour apprendre comment procrastiner et rester productif! J'en profiterai pour vous présenter rapidement trois projets sur lesquels j'ai travaillé en utilisant cette technique : (1) Ott-IFC, un outil qui génère des mécanismes de contrôle de flot d'information à partir de la spécification d'un langage de programmation; (2) Coqatoo, un outil qui génère des preuves en langue naturelle à partir de preuves écrites en Coq; (3) Paper Scout, un outil qui représente visuellement les articles scientifiques de votre librairie pour vous aider à rapidement créer des états de l'art.
    </resume></p><p><b>Liens</b>: <a href="files/AndrewBedford2018.pdf">diapositives</a>, <a href="https://github.com/andrew-bedford">GitHub</a></p></div><div id="2042018"><h3>Vendredi 20 avril 2018</h3><p style="text-align:left;"><b>Pré-optimisation de la position des outils d'une machine CNC selon une séquence arbitraire de production</b><br>Marc-André Ménard<br><i>Étudiant au doctorat, FORAC</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>APN est une entreprise oeuvrant dans le domaine aéronautique qui
requière un usinage de haute précision. Elle utilise des machines
à contrôle numérique prenant en entrée un programme dictant les
actions de la machine pour usiner un produit.  Lorsque l'on passe
de l'usinage d'un produit à un autre, on doit arrêter la machine
afin d'y changer les outils.  Il est possible de réduire ce temps
de transition lorsqu'un outil est utilisé à la même position dans
la machine pour l'usinage de deux produits consécutifs. Je
présenterai un programme à nombres entiers qui optimise
l'emplacement des outils pour l'usinage des produits afin de
réduire le temps de transition selon une séquence arbitraire de
production. J'expliquerai les différentes techniques qui ont été
nécessaires afin que le solveur réussisse à résoudre le problème
dans des délais raisonnables.</resume></p></div><div id="1342018"><h3>Vendredi 13 avril 2018</h3><p style="text-align:left;"><b>Rétrospective de quarante ans de recherches en analyse, modélisation et représentation des informations et des connaissances pour la conception de systèmes à base de connaissances</b><br>Bernard Moulin<br><i>Professeur titulaire, département d'informatique et de génie logiciel, Université Laval</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Quand j'ai reçu l'invitation de présenter en séminaire une rétrospective de ma carrière de chercheur, j'ai trouvé l'idée sympathique et stimulante ... et j'ai accepté de bon coeur. Plus tard, quand j'ai commencé à réfléchir à cette présentation, j'ai réalisé que je me trouvais en face d'un véritable problème ... de données massives! On n'échappe pas à son temps, n'est-ce pas? Mais comment l'aborder? 40 ans de recherches actives, ce n'est pas rien, surtout quand toutes les données sont textuelles, une grande partie étant manuscrite. Après un bon moment de réflexion j'ai choisi d'utiliser SelfBrainPower, une vieille suite d'algorithmes toujours renouvelée ... mais souvent ignorée des jeunes générations qui préfèrent BigDataCruncher! J'ai mis au point une démarche de données massives appropriée au problème en fonction des sources de données disponibles : mon CV (62 pages) et de 38 agendas de poche (format papier). Ce séminaire présente avec humour et beaucoup de modestie les résultats de cette analyse qui devrait vous permettre de prendre connaissance d'une page d'histoire de recherches menées au Département d'informatique et de génie logiciel de l'Université Laval pendant presque 0.4 siècle, dont près de 25 ans au Centre de recherche en géomatique,  dans un domaine en voie de disparition qui vous permettra de 'voyager' de l'analyse et la conception de systèmes d'information à la représentation des connaissances en vue de modéliser et concevoir des systèmes à base de connaissances. Une revue historique de recherches intenses et productives de la 'vieille intelligence artificielle' ... qui nous a permis de réaliser des systèmes experts, des systèmes de planification intelligents, des systèmes multi-agents, des systèmes de géo-simulation à base d'agents, puis à base de populations; de représenter des connaissances dans un grand nombre de domaines (sociologie, archéologie, géographie, ingénierie, géomatique, cartographie, santé environnementale, traitement du langage naturel, modélisation des conversations, modélisation qualitative de formes géographiques, etc.) avec un grand nombre de partenaires universitaires (nationaux et internationaux), gouvernementaux (organismes de recherche en santé et d'autres pour la Défense, divers ministères provinciaux) et des entreprises. Attachez vos ceintures, vous pourriez ressentir une certaine sensation de désorientation à l'issue du voyage qui vous est proposé ici!</resume></p><p><b>Note: </b>Le séminaire devrait durer un peu plus d'une heure.</p><p><b>Lien</b>: <a href="files/MoulinRetrospCarriereAvril18.pdf">diapositives</a></p></div><div id="642018"><h3>Vendredi 6 avril 2018</h3><p style="text-align:left;"><b>Few-shot learning with Kernel Ridge Regression</b><br>Prudencio Tossou<br><i>Étudiant au doctorat, GRAAL</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Recent advances in few-shot learning has provided new ways to train deep neural networks with few data points. While specialized models have been proposed for classification, reinforcement learning and generative models, only few studies have addressed few-shot regression. We propose a metric learning algorithm to fulfill that need. The idea is to learn a feature space in which a linear predictor can fit few data points and still be able to generalize very well. We use Kernel Ridge Regression algorithm to find that predictor and to enforce the generalization criterion. The mapping from the input space to the feature space and the trade-off between fitting the data points and generalization are learned through meta-learning. Our experiments constitute  successful applications of few-shot learning in the pharmaceutical and clinical settings, and demonstrate  that our model is quite competitive with the state of the art.</resume></p><p><b>Note: </b>La présentation sera donnée en français</p><p><b>Lien</b>: <a href="files/tossou2018.pdf">diapositives</a></p></div><div id="452018"><h3>Vendredi 4 mai 2018</h3><p style="text-align:left;"><b>Some Recent Insights on Transfer-Learning in Nonparametric Settings</b><br><a href="http://www.princeton.edu/~samory/">Samory Kpotufe</a><br><i>Assistant Professor, ORFE, Princeton University</i></p><p><b>Heure</b>: 13h00<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Transfer-Learning aims to use data from a 'source' distribution S, towards improving learning performance w.r.t. a target distribution T. One can for instance think of having a large database of images from the web (the source) to be used towards classifying images from a specific domain, say traffic in New York (the target). Much research effort has therefore gone into understanding which relations between T and S allow information-transfer between T and S in such prediction tasks, and more practically, in understanding the relative benefits of source and target data.<p>

In this talk we consider 'nonparametric' settings, i.e., we make little assumptions about S and T, and instead aim to characterize (S,T) so as to capture a continuum from easy to hard transfer problems, and furthermore to tightly capture the relative benefits of source and target data.<p>

In the first part of the talk, we will discuss nonparametric insights on importance sampling methods - a very common algorithmic approach to transfer, involving the estimation of density-ratios, and show that these methods can greatly benefit from structured data (such as manifold or sparse data), attesting to their practical success, however might be solving too-hard a problem in some situations when transfer is easy, and are ill-defined in common situations where transfer is hard but still possible at reasonable rates.<p>

In the second part of the talk, I will argue that an asymmetric notion of relative dimension between S and T, tightly captures the minimax rates of transfer. In particular, this notion reveals a rich set of situations where transfer is possible even at fast rates, even though traditional measures of divergence between S and T might remain large or undefined. Surprisingly, our minimax analysis shows that unlabeled target data have minor benefits in transfer, while few labeled target data can greatly improve the rates of transfer. Furthermore, we are able to characterize sharp thresholds at which the benefits of source data saturates given available target data. Finally, we show that such a threshold (a priori unknown) can nearly be achieved by an adaptive sampling procedure with no knowledge of distributional parameters.<p>

    The talk is partly based on recent work with Guillaume Martinet.</resume></p><p><b>Biographie: </b>Samory Kpotufe is Assistant Professor at ORFE, Princeton University, and works at the intersection of Machine Learning and modern Statistical areas such as Nonparametrics and High-dimensional Inference. He obtained his PhD in 2010 in Computer Science at UC San Diego, followed by postdoctoral research positions at the MPI for Intelligent Systems, and the Toyota Technological Institute at Chicago.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p></div><div id="3112020"><h3>Vendredi 31 janvier 2020</h3><p style="text-align:left;"><b>Faire face aux défis de l'utilisabilité des APIs : du développement au déploiement des applications logicielles</b><br>Mohamed Aymen Saied<br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>L'un des principaux défis de l'industrie du logiciel est de développer des systèmes à grande échelle, de haute qualité, et dans les délais requis. Dans ce contexte, la productivité des développeurs, est un facteur clé. La réutilisation logicielle peut améliorer considérablement la productivité des développeurs, et les systèmes à grande échelle sont généralement construits en se basant sur des librairies logicielles existantes à travers leurs APIs « Application Programming Interfaces». Les APIs fournissent un mécanisme de réutilisation du code afin que les développeurs puissent bâtir au-dessus de ce que d'autres ont déjà développé. Toutefois, une utilisation incorrecte des APIs peut générer des coûts supplémentaires de débogage et de correction. Dans cette présentation, j'exposerai les différents défis de l'utilisabilité des librairies logicielles. Ensuite je montrerai comment les techniques d'analyse statique et dynamique de programme peuvent aider à faire face à ces défis tout en étant pragmatique dans les sources de données considérées. Vers la fin de la présentation, j'exposerai brièvement les opportunités et les défis de l'utilisation des APIs dans le contexte de la nouvelle génération des architectures logicielles distribuées tels que les microservices.</resume></p><p><b>Lien</b>: <a href="files/AymenSaied.pdf">diapositives</a></p></div><div id="2412020"><h3>Vendredi 24 janvier 2020</h3><p style="text-align:left;"><b>Spécialisation sémantique des modèles de représentation distributionnels</b><br>Roxane Debruyker<br><i>Étudiante au doctorat (directeur de recherche: Richard Khoury)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Les modèles basés la sémantique distributionnelle tels que GloVE et word2vec utilisent les co-occurrences entre les mots dans des gros corpus en langages naturels. Bien que ces co-occurences démontrent des relations entre les concepts, l'information sur le type de relations qui les relie n'est pas explicitement encodé. Un exemple de type d'erreur et de conséquence sur une tâche est qu'il est parfois difficile de discerner les synonymes des antonymes, ce qui mène à de graves confusions dans des tâches de simplification de texte. Les lexiques sémantiques (qui sont un type de graphe de connaissances), tels que WordNet ou ConceptNet, annotent explicitement les relations qui relient différents concepts sans être suffisants pour pré-entrainer des word embeddings de manière pertinente. Ce séminaire présente quelques enjeux, techniques et applications de l'utilisation des lexiques sémantiques pour augmenter la qualité de représentation des word embedding distributionnels.</resume></p><p><b>Lien</b>: <a href="files/Debruyker.pdf">diapositives</a></p></div><div id="1712020"><h3>Vendredi 17 janvier 2020</h3><p style="text-align:left;"><b>Apprentissage de réseaux de neurones à activations binaires avec garanties statistiques
    </b><br>Gaël Letarte<br><i>Étudiant au doctorat, membre du GRAAL</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Malgré les exploits empiriques des réseaux de neurones profonds, il existe peu de garanties théoriques solides expliquant leurs performances. Nous allons présenter nos récents travaux portant sur l'analyse des réseaux de neurones profonds avec activations binaires, pour lesquels nous pouvons obtenir des garanties statistiques sur leur taux d'erreur. Il en découle une approche originale permettant d'entraîner un modèle composé d'une agrégation de tels réseaux. Cette analyse se base sur la théorie PAC-Bayésienne, qui s'est illustrée récemment comme un outil fructueux pour analyser la capacité de généralisation des réseaux de neurones profonds.
    </resume></p><p><b>Liens</b>: <a href="files/GaelLetarte2020.pdf">diapositives</a>, <a href="https://arxiv.org/pdf/1905.10259">article</a></p></div><div id="722020"><h3>Vendredi 7 f&eacute;vrier 2020</h3><p style="text-align:left;"><b>Apprendre la sensitivité d'un ordonnancement en analysant le processus de recherche</b><br>Marc-André Ménard<br><i>Étudiant au doctorat, Consortium de recherche en ingénierie des systèmes industriels 4.0</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La résolution du problème est une partie importante de l'optimisation. Une autre partie tout aussi importante est l'analyse de la solution où plusieurs questions peuvent survenir. Par exemple, pour un problème d'ordonnancement, est-il possible d'obtenir une meilleure solution en augmentant la capacité d'une ressource? Qu'elle est l'impact sur la valeur de la fonction objectif de finir une tâche plus tôt? Répondre à ces questions est important pour l'explication et l'acceptabilité de la solution. Beaucoup de recherche a été effectuée sur l'analyse de sensibilité, mais peu de méthodes peuvent s'appliquer à la programmation par contraintes. Nous présentons une nouvelle méthode d'analyse de sensibilité pour la programmation par contraintes. Notre méthode collecte des informations sur la propagation de la contrainte Cumulative et le filtrage des variables lors de la recherche et sur la solution retournée par le solveur. En utilisant des algorithmes d'apprentissage automatique, nous prédisons si augmenter/réduire la capacité d'une ressource cumulative permet une meilleure solution. Nous prédisons également l'impact sur la valeur de la fonction objectif lorsque l'on force une tâche à finir plus tôt. Nous validons notre méthode sur le problème de gestion de projet à contraintes de ressources.</resume></p><p><b>Lien</b>: <a href="files/Menard2020.pdf">diapositives</a></p></div><div id="2122020"><h3>Vendredi 21 f&eacute;vrier 2020</h3><p style="text-align:left;"><b>Preuves zero-knowledge pour NP, relativistes et réalisables</b><br><a href="https://www.cs.mcgill.ca/~crepeau/">Claude Crépeau</a><br><i>Professeur à l'Université McGill</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Nous présenterons comment il est possible de démontrer un énoncé dans NP
sans transférer sa preuve (zero-knowledge) grâce aux contraintes posées
par la relativité: que l'information ne peut voyager plus vite que la lumière.
Nous verrons comment réaliser une telle chose en pratique bien que les
prouveurs ne soient que limité par ce qui leur est permis par la physique quantique.
Nous soutiendrons enfin que la même méthode ne peut être consistante face à
des prouveurs uniquement limités par l'impossibilité de communiquer.<p>

Travail conjoint avec A. Massenet, L. Salvail, L. Stinchcombe et N. Yang.
</resume></p><p><b>Biographie: </b>Le Prof. Claude Crépeau est né à Montréal, en 1962. Il obtient une maitrise de l'Université de Montréal en 1986 et son doctorat d'informatique au MIT en 1990, en travaillant en cryptographie sous la direction des Professeurs Gilles Brassard (UdeM), et Silvio Micali (MIT). Il fit deux ans d'études post doctorales à l'Université d'Orsay, sous Miklós Sántha et fut un chercheur CNRS à l'École normale supérieure de 1992 à 1995 dans l'équipe de Jacques Stern. Il a été nommé professeur agrégé à l'Université de Montréal en 1995, et enfin à l'école d'informatique de l'Université McGill depuis 1998. Il fut membre du programme de l'Institut Canadien de Recherches Avancées sur l'informatique quantique de 2003 à 2013.<p>

Le Professeur Crépeau est connu pour son travail sur les preuves à divulgation nulle de connaissance, les protocoles de calculs multiparties classiques et quantiques, et la cryptographie quantique (dont l'authentification quantique).
En 1992, en collaboration avec Charles H. Bennett, Gilles Brassard, Richard Jozsa, Asher Peres, et William Wootters, le Professeur Crépeau inventa la téléportation quantique, un concept vérifié de façon expérimentale. Ce travail a depuis été cité plus de 13600 fois dans la littérature scientifique.</p><p><b>Lien</b>: <a href="files/ClaudeCrepeau2020.pdf">diapositives</a></p></div><div id="2822020"><h3>Vendredi 28 f&eacute;vrier 2020</h3><p style="text-align:left;"><b>Kernel-based few-shot regression and applications in drug discovery</b><br>Prudencio Tossou<br><i>Étudiant au doctorat, membre du GRAAL</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Due to the significant costs of data generation, many prediction tasks within drug discovery are by nature few-shot regression (FSR) problems, including accurate modeling of biological assays.  
The first part of the talk will provide some background for understanding the challenges faced by current models in drug discovery and the second part will present new algorithms that we have developed to face these challenges. 
More precisely, we have developed Adaptive deep kernel learning, that consist of learning deep networks in combination with kernel functions using differentiable kernel algorithms. 
The algorithm learns to find the appropriate kernel for each task during inference. It thus performs more effectively with complex task distributions, outperforming current state-of-the-art algorithms on both toy and novel, real-world benchmarks that we introduced.
</resume></p><p><b>Note: </b>La présentation sera donnée en français.</p></div><div id="1422020"><h3>Vendredi 14 f&eacute;vrier 2020</h3><p style="text-align:left;"><b>Systèmes de perception bayésienne pour la localisation de robots mobiles</b><br>Romuald Aufrère<br><i>Maître de conférences (Assistant Professor) à l'ISIMA (école d'ingénieurs en informatique)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Depuis quelques années, l'équipe PerSyst de l'Institut Pascal développe notamment une stratégie innovante de perception et de fusion mutli-sensorielle de type top-down qui prend en compte, en fonction des différents objectifs à atteindre, le contexte opérationnel mais également la disponibilité de l'information. Ces travaux sont particulièrement adaptés pour des applications de localisation de véhicules en particulier dans des cartes existantes imprécises et incomplètes. Cette approche fera l'objet de la première partie du séminaire. Dans un second temps, une approche de localisation de robots mobiles réalisée uniquement par de simples mesures de distance sera présentée. Cette approche exploite des informations fournies par des balises UWB. L'ensemble des travaux seront illustrées par des vidéos d'expérimentations sur des véhicules réels.
    </resume></p><p><b>Biographie: </b>Romulad Aufrère a reçu son doctorat en vision par ordinateur en 2001 En 2002, il a effectué, sous la responsabilité de Chuck Thorpe, un stage post-doctoral au Robotics Institute de Carnegie Mellon University (USA). Depuis 2003, il est maître de Conférences (assistant professor) à l'ISIMA (école d'ingénieurs en informatique) où il enseigne de l'automatique, robotique, traitement du signal, etc. Parallèlement il effectue ses recherches au sein du laboratoire Institut Pascal de l'Université Clermont Auvergne. Ces domaines d'activités sont liés à ceux du véhicule intelligent en particulier sur le développement de processus de perception multi-sensorielle pour la localisation. Il a obtenu son Habilitation à Diriger des Recherches (HDR) en juillet 2019.</p></div><div id="1332020"><h3>Vendredi 13 mars 2020</h3><p style="text-align:left;"><b>Les arbres de décision en tant que machines à partitionner</b><br>Jean-Samuel Leboeuf<br><i>Étudiant au doctorat, membre du GRAAL</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Les arbres de décision font partie des modèles les plus utilisés dans le domaine de l'apprentissage automatique en raison de leur flexibilité et de la possibilité d'interpréter leurs prédictions. Toutefois, bien que leur origine remonte à plus de 50 ans, les propriétés théoriques qui affectent leur garantie de généralisation, comme la dimension de Vapnik-Chervonenkis (VC), sont encore peu connues. Lors de cette présentation, je montrerai comment une approche basée sur le concept des partitions permet d'évaluer la dimension VC des souches de décision lorsque les attributs sont à valeur réelle. De plus, j'étendrai cette approche aux arbres avec une structure quelconque pour borner supérieurement et inférieurement leur dimension VC.</resume></p><p><b>Lien</b>: <a href="files/JeanSamuelLeboeuf2020.pdf">diapositives</a></p></div><div id="632020"><h3>Vendredi 6 mars 2020</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></p><p></p></div><div id="2912021"><h3>Vendredi 29 janvier 2021</h3><p style="text-align:left;"><b>Une métrique d'évaluation pour la génération textuelle "Data-to-Text" utilisant la recherche d'informations (DEIR)</b><br>Nicolas Garneau<br><i>Étudiant au doctorat (directeur: Luc Lamontagne)</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/QMSKRaw4PnPIpjQjdvslDxS2QuQb3TtUUVf8LDOMje_XdLmrygGKptGMJquhYCc._ipF1IwaqCmU6iGj?startTime=1611944439000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Dans ce séminaire, nous présentons tout d'abord un nouvel ensemble de données Data-to-Text francophone pour la génération automatique de contenu textuel dans le domaine juridique, Plum2Text.
Ce nouvel ensemble de données a des entrées et sorties qui sont uniques en soi ; du côté de la table (data), les valeurs contiennent de longs énoncés textuels, et du côté du texte (text), on retrouve souvent une paraphrase des valeurs du tableau.
Nous décrivons la manière dont nous avons conçu les annotations tables-textes en introduisant un outil d'annotation et une méthodologie spécifique à la tâche de génération de langage naturel "Data-to-Text".<p>

Nous présentons ensuite une nouvelle métrique d'évaluation utilisant la recherche d'informations (DEIR), qui est mieux adaptée que les métriques introduites précédemment lorsque les tables contiennent du texte pouvant être associé à de multiples références. Cette métrique constitue notre principale contribution.<p>

Nous menons également des expériences en utilisant un modèle de génération de référence sur notre ensemble de données et évaluons sa performance avec notre métrique, qui démontre la nécessité d'une architecture spécifique étant donné la nature de l'ensemble de données que nous introduisons, surtout lorsque les ressources sont limitées.</resume></p><p><b>Lien</b>: <a href="https://www.aclweb.org/anthology/2020.inlg-1.3.pdf">article</a></p></div><div id="2622021"><h3>Vendredi 26 f&eacute;vrier 2021</h3><p style="text-align:left;"><b>Multi-task Learning by Leveraging the Semantic Information</b><br>Fan Zhou<br><i>Étudiant au doctorat, membre du Damas</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/xR2thwOkOvx0KJ7Ov85K8md3v7mwUCpYKfHGLpLG3fPpGNtGqokmpTxFmEdvSMzf.rCT8Y6h0KVbJe3_T?startTime=1614364126000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Multi-task learning aims to solve the related tasks simultaneously by exploiting shared knowledge to improve individual task performance. One crucial objective of multi-task learning is to align distributions across tasks so that the information between them can be transferred and shared. However, existing approaches only focused on matching the marginal feature distribution while ignoring the semantic information, which may hinder the learning performance. To address this issue, we propose to leverage the label information in multi-task learning by exploring the semantic conditional relations among tasks. We first theoretically analyze the generalization bound of multi-task learning based on the notion of Jensen-Shannon divergence, which provides new insights into the value of label information in multi-task learning. Our analysis also leads to a concrete algorithm that jointly matches the semantic distribution and controls label distribution divergence. To confirm the effectiveness of the proposed method, we first compare the algorithm with several baselines on some benchmarks and then test the algorithms under label space shift conditions. Empirical results demonstrate that the proposed method could outperform most baselines and achieve state-of-the-art performance, particularly showing the benefits under the label shift conditions.</resume></p><p><b>Note: </b>Cette présentation sera donnée en anglais</p></div><div id="1922021"><h3>Vendredi 19 f&eacute;vrier 2021</h3><p style="text-align:left;"><b>Differentiable Causal Discovery with Observational and Interventional Data</b><br>Alexandre Drouin<br><i>Research Scientist, Element AI</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://youtu.be/s323WswRMbE">Enregistrement</a></p><p><b>Résumé: </b><resume>Knowledge of the causal structure that underlies a data generating process is essential to answering questions of causal nature. Such questions are abundant in fields that involve decision making such as econometrics, epidemiology, and social sciences. When causal knowledge is unavailable, one can resort to causal discovery algorithms, which attempt to recover causal relationships from data. This talk will present a new algorithm for this task, that combines continuous-constrained optimization with the flexible density estimation capabilities of normalizing flows. In contrast with previous work in this direction, our method combines observational and interventional data to improve identification of the causal graph. We will present empirical results, along with a theoretical justification of our algorithm.</resume></p><p><b>Biographie: </b>Alexandre Drouin is a Research Scientist at Element AI in Montréal, Canada and an Adjunct Professor of computer science at Laval University. He received a PhD in machine learning from Laval University in 2019 for his work on antibiotic resistance prediction in bacterial genomes. His interests include causal inference, deep learning, and bioinformatics.</p><p><b>Note: </b>Cette présentation sera donnée en français<br>
    ID de réunion : 859 7996 4238<br>
    Code secret : 027622</p><p><b>Lien</b>: <a href="https://papers.nips.cc/paper/2020/file/f8b7aa3a0d349d9562b424160ad18612-Supplemental.pdf">article</a></p></div><div id="1222021"><h3>Vendredi 12 f&eacute;vrier 2021</h3><p style="text-align:left;"><b>Reconnaissance d'objets dans leur contexte à partir de peu d'exemples d'entraînement</b><br>Mathieu Pagé-Fortin<br><i>Étudiant au doctorat, membre du Damas</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/jND4cz6qAeRtIYDMwsoLD0NrnAzXYiXCGVf8Hp6UR-YvNuf4oYI6mKyOpgGStj0k.ksvwcivvNZ4QdvSL?startTime=1613154421000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Le Few-shot Learning vise à reconnaître de nouvelles classes d'objets à partir de peu d'exemples d'entraînement.  C'est en soi un problème fort difficile et les modèles existants en vue de le résoudre ont principalement été développés pour des ensembles de données relativement « simples » où chaque image ne contient qu'un seul objet. Dans mes travaux, je propose d'étudier le Few-shot Learning dans le cadre d'images de scènes complexes et, par le fait même, d'exploiter la sémantique du contexte pour apprendre et reconnaître les objets plus facilement. Je montrerai dans cette présentation comment j'ai modélisé la sémantique du contexte, et je décrirai par la suite deux modules que j'ai développés pour intégrer la contextualité au Few-shot Learning.</resume></p></div><div id="522021"><h3>Vendredi 5 f&eacute;vrier 2021</h3><p style="text-align:left;"><b>Planification et ordonnancement de la production chez Biscuits Leclerc</b><br>Nicolas Blais<br><i>Étudiant à la maîtrise (directeurs: Claude-Guy Quimper, Nadia Lehoux, Jonathan Gaudreault)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/-cDPxDeF05XFm8WgtI1clUsEtIrmte47ivs-D4f0Ke6EK00ap2YLw5C5u_y-Vw.qavvQ-9-QvdYOhiO?startTime=1612549444000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Faire l'ordonnancement d'une ligne de production est une tâche ardue. C'est particulièrement le cas pour les lignes de production de Biscuits Leclerc. En effet, l'industrie agroalimentaire vient avec son lot de contraintes opérationnelles. Une des principales contraintes présentes chez Biscuits Leclerc est le temps de setup entre les différents produits, qui dépend grandement de la séquence de production. Il existe ainsi des séquences où le temps global passé à faire du setup est plus petit que d'autres. Le projet de recherche consiste donc à créer un outil d'aide à la décision permettant de trouver des horaires de production qui minimisent le temps de setup tout en tenant compte d'une panoplie de contraintes présentes dans l'industrie agroalimentaire.
La présentation va d'abord porter sur les différents concepts liés à la recherche opérationnelle et la programmation par contraintes. Puis, la problématique et le modèle d'optimisation seront mis de l'avant. Pour finir, des algorithmes de recherche locale à voisinage large utilisés pour améliorer la qualité des solutions seront présentés.</resume></p><p><b>Lien</b>: <a href="http://www2.ift.ulaval.ca/~quimper/publications/ILS2020.pdf">article</a></p></div><div id="1232021"><h3>Vendredi 12 mars 2021</h3><p style="text-align:left;"><b>Relaxation lagrangienne en programmation par contraintes appliquée au problème du commis voyageur</b><br>Raphaël Boudreault<br><i>Étudiant à la maîtrise</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/4A6Rdv8scwQmV4zB8fKtkK6HCNpxkXp_JvjwJ-yyLf9iC9kK4xOQc0FexsODAiIG.eDo362qsyrKd0s54?startTime=1615573017000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>La relaxation lagrangienne est une technique d'optimisation efficace et largement utilisée en optimisation. Dans le contexte de la programmation par contraintes, il est possible d'en faire usage en la combinant à une technique de filtrage spécifique basée sur la notion de coût. En particulier, les algorithmes de filtrage de l'état de l'art pour la contrainte WeightedCircuit, qui encode le problème du commis voyageur (traveling salesman problem, TSP), sont basés sur cette approche. Dans ce séminaire, nous introduirons l'ensemble des notions permettant de comprendre chacun des mots des précédentes phrases de ce résumé. Puis, nous proposerons une nouvelle approche modifiant localement les multiplicateurs de Lagrange dans le but d'augmenter le nombre de valeurs filtrées. En appliquant cette dernière au filtrage de la contrainte WeightedCircuit sur des instances du TSP, un gain significatif sur le temps de résolution et la taille de l'espace de recherche est obtenu par rapport à l'état de l'art.</resume></p><p><b>Lien</b>: <a href="files/CP-LR-TSP_beamer.pdf">diapositives</a></p></div><div id="2632021"><h3>Vendredi 26 mars 2021</h3><p style="text-align:left;"><b>La stabilité algorithmique de la descente du gradient stochastique</b><br>Alexandre Lemire Paquin<br><i>Étudiant au doctorat, membre du Damas</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/FtGDNHg7vslt5ejn210Xc6O-Wk8FVH_yaWC3MibQez3-c7ZxNoBE6bjQkS07uTqM.vxatCfJw3u-BrTbP?startTime=1616779032000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Au cours des dernières années, l'apprentissage profond a réussi à établir des performances de pointe dans une grande variété de tâches dans des domaines comme la vision par ordinateur, le traitement du langage naturel et la bioinformatique (LeCun et al., 2015). Comprendre quand et comment ces réseaux généralisent mieux est important pour continuer à améliorer leurs performances. De nombreux travaux à partir principalement de (Neyshabur et al., 2015), (Zhang et al., 2017) et (Keskar et al., 2017) suggèrent une interaction riche entre la régularisation et le processus d'optimisation qui permet d'apprendre les poids du réseau. L'idée est qu' une forme de biais inductif peut être réalisé implicitement par l' algorithme d'optimisation. L'algorithme le plus populaire pour entraîner les réseaux de neurones est la descente de gradient stochastique (SGD). Il est donc d'un grand intérêt d'étudier les propriétés de généralisation de cet algorithme. Une approche particulièrement bien adaptée pour étudier directement les algorithmes d'apprentissage est la stabilité algorithmique (Bousquet and Elisseeff, 2002), (Elisseeff et al., 2005). Il est argumenté dans (Nagarajan and Kolter, 2019) que les bornes de généralisation basées sur la convergence uniforme pourrait être condamnées à être essentiellement vides pour les réseaux profonds. Les bornes basées sur la stabilité offrent une alternative possible en essayant de borner directement l'erreur de généralisation de la sortie de l'algorithme. Dans cette présentation, nous allons introduire le(s) concept(s) de stabilité algorithmique ainsi que son application à l'étude de la descente du gradient stochastique. Nous présenterons aussi nos travaux en cours sur le sujet.</resume></p></div><div id="1932021"><h3>Vendredi 19 mars 2021</h3><p style="text-align:left;"><b>Analyse de l'erreur en vérification probabiliste</b><br>Gildas Syla Déo Kouko<br><i>Étudiant au doctorat</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/qsNe1qo1qyqYBr6u1pMFCGzK3psC0suIEaPyL4-AXANQFWo45qwV0Zqdlc50LeeP.S8YCwtPEJRMrZNGi?startTime=1616174117000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>La vérification de systèmes est un sujet de recherche important et différentes techniques permettent de vérifier formellement des systèmes critiques dont il faut garantir la correction. Nous aborderons l'une des techniques formelles les plus utilisées et les plus efficaces, l'évaluation de modèle (model checking en anglais). Pour vérifier un système par évaluation de modèle, on abstrait d'abord son comportement sous la forme d'un système de transitions appelé modèle. Ensuite, on formule une propriété désirée du système dans une logique temporelle. Enfin, on utilise un outil logiciel appelé vérificateur pour vérifier automatiquement si le modèle satisfait la propriété. Contrairement aux vérifications par tests, cette vérification est hors de tout doute.<p>

Nous alons montrer une nouvelle technique pour vérifier des propriétés d'atteignabilité dans des modèles probabilistes appelés processus de Markov étiquetés (en anglais, LMP pour Labelled Markov processes) et qui ont possiblement un ensemble d'états non dénombrable. À partir des propriétés particulières à vérifier, nous construisons un modèle fini probabiliste qui lui peut être vérifié avec des outils existants.<p>

Le modèle fini est construit de telle sorte que nous inférons que le LMP satisfait la propriété d'atteignabilité, si et seulement si le système fini la satisfait. Ainsi, théoriquement, notre approche donne un résultat ultime exact et nous l'avons prouvé. À l'implémentation, nous utilisons une méthode d'intégration numérique pour déterminer les probabilités de transition dans le modèle fini. Malgré les imprécisions numériques qui peuvent nuire au résultat d'une vérification, nous avons prouvé que notre approche a du sens en quantifiant les erreurs. Notre méthode repousse les limites de l'évaluation de modèle, notamment l'explosion combinatoire de l'espace d'états ou de chemins dans la vérification de systèmes probabilistes infinis.
</resume></p></div><div id="2342021"><h3>Vendredi 23 avril 2021</h3><p style="text-align:left;"><b>Programmation dynamique pour l'optimisation d'un itinéraire en véhicule électrique</b><br>Anthony Deschênes<br><i>Étudiant au doctorat, membre du Consortium 4.0</i></p><p><b>Heure</b>: 13h30<br><b>Visioconférence</b>: <a href="https://ulaval.zoom.us/j/83443481229?pwd=NjM4aXBHOFNUTllTNXlxUzFmMDZuQT09">Zoom</a></p><p><b>Résumé: </b><resume>Avec la montée en popularité des véhicules électriques, il devient impératif de développer des modèles efficaces et modulables pour en réduire les inconvénients. Un de ses inconvénients est la peur de tomber en panne. Cette peur est particulièrement présente lors de la planification d'un long itinéraire en véhicule électrique. Dans cette présentation, je montrerai un algorithme basé sur la programmation dynamique qui permet de rapidement optimiser un itinéraire en véhicule électrique permettant ainsi de contrôler cette peur. Les décisions prises par cet algorithme concernent les endroits où arrêter charger, le temps de charge nécessaire le cas échéant et la vitesse à laquelle rouler sur la route. Je comparerai cette approche avec l'état de l'art actuel pour ce problème et comparerai la qualité de la solution ainsi que le temps nécessaire pour calculer cette solution.</resume></p></div><div id="1642021"><h3>Vendredi 16 avril 2021</h3><p style="text-align:left;"><b>Apprentissage profond de similarité pour la reconnaissance visuelle de lieux</b><br>Amar Ali-Bey<br><i>Étudiant au doctorat, membre du Damas</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/l3pumm9WGFCNiAulk4IJWA516Tn7jn2Mx_FALNyHpXRWIMsXK0u0wb2vAcHp4wn1.ZlK8Bq8SXgaMSJ2d?startTime=1618593484000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>La reconnaissance visuelle de lieux est la capacité d'un système à déterminer la l'emplacement d'un lieu présent dans une photographie. Cette tâche est difficile à réaliser au vu des variations environnementales qui se manifestent sous forme de changements d'apparence. Les techniques traditionnelles de la reconnaissance visuelle de lieux se basent généralement sur des descripteurs issus de l'ingénierie de caractéristiques.<p>

     Récemment, les réseaux de neurones convolutifs (CNN) ont montré de très bonnes performances au niveau de plusieurs tâches de vision par ordinateur. À cet égard, plusieurs architectures CNN ont été proposées pour aborder la reconnaissance visuelle de lieux à grande échelle, où l'apprentissage est basé sur une fonction de perte à triplets permettant l'entraînement sur des images téléchargées à partir de la plateforme Google Street View.<p>

      Dans ce séminaire, nous présentons le problème de la reconnaissance visuelle de lieux (RVL) sous une nouvelle perspective. À cette fin, nous avons collecté une nouvelle base de données (GSV-Cities) qui établit un lien entre la reconnaissance visuelle de lieux et l'apprentissage profond de similarités (Deep Metric Learning). Une telle base de données pourrait être utilisée dans le but d'étudier et de développer de nouvelles fonctions de perte spécifiques au problème de la RVL. Enfin, nous montrons, à travers diverses expériences, comment la reconnaissance visuelle de lieux peut grandement bénéficier des techniques d'apprentissage profond de similarités.</resume></p></div><div id="942021"><h3>Vendredi 9 avril 2021</h3><p style="text-align:left;"><b>La contrainte MinCumulative</b><br>Yanick Ouellet<br><i>Étudiant au doctorat</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/-_WhbeembHdfFVwZZxLrtb8rvz19MC1byCjf2jjmezg-k4AI3ApkjTWi1D9idp-K.7tvQDb5uUZUOTyFs?startTime=1617987564000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>La programmation par contrainte est l'une des techniques qu'il est possible d'utiliser pour s'attaquer aux problèmes NP-Difficiles. Elle est particulièrement efficace pour résoudre des problèmes d'ordonnancement. La contrainte Cumulative est l'une des clés du succès de la programmation par contrainte en ordonnancement. Cette contrainte limite la quantité d'une ressource qui peut être utilisée par des tâches à tout moment. On pourrait, par exemple, avoir 100 personnes (les tâches) qui doivent aller faire l'épicerie, alors que les règles gouvernementales limitent la capacité d'accueil à 30 personnes à la fois (la ressource). Jusqu'à maintenant, la programmation par contraintes ne disposait pas de contraintes globales pour le problème inverse, c'est-à-dire avoir une quantité minimale de la ressource utilisée. C'est cependant un scénario fréquent dans la réalité. On peut vouloir avoir au moins 5 employés en tout temps dans l'épicerie pour en assurer le bon fonctionnement. Nous proposons la MinCumulative, une contrainte globale pour modéliser et résoudre ce problème.  Lors du séminaire, nous nous assurerons d'introduire les notations préalables en programmation par contraintes et en ordonnancement. Nous présenterons ensuite la MinCumulative et démontrerons, à l'aide d'une réduction, que décider si la contrainte admet une solution est NP-Complet. Nous introduirons ensuite les algorithmes de vérification et de filtrage qui permettent à la MinCumulative d'être efficace. Nous terminerons par la présentation de résultats expérimentaux démontrant l'efficacité pratique de la nouvelle contrainte.</resume></p><p><b>Lien</b>: <a href="files/YanickOuellet2021.pdf">diapositives</a></p></div><div id="242021"><h3>Vendredi 2 avril 2021</h3><p style="text-align:left;"><b>Vendredi saint (pas de séminaire)</b><br></p><p></p></div><div id="22102021"><h3>Vendredi 22 octobre 2021</h3><p style="text-align:left;"><b>Interrelations entre le microbiote, les bactériophages et la nutrition: élaboration d'approches en intelligence artificielle pour déterminer leurs impacts sur la santé cardiométabolique</b><br><a href="https://www.ift.ulaval.ca/departement-et-professeurs/professeurs-et-personnel/professeurs-reguliers/fiche/show/rousseau-elsa/">Elsa Rousseau</a><br><i>Professeure au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 10h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/MEQauvlhGpd6GTLbiM_kZQJ89_gGswYJt72SMtPlcaw6eci3Svq3jLziYDByXgXO.CrjBcD8L1XhJgLNV?startTime=1634912036000">Enregistrement</a> (mot de passe: Séminaire-2021)</p><p><b>Résumé: </b><resume>Par l'intermédiaire de mon programme de recherche, je désire investiguer le lien entre le microbiote intestinal, les phages, l'alimentation et les facteurs de risque métabolique de maladies chroniques comme, par exemple, le cholestérol, la pression sanguine et le glucose sanguin. Pour cela, je vais analyser le microbiote intestinal et les phages de plusieurs centaines d'hommes et de femmes par des analyses sophistiquées de laboratoire qui mesurent la présence de milliards de bactéries dans l'intestin et de nombreuses molécules associées à ces bactéries dans l'intestin. Des outils basés sur l'intelligence artificielle (IA) permettront dans un premier temps d'identifier les couples de bactéries et phages les plus importants pour la santé métabolique. Dans un deuxième temps, ces outils d'IA me permettront de mieux comprendre comment le microbiote, les phages, la nutrition et la santé cardiométabolique sont interreliés. Finalement, je m'assurerai que ces outils d'IA sont interprétables, c'est à dire qu'ils sont utiles et applicables dans un contexte de nutrition de précision. La nutrition de précision permet de personnaliser les habitudes alimentaires d'une personne en fonction de ses caractéristiques dans le but de maximiser les effets sur la santé. Ces recherches ont donc le potentiel de mener au développement d'approches de nutrition de précision pour la prévention et/ou le traitement de plusieurs maladies chroniques liées à l'alimentation dans notre société.<p>
    Lors de ma présentation, je commencerai par un résumé de mes travaux précedent mon entrée en poste: lors de mon doctorat, premier postdoctorat chez IBM et second postdoctorat supervisé par Jacques Corbeil et François Laviolette. Je présenterai ensuite mon programme de recherche en tant que nouvelle professeure au département.</resume></p><p><b>Biographie: </b>Elsa a récemment commencé sa fonction de professeure au Département d'informatique et de génie logiciel de la Faculté des sciences et de génie - Université Laval, et au sein du Centre NUTRISS (en juillet 2021). Son projet de recherche porte sur l'étude des interrelations entre le microbiote, les bactériophages et la nutrition, via l'élaboration d'approches en intelligence artificielle pour déterminer leurs impacts sur la santé cardiométabolique.<p>
Elsa a obtenu son diplôme d'ingénieure en bio-informatique et modélisation à l'INSA Lyon - Institut National des Sciences Appliquées de Lyon. Elle a choisi de s'orienter vers l'informatique et les mathématiques appliquées à la santé pour ses deux stages d'étudiante-ingénieure. Elle a effectué un doctorat entre l'Inria (Institut national de recherche en sciences et technologies du numérique) et l'INRAE (Institut national de la recherche agronomique), en modélisation de l'épidémiologie et de l'évolution des virus de plantes, intitulé « Étude de la dérive génétique et de la sélection sur la durabilité de la résistance des plantes aux virus ». Elle a ensuite réalisé un postdoctorat chez IBM, dans leur réputé Centre de recherche Almaden à San Jose (CA), en collaboration avec l'UCSF (Université de Californie San Francisco). Le laboratoire d'innovation IBM Almaden est entre autres reconnu internationalement pour ses recherches pionnières en intelligence artificielle, en santé et en sciences de la vie. Ses travaux de modélisation mathématique ont porté sur l'utilisation de particules virales défectueuses interférentes comme nouveau type de traitement antiviral.<p>
Elsa Rousseau était membre étudiante au CRDM et est maintenant membre régulière.<p>
    Elle a effectué son deuxième postdoctorat dans le laboratoire de Jacques Corbeil au Centre de recherche du CHU de Québec-Université Laval, en codirection avec Francois Laviolette, au cours duquel elle a décidé d'élargir ses compétences en bio-informatique en y intégrant l'intelligence artificielle. Elle a travaillé sur divers projets de génomique et métagénomique appliqués à la santé.</p></div><div id="2812022"><h3>Vendredi 28 janvier 2022</h3><p style="text-align:left;"><b>Les vulnérabilités du processus de compilation</b><br><a href="http://www.uqac.ca/portfolio/raphaelkhoury/">Raphaël Khoury</a><br><i>Professeur au département d'informatique et de mathématique, UQAC</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/JsWB63sr0ooqPr_eeniG7qg08jq5q_ktH1u6is5u6_LzZUYWxTS_zb3c4fy1EF_K.cHUG0VD9K-8am5t3?startTime=1643394009000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>Les développeurs considèrent généralement que le code binaire généré par les compilateurs est conforme au code source qu'ils ont écrit. Or, à leur insu, les optimisations effectuées par les compilateurs peuvent souvent modifier le comportement de leurs programmes. Dans certains cas, ces optimisations peuvent même introduire des vulnérabilités dans le code. D'ailleurs, plusieurs vulnérabilités ayant leurs origines dans les optimisations effectuées par les compilateurs ont été recensées dans des programmes aussi connus que Linux et Gzip. Cette présentation, explore les causes des vulnérabilités du processus de compilation, et offre au programmeur des conseils qu'il peut mettre en pratique afin de produire du code plus robuste.</resume></p><p><b>Biographie: </b>Raphaël Khoury, Ph.D., P. Eng, a obtenu son doctorat de l'Université Laval en 2011 et sa licence d'ingénieur professionnel de l'Ontario en 2013.  Il a subséquemment été chercheur postdoctoral au Centre de Recherche et Développement pour la Défense Canada, (RDDC-RDDC) à Valcartier. Depuis 2014, il est professeur à l'Université du Québec à Chicoutimi. Il a publié plus de 50 articles scientifiques et il est bénéficiaire de plusieurs subventions de recherche du FQRNT, du CRSNG et de l'industrie.</p><p><b>Lien</b>: <a href="files/RaphaelKhoury2022.pdf">diapositives</a></p></div><div id="2112022"><h3>Vendredi 21 janvier 2022</h3><p style="text-align:left;"><b>Few-Shot Learning: Are We Making Progress?</b><br><a href="https://www.etsmtl.ca/recherche/professeurs-chercheurs/ibenayed/">Ismail Ben Ayed</a><br><i>Chaire de recherche ÉTS sur l'intelligence artificielle en imagerie médicale</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/NdRQkxVECgmoucb7qJPmcOiGRCck77qrS2MuSUl5Hxfy9Oyje_1rat0li7zONDQU.oW98ZapQugHXXGPg?startTime=1642789202000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>Despite their unprecedented performances when trained on large-scale labeled data, deep learning and artificial intelligence models are seriously challenged when dealing with novel (unseen) classes and limited labeled instances. In contrast, humans can learn new tasks easily from a handful of examples, by leveraging prior experience and context. Few-shot learning attempts to bridge this gap, and has recently triggered substantial research efforts. This talk discusses recent developments in the general, wide-interest subject of learning with limited supervision. Specifically, I will discuss state-of-the-art models, which leverage unlabeled data with structural priors, and connect them under a unifying information-theoretic perspective. Furthermore, I will highlight recent results, which point to important limitations of the standard few-shot benchmarks, and question the progress made by an abundant recent few-shot literature, mostly based on complex meta-learning strategies. Classical and simple loss functions, such as the Shannon entropy or Laplacian regularization, well-established in the clustering literature, achieve outstanding performances.
    </resume></p><p><b>Biographie: </b>Ismail Ben Ayed is a Full Professor at the ETS Montreal, where he holds a research Chair on Artificial Intelligence in Medical Imaging. He is also affiliated with the University of Montreal Hospital Research Centre (CRCHUM). His interests are in computer vision, optimization, machine learning and medical image analysis algorithms. Ismail authored over 100 fully peer-reviewed papers, mostly published in the top venues of those areas, along with 2 books and 7 US patents.  In the recent years, he gave over 30 invited talks, including 5 tutorials at flagship conferences (MICCAI'14, ISBI'16, MICCAI'19, MICCAI'20 and MICCAI'21). His research has been covered in several visible media outlets, such as Radio Canada (CBC), Quebec Science Magazine and Canal du Savoir. His research team received several recent distinctions, such as the MIDL'21 best paper award and several top-ranking positions in internationally visible contests. Ismail served as Program Committee for MICCAI'15, MICCAI'17 and MICCAI'19, and as Program Chair for MIDL'20. Also, he serves regularly as reviewer for the main scientific journals of his field, and was selected several times among the top reviewers of prestigious conferences (such as CVPR'21, NeurIPS'20 and CVPR'15).
</p><p><b>Note: </b>La présentation sera donnée en français.</p><p><b>Lien</b>: <a href="https://tinyurl.com/2p8snbfs">diapositives</a></p></div><div id="2522022"><h3>Vendredi 25 f&eacute;vrier 2022</h3><p style="text-align:left;"><b>Prédire le temps de recharge rapide d'un véhicule électrique avec des réseaux de neurones</b><br>Anthony Deschênes<br><i>Étudiant au doctorat, Consortium de Recherche en Ingénierie des Systèmes Industriels 4.0</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/gyPIeK3mkNtNyEeqEmYNDitgVcu2T_-lwgl3Yj6PMpDi-sPrve1Lb1DmRUMRbtYZ.kAeQGV1H4SHUpdR-?startTime=1645813386000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>Prédire le temps nécessaire pour recharger un véhicule électrique de X% à Y% est une tâche difficile en raison de la non-linéarité du processus de charge. De plus, de nombreux facteurs externes l'influencent comme la température ainsi que la dégradation de la batterie. En utilisant environ 28 000 séances de recharge rapide réelle faites sur 16 différents types de véhicules, nous entraînerons et comparerons différents modèles d'apprentissage. Parmi ces modèles se trouvent les régressions linéaires, les régressions polynomiales de degré 2, les forêts aléatoires (degré 1 et 2) ainsi que les réseaux de neurones. Les différents modèles prennent en considération la température, la capacité de la batterie, le nombre de recharges faites dans la même journée, etc. Nous utilisons également des méthodes d'augmentation de données (SMOTE) pour balancer notre jeu de données ainsi que l'optimisation bayésienne pour optimiser les hyperparamètres. La structure des réseaux de neurones est également optimisée grâce à l'optimisation bayésienne. Chaque modèle est ensuite entraîné et comparé statistiquement pour déterminer lequel performe le mieux.</resume></p></div><div id="1122022"><h3>Vendredi 11 f&eacute;vrier 2022</h3><p style="text-align:left;"><b>Infographie et animation physique: solidification de matériaux viscoélastiques</b><br><a href="http://www.alexandremercieraubin.com/">Alexandre Mercier-Aubin</a><br><i>Étudiant au doctorat, McGill</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/tYPFRGxWgn-7JKurkS4btD0kelqnUacnpXiwpaimZsESC1g-yVM_OOHEOAjPwHaO.0wocV1Y83vWU-s8c?startTime=1644603577000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>En animation, il est primordial que les éléments réagissent aux contraintes physiques de leur environnement. Comment peut-on animer graphiquement de la matière élastique qui se déforme? Comment se déforme une matière élastique par rapport à de la matière rigide? La simulation physique apporte des réponses à ces questions et permet de générer des animations fidèles à la réalité. S'il est une chose de procéder à une simulation physique, c'en est une autre de le faire avec des algorithmes efficaces. En effet, ceux-ci doivent être suffisamment rapides pour générer, à la volée, des animations dans des temps de calcul raisonnables. Je présenterai les fondements mathématiques et algorithmiques qui nous ont permis de simuler l'élasticité des formes avec de courts temps de calcul.</resume></p></div><div id="422022"><h3>Vendredi 4 f&eacute;vrier 2022</h3><p style="text-align:left;"><b>Conversion des nombres décimaux en nombres binaires: atteindre le gigaoctet par seconde</b><br><a href="https://www.teluq.ca/siteweb/univ/dlemire.html">Daniel Lemire</a><br><i>Professeur d'informatique à l'Université du Québec (TÉLUQ)</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/NvWQte9OCzxGlxQgsrX5Fv-Y3dzgBM-bN4hrq9pJ0VeBNWiIo4S3SAhVFfkFwB8.ddoTd6tJLFV-pKpg?startTime=1643998858000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>Avec des disques et des réseaux fournissant des gigaoctets par seconde, la lecture des nombres décimaux au sein des fichiers XML, JSON et CSV devient un goulot d'étranglement. Nous considérons le problème de la conversion des nombres décimaux à la valeur binaire flottante la plus proche. Nous présentons une mise en oeuvre C++ qui est souvent 4 fois plus rapide que la bibliothèque C standard sur les systèmes 64 bits modernes (Intel, AMD, ARM et POWER9). Notre travail est disponible en tant que logiciel open source utilisé par des systèmes majeurs tels que Rust, Go, LLVM libc, GNU stdlib++.</resume></p><p><b>Biographie: </b>Daniel est professeur d'informatique à l'Université du Québec (TÉLUQ), et blogueur de longue date. Il est @lemire sur Twitter et il blogue à https://lemire.me/. Ses 75 articles et communications de recherche ont été cités environ 4000 fois. Il figure parmi les 500 utilisateurs de GitHub les plus populaires : https://github.com/lemire. Il a co-présidé le comité informatique au CRSNG de 2018 à 2020. Les techniques d'indexation qu'il a développées sont utilisées par plusieurs systèmes importants tels que GitHub, Apache Spark, etc.</p><p><b>Lien</b>: <a href="files/daniellemire.pdf">diapositives</a></p></div><div id="1822022"><h3>Vendredi 18 f&eacute;vrier 2022</h3><p style="text-align:left;"><b>Understanding meta-learning from information-theoretic perspective</b><br> Qi Chen<br><i>Étudiante au doctorat, membre du Graal</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/HxVGV860X1QCm_8MCHmjpl1JiqjMlhmKqXq0CdrfHdoG1RfviKWl6kOeMkSf1IQ4.cphDMb2HsDcLqUUV?startTime=1645208619000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>We derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms. Concretely, our analysis proposes a generic understanding in both the conventional learning-to-learn framework [Amit and Meir 2018] and the modern model-agnostic meta-learning (MAML) algorithms [Finn et al. 2017]. Moreover, we provide a data-dependent generalization bound for the stochastic variant of MAML, which is non-vacuous for deep few-shot learning. As compared to previous bounds that depend on the square norms of gradients, empirical validations on both simulated data and a well-known few-shot benchmark show that our bound is orders of magnitude tighter in most conditions.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="https://proceedings.neurips.cc/paper/2021/file/d9d347f57ae11f34235b4555710547d8-Paper.pdf">article</a></p></div><div id="2532022"><h3>Vendredi 25 mars 2022</h3><p style="text-align:left;"><b>Allocation dynamique des opérateurs dans un contexte humain-machine d'usinage 4.0</b><br>Maude Beauchemin<br><i>Étudiante à la maîtrise, membre du Consortium 4.0</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/LcgqFKn8ZmmeVISlJrcFuUc2rVTf3nrh4WbCDM1RDAC_WZJaV35OWepAE2CAwpXH.gDNLIZrxMkyU9LUv?startTime=1648228772000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>La transformation numérique et le mouvement « industrie 4.0 » reposent sur des concepts tels que l'intégration et l'interconnexion des systèmes utilisant des données en temps réel. Dans le secteur manufacturier, un nouveau paradigme d'allocation dynamique des ressources humaines devient alors possible. Plutôt qu'une affectation statique des opérateurs aux machines, nous proposons d'affecter directement les opérateurs aux différentes tâches qui nécessitent encore une intervention humaine dans une usine majoritairement automatisée. Nous montrons les avantages de ce nouveau paradigme avec des expériences réalisées à l'aide d'un modèle de simulation à événements discrets. Un modèle d'optimisation qui utilise des données industrielles en temps réel et produit une allocation optimale des tâches est également développé.</resume></p></div><div id="1832022"><h3>Vendredi 18 mars 2022</h3><p style="text-align:left;"><b>Vérités terrains précises prisent avec des stations totales</b><br>Maxime Vaidis<br><i>Étudiant au doctorat, membre du Norlab</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/1po5iVx2M0hogaJ3HLQuanciqnTc1902xkMqwMj6wSHCTNoGpFsDEglDPHoI4aaQ.VT0ao4U35DDLB6Nw?startTime=1647624085000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>En robotique, les vérités terrains de positionnement constituent la base du développement des algorithmes de cartographie et de localisation à travers des jeux de données importants. Dans les environnements extérieurs et sur de longues distances, les stations totales sont les instruments de mesure les plus précis à cet effet. La plupart des systèmes basés sur les stations totales dans la littérature sont limités à trois degrés de liberté, en raison de l'utilisation d'une approche de suivi d'un prisme unique. Nous présentons à travers nos travaux préliminaires la mesure d'une pose complète d'un véhicule, portant le système de référence à six degrés de liberté. Trois stations au total sont utilisées pour suivre en temps réel trois prismes attachés à une plateforme cible.
Nous décrivons la structure du système de référencement et le protocole pour acquérir la vérité du sol avec ce système. Nous avons évalué sa précision dans divers environnements extérieurs, allant du ciel ouvert aux sentiers forestiers, puis comparé ce système à une autre source populaire de position de référence par GPS, la solution de positionnement Real Time Kinematics (RTK).
Les résultats montrent que notre approche est la plus précise, atteignant une erreur de position moyenne de 10 mm et 0,6 degré.
    Cette différence de performance a été particulièrement marquée dans les environnements où les signaux du Système mondial de navigation par satellite (GNSS) peuvent être plus faibles en raison de la végétation excessive.</resume></p><p><b>Lien</b>: <a href="https://arxiv.org/abs/2104.14396">article</a></p></div><div id="432022"><h3>Vendredi 4 mars 2022</h3><p style="text-align:left;"><b>Minimal Correction Sets and Cores in Core-Guided Algorithms: Analysis and Enumeration</b><br><a href="https://research.vmware.com/researchers/nina-narodytska">Nina Narodytska</a><br><i>Senior researcher at VMware Research</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/PTA1xa2ELnthL-yLvEyuWzKrFwtmtvl24LXwVvQJg_CWjWWZDnZWl7wLb_57uP0.4vQIUSWPPRp99424?startTime=1646418032000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>A set of constraints is unsatisfiable if there is no solution that satisfies these constraints. To analyze unsatisfiable problems, the user needs to understand where inconsistencies come from and how they can be repaired. Minimal unsatisfiable cores and correction sets are important subsets of constraints that enable such analysis. In the first part of the talk, we analyze core-guided MaxSAT algorithms and discuss how  the cores found by these algorithms are related to the cores of the original unsatisfiable formula. We will also discuss properties of these cores. Based on these results, in the second part,  we discuss  a new algorithm for extracting minimal unsatisfiable cores and correction sets.</resume></p><p><b>Biographie: </b>Nina Narodytska is a senior researcher at VMware Research. Prior to VMware, she was a researcher at Samsung Research America. She completed postdoctoral studies in the Carnegie Mellon University School of Computer Science and the University of Toronto. She received her PhD from the University of New South Wales. Nina works on developing efficient search algorithms for decision and optimization problems. She was named one of "AI's 10 to Watch" researchers in the field of AI in 2013. She has presented invited talks and tutorials at  FMCAD'18, CP'19, AAAI'20 and IJCAI'20.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Liens</b>: <a href="https://www.ijcai.org/proceedings/2018/188">article</a>, <a href="https://www.springerprofessional.de/en/cores-in-core-based-maxsat-algorithms-an-analysis/2213506">article</a></p></div><div id="2242022"><h3>Vendredi 22 avril 2022</h3><p style="text-align:left;"><b>Esquisse des tendances actuelles et à venir en Intelligence Artificielle</b><br>Brahim Chaib-draa<br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/uzzgDqU4Q0suWwIDPSgpEc07wZ_QtlYPEpPEBhY7zaEYcpbR4y7ygvR14PvIFK-O.8nXiGe70eoZ_89Ee?startTime=1650648019000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>L'intelligence artificielle IA (en particulier l'apprentissage machine) se répand de plus en plus et elle ne cesse de couvrir de plus en plus d'applications. Elle fait l'objet toutefois de changements/évolutions/remises en question très rapides; et est difficile de suivre un tel changement particulièrement au niveau de la recherche. Dans cette présentation, je ferai état des changements récents, des tendances actuelles et de ce qui pointe à l'horizon. Voici pour l'essentiel ce dont je vais parler :<p>
<b>Où en est actuellement l'apprentissage profond (DL) ?</b> Bataille CNN-Transformers-MLP; GPT (Generative Pretraining Transformers); Autres avenues prometteuses.<p>
<b>Grandes tendances qui commencent à émerger</b> : Le symbolico-numérique; Foundation Models; Sémantique de la boite noire.<p>
<b>Tendance à venir</b> : Artificial General Intelligence (AGI) ? Human vs AI. </resume></p><p><b>Lien</b>: <a href="files/Chaib-AI-H22.pdf">diapositives</a></p></div><div id="1542022"><h3>Vendredi 15 avril 2022</h3><p style="text-align:left;"><b>Vendredi saint (pas de séminaire)</b><br></p><p></p></div><div id="842022"><h3>Vendredi 8 avril 2022</h3><p style="text-align:left;"><b>Introduction à la programmation par contraintes et application à l'apprentissage d'arbres de décisions optimaux</b><br>Hélène Verhaeghe<br><i>Chercheuse postdoctorale, École Polytechnique de Montéral, Université Laval</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/syOe5upzF8JOll5w_Aisqpv3_J65yrSgYZ6-EGnvBRkG6xHUR3XdINFVeu5wueaM.JfFD77kjrmv5jIkG?startTime=1649438463000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>La programmation par contraintes (CP) est une manière déclarative de résoudre des problèmes d'optimisations combinatoires. La tâche de l'utilisateur est de décrire à quoi la solution correspond et non pas comment la trouver. Le solveur est responsable de résoudre le problème. Dû à cela, la CP est souvent considérée comme proche du Saint-Graal de la résolution de problèmes.

Les arbres de décisions sont parmi les modèles les plus populaires en apprentissage automatique. L'utilisation d'algorithmes gloutons pour les apprendre peut poser plusieurs désavantages : il est difficile de limiter la taille de l'arbre de décisions tout en maintenant une bonne qualité de classification et il est difficile d'imposer de nouvelles contraintes sur le modèle appris. Ces raisons sont à la base de l'émergence d'un intérêt pour des algorithmes exacts et flexibles dans l'apprentissage des arbres de décision. Nous introduisons une nouvelle approche pour apprendre des arbres de décisions en utilisant la programmation par contraintes. Comparée aux précédentes approches, notre approche montre de meilleures performances ainsi qu'une flexibilité qui permet l'inclusion de contraintes additionnelles au modèle.</resume></p><p><b>Biographie: </b>Hélène Verhaeghe a obtenu son Ph.D. en 2021 à l'UCLouvain (Belgique) après avoir obtenu un master en ingénieur civil en informatique et un master complémentaire en gestion à l'UCLouvain également. Son sujet principal est la programmation par contraintes (CP). Le sujet de sa thèse est la contrainte en extension. En plus de son sujet de Ph.D., elle a déjà eu l'opportunité de travailler sur d'autres sujets, principalement liés à la CP (planification, échantillonnage uniforme) ou aux techniques d'hybridation entre CP et ML (formulation d'arbres de décisions comme problème d'optimisation, amélioration de l'apprentissage de réseaux de neurones en utilisant la CP). Elle est actuellement en postdoctorat à Polytechnique Montréal sous la supervision des professeurs Gilles Pesant et Claude-Guy Quimper et elle a récemment obtenu une bourse de postdoctorat IVADO.</p><p><b>Lien</b>: <a href="http://www2.ift.ulaval.ca/~quimper/publications/Learning_Optimal_Decision_Trees.pdf">article</a></p></div><div id="142022"><h3>Vendredi 1<sup>er</sup> avril 2022</h3><p style="text-align:left;"><b>Acquisition de contraintes basée sur le dénombrement de solutions</b><br>Christopher Coulombe<br><i>Étudiant à la maîtrise, Laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/XKl7mfV2tuM-_DjMdRW7ndq4NsZKmie9yvV2_wd5gNoIhEoPaA2Q12ziwRS2TlUb.u8qAA_0J9_msIyvJ?startTime=1648833676000">Enregistrement</a> (mot de passe: Séminaire-2022)</p><p><b>Résumé: </b><resume>Pour résoudre un problème à l'aide de la programmation par contraintes, ce problème doit d'abord être modélisé sous la forme d'un problème de satisfaction de contraintes (CSP). Les CSP sont typiquement modélisés par des experts, mais une automatisation partielle est possible. Nous suggérons CABSC, un système qui performe de l'acquisition de contraintes basée sur le dénombrement. Pour apprendre un modèle, l'utilisateur fournit des exemples positifs de solutions et un Meta-CSP, c'est-à-dire une modélisation d'un problème combinatoire dont la solution est un CSP. Ce Meta-CSP permet de lister les contraintes potentielles  qui peuvent faire partie du CSP que l'utilisateur veut apprendre. Il permet également de décrire les paramètres des contraintes, tels que les coefficients d'une équation linéaire, et d'imposer des contraintes sur ces paramètres. CABSC lit le Meta-CSP en utilisant une version modifiée de MiniZinc et retourne le CSP qui accepte le moins de solutions parmi les CSP qui acceptent tous les exemples de solutions fournis. La résolution du Meta-CSP se fait par séparation et évaluation où le calcul des bornes utilise des dénombreurs. Nos expérimentations montrent que CABSC réussit à apprendre des contraintes et leurs paramètres avec un nombre variable d'exemples positifs.</resume></p></div><div id="2712023"><h3>Vendredi 27 janvier 2023</h3><p style="text-align:left;"><b>Apprentissage automatique et optimisation pour une utilisation judicieuse d'un véhicule électrique</b><br>Anthony Deschênes<br><i>Doctorant au Consortium de recherche en ingénierie des systèmes industriels 4.0</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/Tj7YZTzRNoCsh0F55XI6bExQ0JhQnyr0sP2JFucbYHbkL01O6VqdIX-3mr9MVubf.Gvjb0Z4SJAZ-Cxl7">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>Les véhicules électriques sont un moyen efficace pour réduire les émissions de gaz à effet de serre qui devient de plus en plus populaire. Planifier un long voyage en véhicule électrique est plus complexe que pour un véhicule à essence, car le nombre de stations de recharge est limité et il est difficile de prévoir avec précision le temps nécessaire pour recharger le véhicule ou sa consommation en énergie sur un trajet. Le risque de mal planifier son voyage est donc plus élevé. Nous présenterons différents problèmes liés aux véhicules électriques tirés de la littérature, comme la prédiction de la consommation d'un véhicule électrique pour aller d'un point A à un point B, la prédiction du temps de recharge, l'optimisation d'un itinéraire ainsi que la détection automatique de bris dans les bornes de recharge rapide. Je présenterai également nos contributions pour les résoudre qui varient de l'apprentissage automatique à l'optimisation combinatoire.</resume></p></div><div id="2012023"><h3>Vendredi 20 janvier 2023</h3><p style="text-align:left;"><b>Une théorie des processus probabilistes continus: équivalence  et distance</b><br><a href="https://www.ift.ulaval.ca/departement-et-professeurs/professeurs-et-personnel/professeurs-reguliers/fiche/josee-desharnais-167">Josée Desharnais</a><br><i>Professeure titulaire au département d'informatique et de génie logiciel, Université Laval</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2744<br><b>Visioconférence</b>: <a href="https://ulaval.zoom.us/j/83443481229?pwd=NjM4aXBHOFNUTllTNXlxUzFmMDZuQT09">Zoom</a></p/></p><p><b>Résumé: </b><resume>Cette présentation donnera un aperçu du contenu de deux articles qui ont gagné des prix de test du temps, 20 ans après leur publication (dont l'été dernier). En voici les auteurs et titres:<p>
- Richard Blute, Josée Desharnais, Abbas Edalat, Prakash Panangaden: Bisimulation for Labelled Markov Processes. LICS 1997: 149-158<p>
- Josée Desharnais, Radha Jagadeesan, Vineet Gupta, Prakash Panangaden: The Metric Analogue of Weak Bisimulation for Probabilistic Processes. LICS 2002: 413-422<p>
Les prix ont été attribués par LICS (Logics in Computer Science), une conférence internationale sur la logique.
Ces articles font partie d'une suite de plusieurs articles issus de mes travaux de doctorat sur les systèmes (~automates) probabilistes ayant un ensemble d'états infini continu.<p>
Voici un extrait de la présentation du deuxième article par le comité qui a attribué le prix l'été dernier:
  « This landmark paper is a tour de force, applying new techniques in novel ways to support what constitutes groundbreaking research into how to analyse probabilistic processes and their applications, in varied domains such as security (including privacy and information flow), fuzzy systems, control systems, mobile process theory, software engineering, programming language theory, formal methods, and coalgebraic process theory, among others. »</resume></p><p><b>Liens</b>: <a href="files/sem_Laval22.pdf">diapositives</a>, <a href="https://doi.org/10.1006/inco.2001.2962">premier article</a>, <a href="http://dx.doi.org/10.1109/LICS.2002.1029849">deuxième article</a></p></div><div id="1722023"><h3>Vendredi 17 f&eacute;vrier 2023</h3><p style="text-align:left;"><b>Apprentissage de cartes de conjectures sur des bornes précises des caractéristiques d'objet combinatoires</b><br>Jovial Cheukam Ngouonou<br><i>Doctorant au laboratoire de programmation par contraintes</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/0rrhAao3SCLhrdrBTA2YOrFB3BiuswLQ7fQFrRqPamRh3TLwlMmhJs9NApEUQssT.87eeoTR1bV81qNd1">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>En programmation par contraintes, la satisfaction de nombreuses contraintes globales peut être détectée en testant les propriétés d'un objet combinatoire. Par exemple, la contrainte All-Different sur N variables est satisfiable si et seulement s'il existe une correspondance de cardinalité N dans un graphe biparti particulier [Regin 1994]. En construction et traitement de preuves de théorèmes, on veut parfois prouver des conjectures sur des objets combinatoires tels que des graphes. Dans les deux domaines de recherche, les chercheurs bénéficieraient de la découverte automatique des relations qui lient entre elles les caractéristiques des objets combinatoires.<p>
C'est ainsi que nous introduisons le concept de carte de conjectures sur des bornes précises des caractéristiques des objets combinatoires, qui fournit un ensemble de bornes précises interdépendantes pour lesdits objets combinatoires.<p>
Nous décrivons ensuite  un système basé sur la programmation par contraintes, qui acquiert progressivement des cartes de conjectures. Le système a été testé pour rechercher des conjectures sur les bornes des caractéristiques des graphes orientés : il construit 16 cartes impliquant 431 conjectures sur des bornes inférieures et supérieures précises de huit caractéristiques des graphes orientés.</resume></p><p><b>Lien</b>: <a href="http://www2.ift.ulaval.ca/~quimper/publications/boundseeker.pdf">article</a></p></div><div id="1022023"><h3>Vendredi 10 f&eacute;vrier 2023</h3><p style="text-align:left;"><b>Détection et localisation de billots de bois par apprentissage profond</b><br>Jean-Michel Fortin<br><i>Étudiant à la maitrise au Norlab et au consortium de recherche FORAC</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2700</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/q8sqTn_YoapzATPopA_KQN8vNPvZZU8cxJS6Anpl2GZH3K6AOSPTWM9aB0dAGGFO._S0lqv1QexfiaFjx?startTime=1676053257000">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>L'industrie forestière vit une grande pénurie de main d'oeuvre, qui la pousse à chercher des alternatives à la récolte manuelle, comme la robotisation des opérations. Notamment, la cueillette des billots de bois par le porteur forestier semble un bon candidat pour l'automatisation, mais reste une tâche difficile à automatiser. En effet, les billots se présentent généralement dans des configurations encombrées, orientés de manière aléatoire et se chevauchant. Les travaux récents sur l'automatisation de la cueillette des billots de bois supposent généralement que leur pose est connue, sans tenir compte de la complexité du problème de perception. Cette présentation évalue la faisabilité de détecter et localiser les billots de bois, en utilisant une approche basée sur les données. Tout d'abord, nous introduisons un nouveau jeu de données, nommé "TimberSeg 1.0", qui est densément annoté, c'est-à-dire qui comprend à la fois des boites et des masques de segmentation pour chaque billot. Ce jeu de données comprend 220 images avec 2500 billots segmentés individuellement. À l'aide dujeu de données, nous comparons ensuite trois architectures de réseaux de neurones sur la tâche de détection et de segmentation des billots individuels ; deux méthodes basées sur les régions d'intérêt et une méthode basée sur l'attention. Notre cas d'utilisation démontre les limites des approches basées sur les régions pour les objets encombrés et allongés. Il met également en évidence le potentiel des méthodes basées sur l'attention sur cette tâche spécifique, car elles fonctionnent directement au niveau du pixel. Ces résultats encourageants indiquent qu'un tel système de perception pourrait être utilisé pour aider les opérateurs à court terme, ou pour automatiser entièrement les opérations de prélèvement de billots dans le futur.</resume></p><p><b>Lien</b>: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982286">article</a></p></div><div id="322023"><h3>Vendredi 3 f&eacute;vrier 2023</h3><p style="text-align:left;"><b>Recommendation des paramètres de soudure à l'arc par apprentissage supervisé</b><br>Tom Picherit<br><i>Étudiant à la maîtrise au Consortium de recherche en ingénierie des systèmes industriels 4.0</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/epyDrZP0yApACIBe8xi9MMxq8wS09rue9GsOzYEYNBzWPxWc0GRgEHVdip7j5O0v.evlFNRTSAkm0hs29">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>Le procédé de soudage à l'arc sous gaz métal (MIG-MAG) dépend de nombreux paramètres inter- dépendants et choisir les bonnes valeurs pour chacun est complexe, même pour un expert. Ces derniers procèdent par essais-erreurs pour trouver tous les paramètres de chacune des petites soudures (passes) nécessaires au soudage de deux pièces. Ainsi, la méthode actuelle n'est pas optimale et peut nécessiter beaucoup de temps et de matériel dépendamment de l'intuition de l'expert.<p>
Cette présentation porte sur la possibilité d'utiliser des techniques d'apprentissage supervisé pour aider les soudeurs dans leur prise de décision. Dans cette mesure, un système de recommandation en deux parties est proposé. Alors que la première étape (classification) est dédiée à identifier le nombre de passes nécessaires pour une soudure, la seconde (régressions) suggère les valeurs des sept paramètres restants pour chaque passe : couche, ampérage, tension, vitesse de dévidage du fil, fréquence, coupure et vitesse de soudage.<p>
  Après avoir extrait les données des formulaires de spécification de procédure de soudage, nous avons testé 11 algorithmes d'apprentissage supervisé, dont des réseaux de neurones, pour chaque étape de la solution et comparé les résultats. Nous avons constaté qu'un tel système de recommandation est capable de fournir des résultats intéressants pour tous les différents paramètres mentionnés ci-dessus, même si les données sont bruyantes en raison de la nature du processus des experts. Le meilleur modèle de classification est CatBoost avec 81,94% F1-Weighted-Score. Les meilleurs modèles de régression varient selon le paramètre étudié mais parviennent à obtenir une erreur absolue moyenne suffisamment réduite pour correspondre aux critères des experts.</resume></p></div><div id="1732023"><h3>Vendredi 17 mars 2023</h3><p style="text-align:left;"><b>Révélation de l'hétérogénéité dans les processus neurodéveloppementaux à l'aide d'outils computationnels</b><br>Aymeric Ferreira<br><i>Doctorant en biochimie sous la direction des professeurs Simon Hardy et Armen Saghatelyan</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Résumé: </b><resume>L'imagerie cérébrale comprend un ensemble de techniques qui permettent de collecter de nombreuses données neurobiologiques. Afin d'analyser et décrire toute leur complexité, de nombreuses métriques doivent être extraites pour caractériser les évènements observés. Ces données sont hétérogènes et multidimensionnelles. Pour possiblement mieux cerner et comprendre la diversité de ces phénomènes biologiques, nous avons choisi d'utiliser des méthodes computationnelles comprenant la réduction de la dimensionnalité des données et le clustering. Ici, nous présenterons deux exemples d'applications.<p>
La première partie est consacrée à l'étude de l'hétérogénéité des cellules en migration en fonction de leur dynamique migratoire. La migration cellulaire est un phénomène important dans le développement du cerveau, notamment dans le cadre des troubles neurodéveloppementaux. Les précurseurs neuronale, appelé neuroblastes, changent de formes lors de leur migration. Il existe deux phases pour ce procédé, une dite phase stationnaire et une phase migratoire. L'objectif de cette étude est de déterminer si ces populations de neuroblastes peuvent être séparées en utilisant leurs propriétés migratoires mais également d'utiliser des méthodes d'analyses statistiques pour trouver les différentes sous-populations afin de déterminer lesquelles sont communes. Enfin, nous avons étudié les propriétés migratoires de ces différentes populations des neuroblastes en venant perturber la migration à l'aide de modification génétique ou environnementales.<p>
La seconde partie est portée sur l'études de la plasticité synaptique, qui fait référence à la capacité que deux neurones ont à former une connexion, appelée synapse, qui peut se renforcer ou s'affaiblir. Ces changements sont centraux dans le cadre du remodelage synaptique qui mènent lors de la phase d'apprentissage et de la mémoire. À partir d'images de dendrites, prises avec un microscope confocal dans le bulbe olfactif, nous avons mis en place un pipeline pour effectuer un prétraitement des images puis extraire les épines dendritiques, qui sont des protrusions sur la surface de la dendrite qui servent à recevoir les entrées synaptiques. Après reconstruction de la dendrite en 3D, ces épines sont extraites et plusieurs métriques sont calculées incluant la longueur et la surface de l'épine qui sont des métriques standards dans l'analyse des épines. Après réduction de la dimensionnalité du jeu de données et clustering, nous avons relié la morphologie de chacune de ces sous-population avec leurs propriétés structurelles. Enfin, nous avons comparé le groupe contrôle et le groupe expérimental dans le cas de trois expériences qui ont conduit à des changements de plasticité. Les résultats montrent que la morphologie des épines ou leurs densités est affectée par ses différentes conditions.<p>
En résumé, nous avons développé des outils computationnels permettant de révéler l'hétérogénéité des neurones en développement en fonction de leur dynamique migratoire et de leurs propriétés structurelles.</resume></p></div><div id="2842023"><h3>Vendredi 28 avril 2023</h3><p style="text-align:left;"><b>Tensor Networks for Machine Learning</b><br>Guillaume Rabusseau<br><i>Professeur à l'Univeristé de Montréal</i></p><p><b>Heure</b>: 15h00<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/UlFOFyN16RmUjxb_jXx9mEv5KenJUW3SiW8eh9kgEhM1MzRcWMbiONtaE1RnjUG9.jxcFKRJH8bk3yNJZ">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>In this talk, I will give an introduction to tensor networks and give a very brief overview of three recent contributions from my group aiming at going beyond classical tensor decomposition models using the tensor network formalism.<p>
Tensors are high order generalization of vectors and matrices. Similar to matrix factorization techniques, one of the goal of tensor decomposition techniques is to express a tensor as a product of small factors, thus reducing the number of parameters and potentially regularizing machine learning models. While linear algebra is ubiquitous and taught in most undergrad curriculum, tensor and multilinear algebra can be daunting. In the first part of the talk, I will try to give an easy and accessible introduction to tensor methods using the tensor network formalism. Tensor networks are an intuitive diagrammatic notation allowing one to easily reason about complex operations on high-order tensors.<p>
  In the second part of the talk, I will very briefly give an overview of three recent work from my group, ranging from tensorizing random projections to studying the VC dimension of tensor network models and desigining novel aggregation function for graph neural networks.</resume></p><p><b>Biographie: </b>Guillaume Rabusseau is an assistant professor at Univeristé de Montréal and holds a Canada CIFAR AI chair at the Mila research institute. Prior to joining Mila, he was an IVADO postdoctoral research fellow in the Reasoning and Learning Lab at McGill University, where he worked with Prakash Panangaden, Joelle Pineau and Doina Precup. He obtained his PhD in computer science in 2016 at Aix-Marseille University under the supervision of François Denis and Hachem Kadri. His research interests lie at the intersection of theoretical computer science and machine learning, and his work revolves around exploring inter-connections between tensors and machine learning to develop efficient learning methods for structured data relying on linear and multilinear algebra.</p></div><div id="2842023"><h3>Vendredi 28 avril 2023</h3><p style="text-align:left;"><b>Apprentissage par renforcement et simplification automatique de texte</b><br>David Beauchemin<br><i>Candidat au doctorat au GRAAL sous la supervision de Richard Khoury</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/h_ZflOW4SRmraK3XPOjs5qPQXsUG5rxiZqQ0qTFhk-gdlkVGsRTeMEXc8a1HA22n.LVuRllocpassjDZH">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>Ce séminaire s'intéresse à la simplification automatique de texte supervisé et non supervisé. Il aborde cette tâche selon une approche d'apprentissage par renforcement non-supervisé tiré de l'article "Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text". Les limitations de cette approche seront discutées et des pistes de solutions seront présentées en lien avec les travaux de recherche en cours de l'étudiant pour améliorer la préservation du sens. Ces travaux ont été exécutés lors de son passage à l'Université Pompeu Fabra (UPF) sous la supervision d'Horacio Saggion.</resume></p></div><div id="2742023"><h3>Jeudi 27 avril 2023</h3><p style="text-align:left;"><b>StressLink: Adaptive Forecast System with Time-Series Neural Networks</b><br>Jad Bakieh<br><i>Étudiant au doctorat sous la supervision des professeurs Marc Parizeau, Philippe Giguère et Cem Subakan.</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-1120</p><p><b>Résumé: </b><resume>Time is an essential element in machine learning systems, enabling the identification of patterns and trends within datasets. Moreover, the time-dependant variables can impact system operations incrementally, with both linear and non-linear effects based on the temporal dependencies between the variables and system functions. This principle applies across a broad range of fields, including mechanical, electrical, and biochemical systems. Incorporating time dimension in deep learning architectures is crucial to develop precise and predictive models that account for the dynamic changes in the system. In this way, wearable AI technology can collect real-time data about electrophysiological changes, enabling the analysis of temporal patterns and fluctuations in variables that impact human health. This research project focuses on the development of a wearable platform that can offer prognostic intelligence about one's physiological and mental health by examining the impact of time-varying vital signs on future health outcomes. This study aims to develop a deep learning system that can be deployed on smart wearable devices, specifically smart wristbands, to continuously classify mental states and provide probabilistic forecasting of symptoms triggered by mental stress. To achieve stress level classification, time-series biosignals will be extracted from the wrist, namely, Electrodermal Activity (EDA), Photoplethysmography (PPG), and Skin Temperature (SKT) due to their collective ability in discriminating human stress levels. The prediction system will add a time dimension to enable a temporal classification system to learn hidden correlations between accumulative features and the probability of the target forecast.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais et ne sera pas diffusée par visioconférence.</p></div><div id="2142023"><h3>Vendredi 21 avril 2023</h3><p style="text-align:left;"><b>Aiming for Generalization, Efficiency and Interpretability in Machine Learning for Speech and Audio</b><br>Cem Subakan<br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/k_0f4rB4lFV8OtlLqg7ZFHhD8DalvdsceppfGMuFL9WfVsCiyA1RQg6uSVQfd38_.EBfP8AkD_O2vUJ4s?startTime=1682097916000">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>In recent years, machine learning models for speech and audio made significant strides with the proliferation of deep learning. However, there still remain interesting areas to work on regarding improving generalization on out-of-domain data, learning efficiency, and interpretability. In this talk, my goal is to offer samplers from each of these sub-aims to introduce my recent and ongoing research.<p>
  I will first of all talk about our work on speech separation where we try to measure the speech separation performance under real-life conditions. I will then move on to talk about our recent works on continual representation learning where we aim to continually train an audio encoder with minimal continual learning interventions, and cross-modal representation learning where we aim to increase cross-modal encoder performance by using unpaired text and audio data. Finally, I will introduce our recent efforts on developing neural network interpretation methods to reconstruct audio data to understand audio domain classifiers. I will finish the talk by briefly describing our two audio domain healthcare applications: Machine Learning for Infant Cry Analysis and Machine Learning for Speech and Language Disorders.</resume></p><p><b>Biographie: </b>Cem is an Assistant Professor at Université Laval in the Computer Science and Software Engineering department. He is also currently an Affiliate Assistant Professor in Concordia University Computer Science and Software Engineering Department, and an invited researcher at Mila-Québec AI Institute. He received his PhD in Computer Science from University of Illinois at Urbana-Champaign (UIUC), and did a postdoc in Mila-Québec AI Institute and Université de Sherbrooke. He serves as reviewer in several conferences including Neurips, ICML, ICLR, ICASSP, MLSP and journals such as IEEE Signal Processing Letters (SPL), IEEE Transactions on Audio, Speech, and Language Processing (TASL), IEEE Pattern Analysis and Machine Intelligence (PAMI). His research interests include Deep learning for Source Separation and Speech Enhancement under realistic conditions, Neural Network Interpretability, and Latent Variable Modeling. He is a recipient of the best paper award in the 2017 version of IEEE Machine Learning for Signal Processing Conference (MLSP), as well as the Sabura Muroga Fellowship from the UIUC CS department. He's a core contributor to the SpeechBrain project, leading the speech separation part.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="files/CemSubakan.pdf">présentation</a></p></div><div id="1442023"><h3>Vendredi 14 avril 2023</h3><p style="text-align:left;"><b>On Representation Learning of Multi-Source Domain Adaptation: Algorithm and Algorithm-Dependent Bounds</b><br>Qi Chen<br><i>Doctorante et membre du GRAAL</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/QoQunsxxEf95qLy52j065OBu2sJVwK6ZGJ1hbXFcyEluDuRIvhYxNHEhAqltKq9I.N7-SZCCY3P_6nXCX">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>We use information-theoretic tools to derive a novel analysis of Multi-source Domain Adaptation (MDA) from the representation learning perspective. Concretely, we study joint distribution alignment for supervised MDA with few target labels and unsupervised MDA with pseudo labels, where the latter is relatively hard and less commonly studied. We further provide algorithm-dependent generalization bounds for these two settings, where the generalization is characterized by the mutual information between the parameters and the data. Then we propose a novel deep MDA algorithm, implicitly addressing the target shift through joint alignment. Finally, the mutual information bounds are extended to this algorithm providing a non-vacuous gradient-norm estimation. The proposed algorithm has comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais.</p></div><div id="2652023"><h3>Vendredi 26 mai 2023</h3><p style="text-align:left;"><b>Differential Privacy: Toward a Better Tuning of the Privacy Budget (ε) Based on Risk</b><br>Mahboobeh Dorafshanian<br><i>Étudiante au doctorat sous la supervision du professeur Mohamed Mejri</i></p><p><b>Heure</b>: 13h30<br></p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/86d7krUQKMApi-vtnlssO7Ft72QfGLuNRra66gWixTmFZqabEBgnVOfwKUL7Ert4.v-CMq3ZdZtZKxdh_?startTime=1685121474000">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>Companies have key concerns about privacy issues when dealing with big data. Many studies show that privacy preservation models such as Anonymization, k-Anonymity, l-Diversity, and t-Closeness failed in many cases. Differential Privacy techniques can address these issues by adding a random value (noise) to the query result or databases rather than releasing raw data. Measuring the value of this noise (ε) is a controversial topic that is difficult for managers to understand. To the best of our knowledge, a small number of works calculate the value of ε. To this end, this paper provides an upper bound for the privacy budget ε based on a given risk threshold when the Laplace noise is used. The risk is defined as the probability of leaking private information multiplied by the impact of this disclosure. Estimating the impact is a great challenge as well as measuring the privacy budget. This paper shows how databases like UT CID ITAP could be very useful to estimate these kinds of impacts.</resume></p></div><div id="552023"><h3>Vendredi 5 mai 2023</h3><p style="text-align:left;"><b>Scalabilité architecturale des systèmes tutoriels intelligents</b><br>Alphonse Ébalé Nnemete<br><i>Étudiant au Doctorat, ÉRICAE (Équipes de Recherche en Ingénierie des ConnAissanEs)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2501</p><p><b>Visioconférence: </b><a href="https://ulaval.zoom.us/rec/share/fDGII4AfTHpfU30Ry4BlOT74y3u7hkeIy9sIN1AaKAaZSgmdOaur1j9R2TJkNbI.4MdEk7KaB_QXHUed?startTime=1683307491000">Enregistrement</a> (mot de passe: Séminaire-2023)</p><p><b>Résumé: </b><resume>Les systèmes tutoriels intelligents sont l'une des applications de l'intelligence artificielle en éducation. Ils visent à simuler l'activité « ou plutôt, les activités » des tuteurs humains, et à s'adapter au contexte particulier de chaque apprenant pour l'accompagner vers l'acquisition des habiletés requises pour la résolution des problèmes relatifs à un domaine de compétences donné. La modélisation du tutorat dans les systèmes adaptatifs d'apprentissage se fait grâce à la modélisation de l'expertise du domaine (savoirs et savoir-faire d'un champ particulier de compétences), et à l'automatisation des traitements sur le modèle résultant par une expertise pédagogique adaptée aux contraintes liées à une situation pédagogique et à un type d'environnement donnés.<p>
De la complexité fonctionnelle de ces plateformes a émergé deux approches conceptuelles priorisant, chacune, un seul des deux principaux paramètres à la base de la modélisation du tutorat informatique : la densité des contenus d'apprentissage, et la précision du suivi des acquis et des processus d'apprentissage. L'opérationnalisation de chacune de ces approches soulève des problèmes architecturaux conséquents, et leur divergence principielle induit une certaine complémentarité que nous mettons exergue dans nos travaux. Nous proposant aussi une démarche de modélisation mixte permettant de rendre compatible les paradigmes de traçage des connaissances et de séquençage des contenus.<p>
Le passage à l'échelle d'un système d'intelligence artificielle est un processus critique qui passe par la mise en collaboration de différents formalismes de représentation des connaissances et de mécanismes d'inférence. Ainsi, notre démarche qui vise à concilier les logiques conceptuelles de séquençage de contenus et de traçage des connaissances s'appuie sur la compatibilité, et la flexibilité existant entre le mécanisme de raisonnement à base de règles et le formalisme de représentation orienté objets (ou orienté schémas) dont les mécanismes natifs de propagation d'instances assurent le changement d'états et l'automatisation des traitements métiers.<p>
Nous discuterons des perspectives d'une telle démarche dans l'écologie éducative actuelle.</resume></p></div><div id="1872023"><h3>Mardi 18 juillet 2023</h3><p style="text-align:left;"><b>Efficient line search for ROC optimization in binary classification and changepoint detection</b><br><a href="https://directory.nau.edu/person/th798">Toby Hocking</a><br><i>Assistant Professor à la Northern Arizona University</i></p><p><b>Heure</b>: 10h00<br><b>Local</b>: PLT-2501<br><b>Visioconférence</b>: <a href="https://ulaval.zoom.us/j/69546049716?pwd=anJpYVhBMjZ2cmkwWEtwNEozeFQrZz09">Zoom</a></p/></p><p><b>Résumé: </b><resume>Receiver Operating Characteristic (ROC) curves are commonly used in binary classification, and can also be used to evaluate learned penalty functions in the context of supervised changepoint detection. Since the Area Under the Curve (AUC) is a piecewise constant function of the predicted values, it can not be directly optimized by gradient descent. Recently we showed that minimizing a piecewise linear surrogate loss, AUM (Area Under Min of false positives and false negatives), results in maximizing AUC. In this talk we propose a new algorithm for AUM minimization, which exploits the piecewise linear structure to efficiently compute an exact line search, for every step of gradient descent. Because the exact line search is quadratic time in the worst case, we additionally propose an approximate line search which is log-linear time in the worst case (asymptotically the same as a constant step size). Our empirical results show that the proposed algorithm is more computationally efficient than other variants of gradient descent (constant step size, line search using grid search, etc).</resume></p><p><b>Biographie: </b>A Berkeley-educated California native, Toby Dylan Hocking received his PhD in mathematics (machine learning) from Ecole Normale Superiere de Cachan (Paris, France) in 2012. He worked as a postdoc in Masashi Sugiyama's machine learning lab at Tokyo Tech in 2013, and in Guillaume Bourque's genomics lab in McGill University, Montreal, Canada (2014-2018). Since 2018 he is Assistant Professor at Northern Arizona University, where he directs the Machine Learning Research Lab. His main research interests are new statistical models, optimization algorithms, interactive systems, and software for machine learning.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p></div><div id="2612024"><h3>Vendredi 26 janvier 2024</h3><p style="text-align:left;"><b>IA générative : opportunités, défis et risques des modèles massifs du langage (LLMs)</b><br><a href="http://www2.ift.ulaval.ca/~chaib/">Prof. Brahim Chaib-draa</a><br><i>Professeur au département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Récemment, des modèles du langage pré-entrainés, au moyen d'algorithmes utilisant l'apprentissage machine, sur de massifs corpus du langage, ont montré de très bonnes capacités pour résoudre une variété de tâches reliée au traitement du langage naturel (TLN).  Sur cette base, les chercheurs ont constaté que la mise à l'échelle du modèle (model scaling) peut améliorer drastiquement sa capacité.
Partant de là, ils ont mis en oeuvre des modèles du langage en augmentant le nombre de paramètres qui peuvent facilement dépasser les 200 milliards. Fait très intéressant ces modèles appelés, large langage models (LLMs) ou Modèles massifs du langage (MMDs), permettent non seulement d'obtenir une amélioration très significative des performances, mais offrent aussi des capacités spéciales qui ne sont pas présents dans les modèles linguistiques à petite échelle. Ces modèles ont montré récemment de bonnes capacités à couvrir tout le champ de l'intelligence artificielle.
  Lors de cette présentation, je ferai un tour assez complet sur les opportunités, les défis et les risques reliés aux MMDs.</resume></p><p><b>Lien</b>: <a href="files/Chaib-Prest-H24.pdf">présentation</a></p></div><div id="1622024"><h3>Vendredi 16 f&eacute;vrier 2024</h3><p style="text-align:left;"><b>Efficient ML/DSP Algorithms for Real-Time Speech Communication Systems</b><br><a href="https://jmvalin.ca/">Jean-Marc Valin</a><br><i><a href=http://xiph.org>Xiph.Org</a> Foundation</i></p><p><b>Heure</b>: 13h30<br><br><b>Visioconférence</b>: <a href="https://ulaval.zoom.us/j/83443481229?pwd=NjM4aXBHOFNUTllTNXlxUzFmMDZuQT09">Zoom</a></p/></p><p><b>Résumé: </b><resume>Over the past years, machine learning techniques have been responsible for significant improvements to the state-of-the-art in speech and audio processing. Among potential applications, real-time communication systems impose more stringent requirements, both for processing latency, and for computational resources. In this talk, I will discuss how these constraints can be satisfied while still achieving very high quality. I will then introduce some of our work on improving the Opus audio codec using machine learning. I will discuss how to improve the coded speech quality at low bitrate and how to increase robustness to packet loss through Deep REDundancy (DRED).</resume></p><p><b>Biographie: </b>Jean-Marc Valin received his Ph.D. in Electrical Engineering from the University of Sherbrooke in 2005. He worked as a postdoc in the CSIRO ICT Centre in Sydney, Australia. He is the primary author of the Speex speech codec and one of the main authors of the Opus audio codec. He also contributed to the AV1 video codec. He has volunteered with the Xiph.Org Foundation since 2002. His research interests include real-time communication systems, speech and audio coding, as well as applications of machine learning to speech and audio processing.</p><p><b>Note: </b>La présentation sera donnée en français uniquement par Zoom.</p></div><div id="922024"><h3>Vendredi 9 f&eacute;vrier 2024</h3><p style="text-align:left;"><b>Ordonnancement cumulatif avec calendriers et heures supplémentaires</b><br>Samuel Cloutier<br><i>Doctorant au laboratoire de programmation par contraintes, département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Le problème d'ordonnancement cumulatif consiste à planifier un projet de tâches dans le temps sous diverses contraintes. La principale est une limite pondérée sur les tâches pouvant s'exécuter en même temps. La programmation par contraintes est couramment utilisée pour résoudre ce problème. Lorsque le projet inclut des tâches attribuées à des personnes, les concepts de calendriers et d'heures supplémentaires, qui ne sont pas gérés en ordonnancement standard, deviennent nécessaires.<p>
Nous proposons deux nouvelles contraintes permettant une meilleure modélisation et résolution de ce problème, soit Calendar et CumulativeOvertime. Ces contraintes sont capables de gérer les calendriers de congés arbitraires ainsi que les heures supplémentaires. Les résultats démontrent que les nouvelles contraintes permettent respectivement une accélération des résolutions de 160% et de 310% lorsque l'on minimise les coûts des heures supplémentaires comparé à un modèle utilisant des contraintes existantes. De meilleures solutions sont également trouvées pour les instances non résolues optimalement.</resume></p><p><b>Lien</b>: <a href="files/SamuelCloutier2024.pdf">présentation</a></p></div><div id="2322024"><h3>Vendredi 23 f&eacute;vrier 2024</h3><p style="text-align:left;"><b>Comparaison d'Approches pour l'Interrogation d'Ontologies Formelles via le Langage Naturel</b><br>Anicet Lepetit Ondo<br><i>Étudiant au doctorat, Équipe de Recherche en Ingénierie des ConnAissancEs (ERICAE)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Le web sémantique repose sur l'utilisation d'ontologies pour assurer le partage, la réutilisation et l'interopérabilité des données, représentant ainsi des connaissances compréhensibles par les ordinateurs. Cependant, l'interrogation des ontologies, souvent réalisée en utilisant le langage SPARQL, devient un défi, en particulier pour les utilisateurs non familiers. Les obstacles incluent des barrières linguistiques dues à la complexité syntaxique, la nécessité de comprendre la structure sous-jacente de l'ontologie, les erreurs potentielles dans la formulation des requêtes et la difficulté à exprimer des requêtes complexes. Pour rendre l'accès à la connaissance plus convivial, l'article explore l'interrogation des ontologies en langage naturel. Notre article propose une réflexion visant à guider les futurs concepteurs du domaine dans l'interrogation des ontologies en langage naturel, les orientant dans leur choix d'approche en fonction des applications qu'ils développeront. L'étude repose sur l'application de techniques de Traitement Automatique du Langage Naturel (NLP), intégrant des outils tels que Owlready2, RDFLIB, Protégé2000, et le langage de programmation Python pour atteindre ses objectifs. Trois approches distinctes ont été évaluées à cet effet. La première approche, basée sur des scénarios, a été testée sur deux ontologies distinctes : l'une liée aux concepts universitaires et l'autre à la liquidation de succession. Cette approche démontre une remarquable adaptabilité sur diverses ontologies. Cependant, son efficacité est étroitement liée aux types de scénarios, au jargon spécifique employé et à la nomenclature des entités. Les deux autres approches l'une basée sur les patrons de requêtes SPARQL et l'autre sur la structure d'arbre de décision ont été évaluées sur une ontologie spécifique, celle de la liquidation de succession. Elles présentent des performances robustes en termes de précision des résultats obtenus. Néanmoins, leur efficacité dépend de l'entraînement du modèle pour la détection d'entités nommées, la gestion de la liste des noeuds, et l'enrichissement des patrons de requête SPARQL, en opérant exclusivement au sein de cette ontologie particulière.</resume></p><p><b>Lien</b>: <a href="files/AnicetLepetitOndo.pdf">présentation</a></p></div><div id="222024"><h3>Vendredi 2 f&eacute;vrier 2024</h3><p style="text-align:left;"><b>Programmation dynamique pour résoudre le problème d'optimisation d'une route fixe pour un aéronef hybride électrique</b><br><a href="https://lab-usine.ulaval.ca/">Anthony Deschênes</a><br><i>Doctorant, Lab-Usine, Département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>La mobilité aérienne évolue rapidement vers le développement et l'utilisation d'aéronefs hybrides électriques dans des vols multi missions. Les opérateurs d'aéronefs doivent prendre en compte les multiples contraintes d'infrastructure et opérationnelles dans leurs planifications pendant laquelle prédire efficacement l'utilisation de l'énergie est critique. Nous introduisons donc ce problème comme le Fixed Route Hybrid Electric Aircraft Charging Problem (FRHACP). Étant une route fixe, ce problème vise à décider de la quantité de recharge et de carburant à prendre à chaque terminal et à décider du type d'énergie (électrique ou fossile) à utiliser durant chaque segment des vols (le taux d'hybridation). L'objectif est de minimiser les coûts énergétiques totaux (carburant et électricité) tout en respectant un horaire et certaines contraintes d'hybridation.<p>
  Nous proposons un algorithme de programmation dynamique pour résoudre ce problème et démontrons qu'il est optimal sous certaines hypothèses généralement satisfaites dans un contexte réel. Nous proposons ensuite un post traitement à base de descente de gradient pour assouplir l'une de ces suppositions tout en maintenant l'optimalité de la solution. Les résultats  sur des instances réalistes démontrent que les algorithmes développés surpassent des heuristiques voraces en atteignant une réduction des coûts de jusqu'à 19.4%.</resume></p><p><b>Lien</b>: <a href="files/H2024AnthonyDeschenes.pdf">présentation</a></p></div><div id="2232024"><h3>Vendredi 22 mars 2024</h3><p style="text-align:left;"><b>De la théorie de la compression d'échantillons aux algorithmes de méta-apprentissage</b><br>Benjamin Leblanc<br><i>Étudiant au doctorat au Graal</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>En apprentissage automatique, la théorie de la compression d'échantillon permet d'obtenir des garanties de généralisation pour des prédicteurs qui ne dépendent qu'un de petit sous-ensemble des données d'entraînement, que l'on appelle « ensemble de compression ». Ces garanties tiennent même si l'algorithme d'apprentissage observe l'entièreté des données d'entraînement, du moment qu'il existe une fonction qui soit en mesure de « construire » le prédicteur appris à partir de « l'ensemble de compression ». D'autre part, le « méta-apprentissage » consiste en un paradigme où l'on tente d'apprendre simultanément à partir de plusieurs tâches partageant des similarités. Dans ce séminaire, je présenterai d'abord les grandes lignes de ces deux théories avant de discuter de mes travaux actuels qui traite de la conciliation de ces deux paradigmes.</resume></p><p><b>Lien</b>: <a href="files/BenjaminLeblanc.pdf">présentation</a></p></div><div id="1532024"><h3>Vendredi 15 mars 2024</h3><p style="text-align:left;"><b>Theoretical guarantees for Deep Generative Models: A PAC-Bayesian Approach</b><br>Sokhna Diarra Mbacke<br><i>Étudiante au doctorat au Graal</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>In this presentation, we study the statistical properties of deep generative models.<p>
Generative models have become a central research area in machine learning, with applications in many different areas. The goal of a generative model is to replicate an unknown data-generating distribution, from which only a finite number of samples is available. Intuitively, a good performance metric should measure the discrepancy between the true data-generating distribution and the distribution learned by the model. This is a very challenging task, not only because the true distribution is unknown in general, but also because different discrepancy measures between probability distributions have different behaviours and interpretations.<p>
We use PAC-Bayesian theory to study the theoretical properties of deep generative models. PAC-Bayes provides generalization bounds for machine learning models. The first part of this presentation focuses on adversarial generative models, such as the Wasserstein Generative Adversarial Network, and the second part of the presentation focuses on Variational Autoencoders.</resume></p><p><b>Note: </b>La présentation sera donnée en français.</p><p><b>Lien</b>: <a href="files/SokhnaDiarraMbacke.pdf">présentation</a></p></div><div id="832024"><h3>Vendredi 8 mars 2024</h3><p style="text-align:left;"><b>Semaine de lecture (pas de séminaire)</b><br></a><br><i></i></p><p></p></div><div id="132024"><h3>Vendredi 1<sup>er</sup> mars 2024</h3><p style="text-align:left;"><b>Augmentation du filtrage de la contrainte AuPlusNValeur à l'aide d'altérations locales des multiplicateurs de Lagrange</b><br>Frédéric Berthiaume</a><br><i>Doctorant au laboratoire de programmation par contraintes, département d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>En se basant sur deux techniques éprouvées, une nouvelle idée d'algorithme est testée en programmation par contraintes. Au coeur de la programmation par contraintes se trouvent les algorithmes de filtrage associés à des contraintes. Ces algorithmes retirent du domaine des variables des valeurs incohérentes avec la relation entre les variables imposée par la contrainte. Un algorithme de filtrage d'une contrainte basé sur le filtrage par coût réduit utilise les coûts réduits du programme linéaire encodant la contrainte pour filtrer les valeurs du domaine des variables. Combiné à une relaxation Lagrangienne, il a été montré que les multiplicateurs de Lagrange sous-optimaux sont meilleurs pour faire du filtrage par coût réduit. Un algorithme pour la contrainte CircuitPondéré a poussé cette idée en altérant localement des multiplicateurs de Lagrange optimaux pour accentuer le filtrage de la contrainte.<p>
  Nous poussons cette idée encore plus loin, mais sur une différente contrainte. La contrainte AuPlusNValeur a plusieurs applications entre autres pour le problème de localisation et cartographie simultanée (SLAM) et la couverture d'aire pour des entreprises. Satisfaire la contrainte AuPlusNValeur est NP-difficile. Il existe déjà un algorithme de relaxation Lagrangienne pour la contrainte, mais aucun algorithme d'altérations locales. Nous avons rajouté une étape dédiée aux altérations locales des multiplicateurs Lagrange. Cette étape utilise une deuxième relaxation Lagrangienne qui tient compte des coûts réduits. Nous avons testé l'algorithme sur le problème des reines dominantes et le problème p-median. Sur le problème des reines dominantes, l'algorithme d'altérations locales a accéléré le temps pour trouver une solution de 71% en moyenne. Sur le problème p-median, il y avait trois classes de problèmes. Sur les deux premières, l'algorithme proposé a accéléré le temps de 33% et 8% en moyenne. Sur la dernière, l'algorithme proposé a trouvé 13 meilleures solutions que l'algorithme original sur les 30 instances du banc d'essai.</resume></p><p><b>Lien</b>: <a href="files/seminar_atmostnvalue-31.pdf">présentation</a></p></div><div id="2642024"><h3>Vendredi 26 avril 2024</h3><p style="text-align:left;"><b>Model-based Optimization with Normalizing Flows</b><br>Sara Karami<br><i>Étudiante au doctorat, laboratoire de vision et systèmes numériques</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>This research aims to improve sample efficiency and accelerate convergence in Black-Box Optimization (BBO) using a model-based optimization framework. Our approach leverages a distribution-based Evolutionary Strategy (ES) improved with Normalizing Flows (NF) for better search guidance. Distribution-based ES algorithms are a common choice of search algorithm in BBO. In general, these algorithms rely on predefined distributions (e.g., Gaussian, Cauchy) to conduct queries in the search space, which introduces a potentially harmful implicit constraint for the stochastic search. To address this limitation, we propose the use of flexible distributions specifically tailored to the underlying search space, adopting NF as our generative model. Furthermore, classical BBO methods, such as CMA-ES, are susceptible to converging to local optima due to the limited capacity of their sampling distribution to capture the global structure of the objective landscape. To tackle this problem, better results are commonly achieved through multiple independent runs of the algorithm, each time varying the starting point or the method initialization and selecting the best case without using the knowledge gained in the multiple runs to improve their performance. In contrast, the adaptability of NF to the landscape in our method helps simultaneous exploration of multiple modes and prevents premature convergence to local optima. Moreover, by finetuning the neural network to update the sample distribution instead of multiple runs from scratch, our approach takes the impact of earlier samples into account, improving efficiency and outcomes.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="files/SaraKarami.pdf">présentation</a></p></div><div id="1942024"><h3>Vendredi 19 avril 2024</h3><p style="text-align:left;"><b>On the migration from a monolithic system to a microservices architecture: a study of automated decomposition approaches</b><br>Khaled Sellami<br><i>Étudiant au doctorat sous la supervision du professeur Mohamed Aymen Saied</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Developing and maintaining Monolithic architecture based applications has often led to issues that emerge over time such as scalability and heavy coupling and documentation drift issues. The Microservice Architecture was a popular choice to remedy these problems which is why multiple large organizations have migrated their monolithic applications to this architectural style. However, migration efforts have been shown to be very costly in the short term, very lengthy and, most importantly, very difficult to execute. The decomposition of the monolith and refactoring of the source code in particular have been shown to be the most complex steps in this process since they require a very specific set of skills. The decomposition step consists of matching the components of the original monolithic application to the targeted microservices of the new application while the refactoring step consists of adapting and modifying the code to these changes. Due to the difficulty and complexity of these steps, there was a rising interest in researching approaches and methodologies that would automate the difficult parts of the migration process and offer assistance to the developers in this endeavor. In this presentation, we will review the state of the art of automated decompositions methods that have been proposed in the recent years and describe the challenges and main concepts utilized in the research of this field. We will present as well the solutions that we proposed for this problem utilizing AI-based approaches. For example, we design the decomposition process as a reinforcement learning problem and compare different representations of a monolith. We present as well a study of the quality of source code representations in the context of microservices applications ranging from simple text representations such as word counts to contextual embeddings and Large Language Model (LLM) based embeddings.</resume></p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Lien</b>: <a href="files/seminar_khaled_20240419.pdf">présentation</a></p></div><div id="1242024"><h3>Vendredi 12 avril 2024</h3><p style="text-align:left;"><b>Learning Analytics: Modélisation des profils d'apprenants dans Moodle à l'aide du Data Mining</b><br>Kouassi Rodrigue Ahoussou<br><i>Étudiant au doctorat, Équipe de Recherche en Ingénierie des ConnAissancEs (ERICAE)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Les cours mixtes sont de plus en plus adoptés par les universités. Cette transition vers l'utilisation des plateformes de cours offre de nombreux avantages, notamment une centralisation des ressources pédagogiques et une flexibilité d'apprentissage, permettant aux étudiants d'accéder au contenu à tout moment et de n'importe où. Cependant, ces cours mixtes et en ligne soulèvent des défis importants liés à l'autonomie et à la discipline des étudiants. Il est donc crucial d'identifier des stratégies efficaces pour soutenir les étudiants dans leur expérience d'apprentissage et leur réussite dans leurs études.<p>
Pour surmonter ces défis et optimiser l'expérience d'apprentissage en ligne et mixte, nous proposons une approche initiale visant à modéliser les profils d'apprenants à l'aide des techniques de Data Mining dans Moodle. Cette approche permettra d'évaluer les processus d'analyse des données d'apprentissage afin de détecter les apprenants à risque et de prédire les étudiants en échec avec moins de données et le plus tôt possible.<p>
  Dans un premier temps, nous présenterons quelques travaux antérieurs, puis nous détaillerons les différentes étapes de la modélisation des profils d'un apprenant. Enfin, nous aborderons les défis liés au traitement des données brutes de Moodle et à la complexité du choix des indicateurs d'apprentissage.</resume></p><p><b>Lien</b>: <a href="files/Kouassi.pdf">présentation</a></p></div><div id="542024"><h3>Vendredi 5 avril 2024</h3><p style="text-align:left;"><b>Anticipation des flux monétaires et de produits pour une gestion optimale des revenus</b><br>Nicolas LeBlanc<br><i>Étudiant à la maîtrise / Lab-Usine</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>Dans un contexte d'une entreprise manufacturière, il est essentiel de pouvoir anticiper le mouvement des différents produits ainsi que les revenus monétaires engendrés à la suite des processus de transformation. Il est donc nécessaire de pouvoir déterminer le temps de début et de fin des tâches qui dépendent de plusieurs facteurs tels que le temps d'exécution, le temps d'attente, la capacité des centres de travail sur lequel elles sont effectuées, etc. Pour y parvenir, un modèle de simulation déterministe à évènements discrets a été développé afin de répliquer les règles d'affaires et la politique d'assignation des tâches actuellement en vigueur chez le partenaire industriel qui est premier entré, premier servi aussi connu sous le nom de "FIFO ". L'ordonnancement obtenu n'est pas nécessairement celui qui permet d'obtenir les revenus maximums sur une période donnée, c'est pour cette raison qu'un modèle de programmation par contraintes, a aussi été développé. Ces deux modèles permettent ensuite d'effectuer des analyses de scénario en mode "what-if ?". Par exemple, de mesurer l'impact de l'ajout ou retrait d'un opérateur à un centre de travail ou déterminer quelle opération est la plus critique. Il est ensuite possible de comparer la solution obtenue grâce à ce modèle à la solution du modèle de simulation afin de quantifier le gain en revenus que pourrait engendrer la modification de l'ordonnancement.</resume></p><p><b>Lien</b>: <a href="files/NicolasLeBlanc.pdf">présentation</a></p></div><div id="1752024"><h3>Vendredi 17 mai 2024</h3><p style="text-align:left;"><b>Apprentissage à partir d'une cible manquante: Classification et réduction de dimensions</b><br><a href="http://perso.univ-lyon1.fr/khalid.benabdeslem">Khalid Benabdeslem</a><br><i>Maître de conférence à Université Claude Bernard Lyon 1</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>L'ensemble des activités de recherche décrites dans ce séminaire s'inscrit dans le cadre de l'apprentissage semi-supervisé, au sens large, pour l'extraction de connaissances à partir de données multidimensionnelles. Dans cette thématique, on s'intéresse à l'analyse de données partiellement étiquetées qui peut être abordée selon deux grandes familles d'approches. La première est basée sur la propagation de la "supervision", en vue de l'apprentissage supervisé. La seconde est fondée sur la transformation des données étiquetées en contraintes pour leur intégration dans un processus non-supervisé. Les travaux abordés s'inscrivent dans la deuxième famille d'approches avec une difficulté particulière. Il s'agit d'apprendre à partir de données dont la partie étiquetée est relativement réduite par rapport à la partie non-étiquetée. Pour ce faire, on s'intéresse particulièrement aux modèles topologiques, à l'analyse spectrale de graphes et à l'optimisation convexe et non convexe de la préservation de la similiarité. S'appuyant sur ces modèles, on vise à répondre à plusieurs questions qui sont souvent posées dans les communautés d'apprentissage automatique et de fouille de données, et qui demeurent toujours d'actualité. Les réponses à ces questions se traduisent par quelques contributions qui constituent le coeur de ce séminaire : (1) classification topologique sous contraintes, (2) sélection de variables en mode semi-supervisé et (3) co-sélection de variables/instances en mode simultané.</resume></p></div><div id="2862024"><h3>Vendredi 28 juin 2024</h3><p style="text-align:left;"><b>Graphes pangénomiques pour l'inférence du maximum de vraisemblance sur l'ADN fossile</b><br><a href="https://grenaud.github.io/index.html">Gabriel Renaud</a><br><i>Professeur agrégé à l'Université Technique du Danemark</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2551</p><p><b>Résumé: </b><resume>L'ADN fossile est un outil remarquable qui a complètement modifié notre compréhension des populations humaines anciennes, des hommes archaïques comme les Néandertaliens, ainsi que de la composition des environnements anciens. Nous avons notamment appris que notre espèce s'est croisée avec les Néandertals à un moment donné de notre histoire et que tous les non-Africains ont des ancêtres néandertals. Cependant, l'un des grands problèmes de l'ADN fossile est que les données sont très rares. Alors que l'approche traditionnelle de l'analyse des séquences d'ADN consistait à effectuer un alignement sur le génome humain représenté sous la forme d'une chaîne de caractères, des groupes de recherche ont cherché à intégrer la diversité génomique et à représenter le génome sous la forme d'un graphe plutôt que d'une chaîne de caractères. Ces structures de données sont appelées graphes pangénomiques et représentent un domaine de recherche très actif en bioinformatique car elles réduisent le biais induit par l'existence d'un seul génome de référence. Dans cette présentation, je montrerai que non seulement les graphes pangénomiques atténuent le problème de l'alignement, mais que nous pouvons également construire des algorithmes pour les inférences via le maximum de vraisemblance.  Je présenterai deux applications, l'une où nous déduisons le type de génome mitochondrial d'un fossile d'être humain et l'autre où nous détectons de l'ADN animal dans des sédiments très anciens.</resume></p><p><b>Biographie: </b>Gabriel Renaud a obtenu un Bac en informatique théorique à l'Université de Montréal et une maîtrise à l'University of Waterloo. Il a travaillé pendant quatre ans au National Human Genome Research Institute, près de Washington DC. Il s'est ensuite intéressé à l'ADN fossile et a fait son doctorat à l'Institut Max Planck d'Anthropologie Évolutive à Leipzig en Allemagne. Il s'est concentrée sur les algorithmes d'analyse de l'ADN extrait des Néandertals et des humains anciens. Actuellement, il dirige le groupe « Génomes modernes et anciens » en tant que professeur agrégé à l'Université Technique du Danemark, près de Copenhague.</p></div><div id="572024"><h3>Vendredi 5 juillet 2024</h3><p style="text-align:left;"><b>Evaluation automatique de questions ouvertes : calcul de score par similarité textuelle sémantique</b><br>Idrissa Abdou<br><i>Doctorant à la Chaire de leadearship en enseignement à distance en matière d'informatique et de génie logiciel</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2341</p><p><b>Résumé: </b><resume>L'évaluation d'épreuves pour la validation des connaissances est un élément incontournable pour tout processus d'apprentissage. Deux principaux modes d'évaluation s'opposent : l'évaluation subjective et l'évaluation automatisée. L'évaluation subjective est taxée de beaucoup de tares telles que l'inconsistance du correcteur humain et le temps qu'elle nécessite. Ces insuffisances de l'évaluation subjective ont encouragé les enseignants à se tourner vers l'évaluation automatisée, surtout pour les grands groupes. Cependant, l'enthousiasme suscité par l'évaluation automatisée s'est vite retrouvé confronté à un problème de mise en oeuvre. S'il est aisé de concevoir un outil de correction automatique entièrement satisfaisant pour les questions fermées, il est aussi difficile de le faire pour les questions ouvertes. En effet, le constat est que l'évaluation de celles-ci est complexe à automatiser de manière à être exploitée à grande échelle avec une précision similaire à celle d'un correcteur humain. C'est cette problématique qui nous intéresse.<p>
Nous avons mis en oeuvre une première solution en exploitant des modèles de langue pré-entraînés francophones (Camembert et Flaubert). Cela nous a permis d'obtenir nos premiers résultats qui sont plutôt satisfaisants comparativement à notre situation de référence qu'est l'évaluation du correcteur humain. Pour améliorer ceux-ci, nous avons ajusté les modèles ci-dessus avec deux jeux de données : STSB (Semantic Textual Similarity Benchmark) et SITEXSE (SImilarité TEXtuelle SEmantique). Ce dernier est un jeu de données que nous avons créé pour pallier au manque de jeux de données francophones dédiés au calcul de la similarité textuelle sémantique. STSB est un jeu de données multilangue prenant en charge le français.<p>
Les études qui offrent présentement les meilleurs résultats utilisent les LLMs (Large Language Models). Certes, ces résultats sont encourageants mais toutes ces études ont un dénominateur commun : le calcul de la similarité textuelle sémantique est fait en examinant l'intégralité du texte. Nous appelons ces méthodes, des méthodes à couverture globale. Or, pour le cas d'évaluation des réponses des apprenants, le correcteur humain n'a pas besoin de lire l'intégralité de la réponse. Dès qu'il juge que l'apprenant a compris et répondu correctement à la question, il donne la totalité des points et passe à la réponse suivante. Fort de ce constat et pour se rapprocher davantage de l'évaluation du correcteur humain, nous proposons d'aborder le problème en utilisant une approche à couverture partielle, en l'occurrence, les expressions pondérées. Nous appelons expressions pondérées, des expressions suffisamment pertinentes pour équivaloir ensemble à la réponse de référence. Elles sont contenues dans cette dernière. Elles sont censées figurer sémantiquement dans la réponse de l'apprenant. Cette présentation s'appesantira sur les pistes que nous proposons pour la mise en oeuvre de cette approche.
</resume></p></div><div id="2092024"><h3>Vendredi 20 septembre 2024</h3><p style="text-align:left;"><b>On generalisation and learning - some theoretical results for deep learning</b><br><a href="https://bguedj.github.io/">Benjamin Guedj</a><br><i>Principal Research Fellow in machine learning at University College London</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT 2765</p><p><b>Résumé: </b><resume>Generalisation is one of the essential problems in machine learning and foundational AI. The PAC-Bayes theory has emerged in the past two decades as a generic and flexible framework to study and enforce generalisation abilities of machine learning algorithms. It leverages the power of Bayesian inference and allows to derive new learning strategies. I will briefly present the key concepts of PAC-Bayes and highlight a few recent contributions from my group focusing on the theory of deep learning. </resume></p></div><div id="18102024"><h3>Vendredi 18 octobre 2024</h3><p style="text-align:left;"><b>ChatGPT comme assistant de programmation: atout ou risque?</b><br><a href="https://uqo.ca/profil/khoura02">Raphaël Khoury</a><br><i>Professeur, Université du Québec à Gatineau</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2765</p><p><b>Résumé: </b><resume>Les LLMs, tel que ChatGPT, sont de plus en plus utilisés par les programmeurs pour générer des fragments de code, et même des programmes entiers. Cependant, le niveau de sécurité du code ainsi généré est imparfait. Dans cette présentation, nous passerons en revue les recherches initiales effectuées pour évaluer la capacité des LLMs à générer du code sécuritaire, ainsi qu'à effectuer d'autres tâches de sécurité telles que détecter et classer les vulnérabilités dans le code, de même que corriger des vulnérabilités qui sont subséquemment identifiées.</resume></p><p><b>Biographie: </b>M. Khoury a obtenu son doctorat en sécurité informatique de l'Université Laval et a été chercheur postdoctoral au centre de Recherche et développement pour la défense Canada (RDDC-RDDC) à Valcartier, au Canada. Il est actuellement professeur à l'université du Québec à Gatineau. Ses recherches portent sur tous les aspects de la sécurité des logiciels, et est l'auteur du livre « La Sécurité Logicielle : une approche défensive ». Il est récipiendaire de subventions de recherche du FQRNT et du CRSNG. Ses recherches ont également été financées par l'industrie.</p></div><div id="22112024"><h3>Vendredi 22 novembre 2024</h3><p style="text-align:left;"><b>Natural Language Processing Methods for Population Health Research</b><br><a href="https://philab.uwo.ca/">Dan Lizotte</a><br><i>Professor, Western University</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-2744</p><p><b>Résumé: </b><resume>Natural Language Processing methods offer a way of analyzing text data that traditionally would be used for qualitative research, for example using ethnographic or grounded theory methods. They offer opportunities both to scale up analyses to larger datasets than are feasible for humans to analyze and to scale out analyses to learn about and support populations that may otherwise be inaccessible because of stigma. In both cases, natural language methods offer new possibilities for better understanding population health. In this presentation, I will describe projects spanning biostatistics, computer science, and health research that apply natural language methods to population health questions, explain the technical and conceptual challenges we needed to overcome, and relay what we learned. I will end by proposing some future directions at the intersection of these disciplines.<p>
<b>Content advisory:</b> Topics of mental health, self-harm, substance use, and homelessness will be discussed.
  </resume></p><p><b>Biographie: </b>Dr. Dan Lizotte is Associate Professor in the Department of Computer Science and the Department of Epidemiology and Biostatistics at Western. His research aims to support health decision-making by developing and applying machine learning and statistical tools to new sources of data including electronic health records and social media to better support patients and health professionals, particularly in public health and primary health care. His methodological research combines machine learning, optimal sequential decision-making, and multiple objective optimization. He is also the director of the Rotman Institute of Philosophy at Western and he is currently working with the Alliance for Healthier Communities to develop tools for research and decision support.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p></div><div id="15112024"><h3>Vendredi 15 novembre 2024</h3><p style="text-align:left;"><b>Robotique de terrain intelligente pour la compréhension des environnements</b><br><a href="https://www.usherbrooke.ca/recherche/fr/specialistes/details/louis.petit">Louis Petit</a><br><i>Professeur, Université de Sherbrooke</i></p><p><b>Heure</b>: 10h00<br><b>Local</b>: PLT-2546</p><p><b>Résumé: </b><resume>La présentation passera en revue les travaux passés de Dr. Petit, incluant la planification de trajectoire quasi-optimale dans les grands environnements non-encombrés, l'exploration autonome de cavités minières pour un drone filaire, ainsi que la planification de trajectoire adaptative aux risques pour un drone d'inspection de structures. Un accent particulier sera mis sur la façon dont la compréhension des applications sur le terrain permet souvent de procéder à des simplifications algorithmiques qui rendent possible des missions autrement impossibles (ex: calcul en temps réel). Certains travaux en cours seront également mentionnés, comme la reconnaissance de plantes rares sur les falaises à l'aide d'un drone, un système d'aide à la conduite hors route terrestre et marine, et la coopération entre robots hétérogènes pour comprendre les écosystèmes.</resume></p><p><b>Biographie: </b>Louis Petit is an Assistant Professor in the Department of Electrical and Computer Engineering at the Université de Sherbrooke. His current research focuses on developing efficient behaviors and strategies to endow autonomous mobile robots with intelligence and environmental awareness for safety, maintenance, and conservation. Applications include exploring unknown environments for search and rescue, advanced driver assistance systems, infrastructure inspection, and spatio-temporal mapping of ecosystems to monitor endangered or invasive fauna and flora. His research involves a combination of path planning, optimization, decision-making, machine learning, computer vision, and other robotics techniques.
He is a member of the Createk and IntRoLab research groups at the Interdisciplinary Institute for Technological Innovation (3IT). He was a Postdoctoral Researcher (2024) in Computer Science at the Mobile Robotics Lab at McGill University, working with David Meger and Gregory Dudek. Prior to McGill, he completed his Ph.D. in Mechanical Engineering at the Université de Sherbrooke (2023), where he worked with Alexis Lussier Desbiens. He holds a BSc and a MSc in Mechatronics Engineering from UCLouvain (2019).</p></div><div id="3112025"><h3>Vendredi 31 janvier 2025</h3><p style="text-align:left;"><b>Local clustering and motif discovery of functional data</b><br><a href="https://marziacremona.com">Marzia A. Cremona</a><br><i>Professeure agrégée, Département opérations systèmes de décision (FSA)</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>Recent evolution in data acquisition technologies enabled the generation of high-dimensional, complex data in several research areas - in the sciences and engineering, among other disciplines. Increasingly sophisticated statistical and computational methods are needed in order to analyze these data. Functional data analysis (FDA) can be broadly employed to analyze functional data, i.e. data that vary over a continuum and can be naturally viewed as smooth curves or surfaces, exploiting information in their shapes.<p>
In this talk, I will present probabilistic K-mean with local alignment (probKMA, [1]), a novel unsupervised learning method to locally cluster a set of misaligned curves and to address the problem of discovering functional motifs, i.e. typical "shapes" or "patterns" that may recur several times along and across a set of curves, capturing important local characteristics of these curves. After demonstrating the performance of the method on simulated data and showing how it generalizes other clustering methods for functional data, I will present three applications to the analysis of functional data from different fields. First, I will apply probKMA to discover functional motifs in "Omics" signals related to mutagenesis and genome dynamics. Second, I will employ probKMA as a probabilistic clustering method to group COVID-19 death curves of the different Italian regions during the first wave of the pandemic. Finally, I will present an application to the discovery and characterization of financial bubbles in price time series.<p>
Finally, I will briefly introduce funBIalign [2], another novel method inspired by clustering and biclustering for discovering functional motifs, both in a single curve and in a set of curves.<p>
[1] Cremona, Chiaromonte (2023) Probabilistic K-means with local alignment for clustering and motif discovery in functional data. Journal of Computational and Graphical Statistics 32(3): 1119-1130.<p>
[2] Di Iorio, Cremona, Chiaromonte (2025) funBIalign: a hierarchical algorithm for functional motif discovery based on mean squared residue scores. Statistics and Computing 35:11.
  </resume></p><p><b>Biographie: </b>Marzia A. Cremona est professeure agrégée en science des données au département d'Opérations et Systèmes de Décision de l'Université Laval et chercheuse au centre de recherche du CHU de Québec - Université Laval. Elle est aussi membre de l'IID et du CIRRELT. Elle a obtenu un doctorat en Modèles et Méthodes Mathématiques pour l'Ingénierie du Politecnico di Milano (Italie) en 2016 et elle est arrivée à l'Université Laval en 2019 après avoir travaillé pendant quatre ans à la Pennsylvania State University (États-Unis). <p>
  Ses intérêts de recherche portent sur le développement de méthodes statistiques et informatiques pour l'analyse de données complexes et de grande dimension et sur l'application de ces méthodes aux sciences «omiques» et biomédicales, ainsi qu'aux sciences de l'administration. Une partie importante du programme de recherche du Dr Cremona concerne le développement de nouvelles techniques de l'analyse des données fonctionnelles (ADF), une branche des statistiques visant à analyser les courbes (fonctions mathématiques). Un aspect important de sa recherche est sa nature collaborative et multidisciplinaire. En effet, elle est impliquée dans de nombreuses collaborations internationales impliquant l'analyse des données à l'aide de techniques statistiques et d'apprentissage automatique.</p><p><b>Note: </b>La présentation sera donnée en anglais.</p><p><b>Liens</b>: <a href="https://doi.org/10.1080/10618600.2022.2156522">Article 1</a>, <a href="https://doi.org/10.1007/s11222-024-10537-y">Article 2</a></p></div><div id="2412025"><h3>Vendredi 24 janvier 2025</h3><p style="text-align:left;"><b>La compression d'échantillons sans limite : De nouveaux résultats pour les pertes à valeurs réelles</b><br>Mathieu Bazinet<br><i>Étudiant au doctorat au Graal</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La théorie de la compression d'échantillons permet d'obtenir des garanties de généralisation pour des prédicteurs qui sont complètement définis à partir d'un sous-ensemble du jeu de données et d'un message, généralement défini par une courte séquence binaire. Les travaux précédents proposent des garanties qui sont valides uniquement pour la perte zéro-un, ce qui peut être restrictif, notamment lorsqu'on s'intéresse à l'apprentissage profond. Durant cette présentation, je présenterai de nouveaux théorèmes de compression d'échantillons pouvant être utilisés avec des pertes à valeurs réelles ainsi que des pertes non bornées. Grâce au méta-algorithme Pick-To-Learn, nous avons empiriquement validé la qualité de nos bornes en les évaluant sur divers jeux de données avec des réseaux de neurones de plus en plus profonds.
</resume></p><p><b>Lien</b>: <a href="files/Bazinet2025.pdf">diapositives</a></p></div><div id="722025"><h3>Vendredi 7 f&eacute;vrier 2025</h3><p style="text-align:left;"><b>Programmation Différentiable pour données spatiales</b><br>Jean-Thomas Sexton<br><i>Doctorant au Lab-Usine</i></p><p><b>Heure</b>: 13h30<br><b>Local</b>: PLT-3775</p><p><b>Résumé: </b><resume>La programmation différentiable est un paradigme gagnant en popularité qui place la différentiation automatique au coeur de la conception des programmes. L'apprentissage profond "traditionnel" étant souvent perçu comme une boîte noire, la programmation différentiable favorise une approche plus transparente et modulaire, dans laquelle chaque opération peut être inspectée et optimisée à l'aide de gradients.
Lors de ce séminaire, la programmation différentiable sera expliquée et illustrée par son application directe à des problèmes spatiaux, notamment à l'analyse de nuages de points 3D et de données RGBD (RGB + profondeur), deux formes de données spatiales particulièrement représentatives des enjeux actuels en vision par ordinateur et en robotique. Nous détaillerons comment la programmation différentiable ouvre de nouvelles perspectives pour :<p>
Développer des algorithmes différentiables pouvant être optimisés de bout en bout, depuis l'acquisition des données jusqu'à la prise de décision.<p>

Concevoir des structures de données différentiables adaptées à la représentation de nuages de points et de scènes 3D.<p>

Améliorer la performance et la robustesse des algorithmes en intégrant l'optimisation comme mécanisme central, simplifiant grandement la formulation des problèmes et réduisant la nécessité de règles d'ingénierie spécifiques pour chaque type de données ou d'application.<p>
Enfin, nous ferons un survol des perspectives offertes par cette approche pour relever des défis complexes liés au traitement des données spatiales tout en offrant des solutions flexibles, évolutives et adaptées à une large gamme d'applications.
  </resume></p><p><b>Lien</b>: <a href="files/JeanThomasSexton2024.pdf">diapositives</a></p></div>
</div>
</div>
<!-- <div id="footer"> !>
</div>

</div>
</body>

</html>
